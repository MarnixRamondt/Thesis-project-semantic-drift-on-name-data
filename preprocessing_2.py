# -*- coding: utf-8 -*-
"""Preprocessing_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UpZincQPfR7fL0_bfmXmEGaYJCpTIsfW
"""

try:
  
  from google.colab import drive
  IN_COLAB=True
except:
  IN_COLAB=False

if IN_COLAB:
  print("We're running Colab")

# Commented out IPython magic to ensure Python compatibility.
if IN_COLAB:
  # Mount the Google Drive at mount
  mount='/content/gdrive'
  print("Colab: mounting Google drive on ", mount)

  drive.mount(mount)

  # Switch to the directory on the Google Drive that you want to use
  import os
  drive_root = mount + "/My Drive/Thesis/Code/Pre-processing"
  
  # Create drive_root if it doesn't exist
  create_drive_root = True
  if create_drive_root:
    print("\nColab: making sure ", drive_root, " exists.")
    os.makedirs(drive_root, exist_ok=True)
  
  # Change to the directory
  print("\nColab: Changing directory to ", drive_root)
#   %cd $drive_root

#loading the name data
#Processing the name dataset
    
with open('/content/gdrive/My Drive/Thesis/Data/Overig/1000Names.txt', 'r') as fp:
    nameslist = fp.readlines()
    
names = []
for name in nameslist:
    new = name.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    names.append(new6)
    

print(len(names))

with open('/content/gdrive/My Drive/Thesis/Data/Overig/Dolgopolsky.txt', 'r') as fp:
    dolgolist = fp.readlines()
    
dolgopolsky_list = []
for dol in dolgolist:
    new = dol.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    dolgopolsky_list.append(new6)
    

print(len(dolgopolsky_list))

## Loading the data:
import pandas as pd
import numpy as np

## How to add multiple datasets into one:
import os
## Setting working directory:
os.chdir("/content/gdrive/My Drive/Thesis/Data/30percrun")

datasetlist = []

for filename in os.listdir(os.getcwd()):
    with open(os.path.join(os.getcwd(), filename), 'r') as fp:
        content = fp.readlines()
        for line in content:
            lines = line.split("\t")
            datasetlist.append(lines)
            

df = pd.DataFrame(datasetlist)
print(len(df))
df = df.replace(to_replace='None', value=np.nan).dropna()
print(len(df))

#Creating dataset called df        
df.columns = ["5_gram", "Year", "Match_count", "Volume_count"]
df = df.astype({'Year':'int'})
df = df.astype({'Match_count':'int'})
df["Volume_count"] = df["Volume_count"].replace('\n','', regex=True)
df = df.astype({'Volume_count':'int'})
df["5_gram"] = df['5_gram'].str.lower()#.str.replace('[^\w\s]','')

df = df.astype({'5_gram':'str'})
df

df.to_csv('Subset.csv', index=False)

df.columns = ["5_gram", "Year", "Match_count", "Volume_count"] #Making the columns

df = df.astype({'Year':'int'}) #Setting years to integer

df = df.astype({'Match_count':'int'}) #Setting match_count to integer

df["Volume_count"] = df["Volume_count"].replace('\n','', regex=True) #replacing the useless \n in Volume_count
df = df.astype({'Volume_count':'int'}) #Volume count as integer

df["5_gram"] = df['5_gram'].str.lower()#.str.replace('[^\w\s]','') #Setting the Ngram to lowercase

df = df.astype({'5_gram':'str'}) #Setting the Ngrams to type string
df

import os
## Setting working directory:
os.chdir("/content/gdrive/My Drive/Thesis/Code/Pre-processing")

from nltk.tokenize import sent_tokenize
import nltk
nltk.download('punkt')

#Tokenize the dataset 1:
df.loc[:,"5_gram"].apply(lambda x: sent_tokenize(x)) #Use this lambda function to tokenize the data
#df

df["5_gram"] = df["5_gram"].str.replace('[^a-zA-Z]', " ") #This will delete all the non alphanumeric data
#df

## Making list of lists of words
#for every row in 5_gram, take every word and make list of lists. List1 is row, list2 is every words (5)

#df = df.reset_index() # make sure indexes pair with number of rows


list1 = []
list2 = []

for index, row in df.iterrows():
    list1.append(row["5_gram"])
    
for level in list1:
    seperated = level.split()
    list2.append(seperated)

for small in list2:
    for entry in small:
        if len(entry) < 3:
            small.remove(entry)

import nltk
from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')


#print(stopwords.words('english'))
postags = ["adv", "noun", "verb", "adj", "adp", "conj", "det", "pron", "end", "start"]

for bag in list2:
    for word in bag:
        if word in postags:
            bag.remove(word)
        if word in stopwords.words("english"):
            bag.remove(word)

with open('/content/gdrive/My Drive/Thesis/Data/Overig/moving_words.txt', 'r') as fp:
    moving = fp.readlines()

#Find the name of the column by index
n = df.columns[0]

# Drop that column
df.drop(n, axis = 1, inplace = True)

# Put whatever series you want in its place
df[n] = list2

eval_df = pd.read_csv("/content/gdrive/My Drive/Thesis/Data/Overig/One_evaluation_set.csv")

eval_wordlist = []
for index, row in eval_df.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    eval_wordlist.append(word1)
    eval_wordlist.append(word2)

new_list2 = []
names_present = []
counter = 0
for bag in list2:
    for word in bag:
        if word in names:
            new_list2.append(bag)
            names_present.append(word)
            counter += 1
        if word in moving:
          new_list2.append(bag)
        if word in dolgopolsky_list:
          new_list2.append(bag)
        else:
            if word in eval_wordlist:
                new_list2.append(bag)

names_present = list(set(names_present))
print(len(names_present))

print(len(new_list2))
print(counter)

for index, row in df.iterrows():
    print(index)
    text_list = row["5_gram"]
    if text_list in new_list2:
        pass
    else:
        df = df.drop(index=index)
print(len(df))

prctje = df.copy()
processed_inbetweendf = pd.DataFrame(prctje)

processed_inbetweendf.to_csv('processedinbetween.csv', index = False)

freqyear = df.copy()

#freqyear.drop("Year", axis=1, inplace=True)
freqyear.drop("Volume_count", axis=1, inplace= True)


#freqyear

## Splitting the dataset into two datasets:

freqyear = freqyear.astype({"Match_count" : "int"})

df_pre1949 = freqyear.copy()
df_pre2000 = freqyear.copy()

df_pre1949 = df_pre1949.reset_index(drop = True)
df_pre2000 = df_pre2000.reset_index(drop = True)

## Splitting the dataset into two datasets:
#
all_years = []

for i in range(len(df_pre1949)):
    year = df_pre1949.loc[i, "Year"]
    all_years.append(year)
    if year > 1949 and year < 1899:
        df_pre1949 = df_pre1949.drop((i))
        
for i in range(len(df_pre2000)):
    year = df_pre2000.loc[i, "Year"]
    all_years.append(year)
    if year < 1950 or year > 2000:
        df_pre2000 = df_pre2000.drop((i))
 

df_pre1949.to_csv('PRC_year_49.csv', index = False)
df_pre2000.to_csv("PRC_year_20.csv", index = False)
df_pre1949.drop("Year", axis=1, inplace=True)
df_pre2000.drop("Year", axis=1, inplace=True)

from itertools import repeat

total_freqs_2000 = []

for index, row in df_pre2000.iterrows():
    if row["5_gram"] in total_freqs_2000:
        bracket = [row["5_gram"]]
        total_freqs_2000.append(bracket * row["Match_count"])
    else:
        bracket = [row["5_gram"]]
        total_freqs_2000.append(bracket * row["Match_count"])
    

new_list_ngram_pre2000=[]


for brackets in total_freqs_2000:
    for bracket1 in brackets:
        new_list_ngram_pre2000.append(bracket1)

print(len(total_freqs_2000))
print(len(new_list_ngram_pre2000))

from itertools import repeat
total_freqs_1949 = []


for index, row in df_pre1949.iterrows():
    if row["5_gram"] in total_freqs_1949:
        bracket = [row["5_gram"]]
        total_freqs_1949.append(bracket * row["Match_count"])
    else:
        bracket = [row["5_gram"]]
        total_freqs_1949.append(bracket * row["Match_count"])
    
    

new_list_ngram_pre1949=[]

for brackets in total_freqs_1949:
    for bracket1 in brackets:
        new_list_ngram_pre1949.append(bracket1)

print(len(total_freqs_1949))
print(len(new_list_ngram_pre1949))

dataset_list_1949 = []
for smaller in new_list_ngram_pre1949:
    
    run =" ".join([str(item) for item in smaller])
    dataset_list_1949.append(run)

dataset_list_2000 = []
for smaller in new_list_ngram_pre2000:
    
    run =" ".join([str(item) for item in smaller])
    dataset_list_2000.append(run)

Processed1949 = pd.DataFrame(list(dataset_list_1949))

Processed2000 = pd.DataFrame(list(dataset_list_2000))

Processed1949.to_csv('Processed1949.csv', index = False)
Processed2000.to_csv("Processed2000.csv", index = False)

