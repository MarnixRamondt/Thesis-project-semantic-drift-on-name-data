# -*- coding: utf-8 -*-
"""Bert_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bxy1raiNR4W_Kv3F_F1o1CyL6GQuqyjI
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install torch
# !pip install transformers

from transformers import BertTokenizer, BertModel
import pandas as pd
import numpy as np
import nltk
import torch

# Loading the pre-trained BERT model
###################################
# Embeddings will be derived from
# the outputs of this model
model = BertModel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = True,
                                  )

# Setting up the tokenizer
###################################
# This is the same tokenizer that
# was used in the model to generate 
# embeddings to ensure consistency
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

try:
  from google.colab import drive
  IN_COLAB=True
except:
  IN_COLAB=False

if IN_COLAB:
  print("We're running Colab")

# Commented out IPython magic to ensure Python compatibility.
if IN_COLAB:
  # Mount the Google Drive at mount
  mount='/content/gdrive'
  print("Colab: mounting Google drive on ", mount)

  drive.mount(mount)

  # Switch to the directory on the Google Drive that you want to use
  import os
  drive_root = mount + "/My Drive/Thesis/Code/Descriptives"
  
  # Create drive_root if it doesn't exist
  create_drive_root = True
  if create_drive_root:
    print("\nColab: making sure ", drive_root, " exists.")
    os.makedirs(drive_root, exist_ok=True)
  
  # Change to the directory
  print("\nColab: Changing directory to ", drive_root)
#   %cd $drive_root

import pandas as pd

df_1949 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed1949.csv")
df_2000 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed2000.csv")

texts_list_2000 = []
for ind in df_2000.index:
     toevoegen = df_2000['0'][ind]
     texts_list_2000.append(toevoegen)
    

texts_list_1949 = []
for ind in df_1949.index:
     toevoegen = df_1949['0'][ind]
     texts_list_1949.append(toevoegen)

import io
import pandas as pd

#Processing the name dataset
    
with open('/content/gdrive/My Drive/Thesis/Data/Overig/Famous people.txt', 'r') as fp:
    nameslist = fp.readlines()
    
names = []
for name in nameslist:
    new = name.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    names.append(new6)

dfnames = pd.DataFrame(names)

#BERT uses a fixed-size window that limits the amount of
#text that can be input to the model at one time. The model maximum window size, or maximum sequence
#length, is fixed during pre-training, with 512 wordpieces a common choice

#loading the name data
#Processing the name dataset
    
with open('/content/gdrive/My Drive/Thesis/Data/Overig/1000Names.txt', 'r') as fp:
    nameslist = fp.readlines()
    
names = []
for name in nameslist:
    new = name.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    names.append(new6)
    

print(len(names))

# Text corpus
##############
# These sentences show the different
# forms of the word 'bank' to show the
# value of contextualized embeddings

texts_2000 = texts_list_2000
texts_1949 = texts_list_1949

def bert_text_preparation(text, tokenizer):
    """Preparing the input for BERT
    
    Takes a string argument and performs
    pre-processing like adding special tokens,
    tokenization, tokens to ids, and tokens to
    segment ids. All tokens are mapped to seg-
    ment id = 1.
    
    Args:
        text (str): Text to be converted
        tokenizer (obj): Tokenizer object
            to convert text into BERT-re-
            adable tokens and ids
        
    Returns:
        list: List of BERT-readable tokens
        obj: Torch tensor with token ids
        obj: Torch tensor segment ids
    
    
    """
    marked_text = "[CLS] " + text + " [SEP]"
    tokenized_text = tokenizer.tokenize(marked_text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    segments_ids = [1]*len(indexed_tokens)

    # Convert inputs to PyTorch tensors
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    return tokenized_text, tokens_tensor, segments_tensors
    
def get_bert_embeddings(tokens_tensor, segments_tensors, model):
    """Get embeddings from an embedding model
    
    Args:
        tokens_tensor (obj): Torch tensor size [n_tokens]
            with token ids for each token in text
        segments_tensors (obj): Torch tensor size [n_tokens]
            with segment ids for each token in text
        model (obj): Embedding model to generate embeddings
            from token and segment ids
    
    Returns:
        list: List of list of floats of size
            [n_tokens, n_embedding_dimensions]
            containing embeddings for each token
    
    """
    
    # Gradient calculation id disabled
    # Model is in inference mode
    with torch.no_grad():
        outputs = model(tokens_tensor, segments_tensors)
        # Removing the first hidden state
        # The first state is the input state
        hidden_states = outputs[2][1:]

    # Getting embeddings from the final BERT layer
    token_embeddings = hidden_states[-1]
    # Collapsing the tensor into 1-dimension
    token_embeddings = torch.squeeze(token_embeddings, dim=0)
    # Converting torchtensors to lists
    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]

    return list_token_embeddings

new_test_list_2000 = []
for text in texts_2000:
  new_test = bert_text_preparation(text, tokenizer)
  new_test_list_2000.append(new_test)


new_test_list_1949 = []
for text in texts_1949:
  new_test = bert_text_preparation(text, tokenizer)
  new_test_list_1949.append(new_test)

tokenized_text_list_2000 = []
tokens_tensor_list_2000 = []
segments_tensors_list_2000 = []
for text in texts_2000:
  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)
  tokenized_text_list_2000.append(tokenized_text)
  tokens_tensor_list_2000.append(tokens_tensor)
  segments_tensors_list_2000.append(segments_tensors)



tokenized_text_list_1949 = []
tokens_tensor_list_1949 = []
segments_tensors_list_1949 = []
for text in texts_1949:
  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)
  tokenized_text_list_1949.append(tokenized_text)
  tokens_tensor_list_1949.append(tokens_tensor)
  segments_tensors_list_1949.append(segments_tensors)

target_word_embeddings_2000 = []
name_embedding_list_2000 = []
tokenized_list_2000 = []
list_token_embeddings_2000 = []

for text in texts_2000:
  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)
  tokenized_list_2000.append(tokenized_text) 
  token_embeddings_2000 = get_bert_embeddings(tokens_tensor, segments_tensors, model)
  list_token_embeddings_2000.append(token_embeddings_2000)


print(len(tokenized_list_2000))



target_word_embeddings_1949 = []
name_embedding_list_1949 = []
tokenized_list_1949 = []
list_token_embeddings_1949 = []

for text in texts_1949:
  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)
  tokenized_list_1949.append(tokenized_text) 
  token_embeddings_1949 = get_bert_embeddings(tokens_tensor, segments_tensors, model)
  list_token_embeddings_1949.append(token_embeddings_1949)


print(len(tokenized_list_1949))

d = {'Tokens':tokenized_list_2000,'Word Embeddings':list_token_embeddings_2000}

emb_token_2000 =pd.DataFrame(d)

f = {'Tokens':tokenized_list_1949,'Word Embeddings':list_token_embeddings_1949}

emb_token_1949 =pd.DataFrame(f)

print(list_token_embeddings_1949[0])
print(len(list_token_embeddings_1949))
print(len(texts_1949))

##Making the wordlist:

words_2000 = []


for index, row in emb_token_2000.iterrows():
    sent = row["Tokens"]
    for word in sent:
      if word in words_2000:
        pass
      else:
        words_2000.append(word)


words_1949 = []


for index, row in emb_token_1949.iterrows():
    sent = row["Tokens"]
    for word in sent:
      if word in words_1949:
        pass
      else:
        words_1949.append(word)

print(len(words_2000))
print(len(words_1949))

results_df = pd.read_csv("/content/gdrive/My Drive/Thesis/Data/Overig/One_evaluation_set.csv")

#results_df

results_df = results_df.sort_values(by=['Rating'], ascending=False)

indexlist = []
for number in range(1, len(results_df) + 1):
    indexlist.append(number)
    
results_df["index"] = indexlist
print("length evalset before processing:", len(results_df))

setlist = []
for index, row in results_df.iterrows():
    temp_set = []
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    temp_set.append(word1)
    temp_set.append(word2)
    if temp_set in setlist:
        results_df = results_df.drop([index])
    else:
        setlist.append(temp_set)


print("length of evalset after processing:", len(results_df))

results_df[196:199]

eval_words = []
eval_sets = []
for index, row in results_df.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    set_list = []
    set_list.append(word1)
    set_list.append(word2)
    eval_sets.append(set_list)
    eval_words.append(word1)
    eval_words.append(word2)

from scipy.stats import spearmanr

spearmans_list_2000 = []
spearmans_set_list_2000 = []

for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in words_2000 and word2 in words_2000:
      for tokenlist in tokenized_list_2000:
        tokenlistje = []
        if word1 in tokenlist and word2 in tokenlist:
          for token in tokenlist:
            if token == word1:
              tkindex1 = tokenized_list_2000.index(tokenlist)
              word_index1 = tokenlist.index(word1)
              word_embeddings1 = list_token_embeddings_2000[tkindex1][word_index1]
              array1 = np.array(word_embeddings1)
              tokenlistje.append(array1)
            if token == word2:
              tkindex2 = tokenized_list_2000.index(tokenlist)
              word_index2 = tokenlist.index(word2)
              word_embeddings2 = list_token_embeddings_2000[tkindex2][word_index2]
              array2 = np.array(word_embeddings2)
              tokenlistje.append(array2)
            else:
              pass

        #for array in tokenlistje:
          #print("array in tokenlistje", array)
          #print("length of tokenlistje", len(array))
          #first = array[0]
          #second = array[1]
          df = pd.DataFrame(array1)
          df2 = pd.DataFrame(array2)
          #df = df.T
          #df2 = df2.T
          rho, p = spearmanr(df, df2)
          result_list = []
          result_list.append(word1)
          result_list.append(word2)
          result_list.append(float(rho))
          spearmans_set_list_2000.append(result_list)
          spearmans_list_2000.append(rho)

            
      
      
      
          #word_index1 = word1.index(tk)
          #word_embedding1 = list_token_embeddings[0][word_index1].reshape((1,100))
          #array1 = np.array(word_embedding1)

spearmans_list_1949 = []
spearmans_set_list_1949 = []

for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in words_1949 and word2 in words_1949:
      for tokenlist in tokenized_list_1949:
        tokenlistje = []
        if word1 in tokenlist and word2 in tokenlist:
          for token in tokenlist:
            if token == word1:
              tkindex1 = tokenized_list_1949.index(tokenlist)
              word_index1 = tokenlist.index(word1)
              word_embeddings1 = list_token_embeddings_1949[tkindex1][word_index1]
              array1 = np.array(word_embeddings1)
              tokenlistje.append(array1)
            if token == word2:
              tkindex2 = tokenized_list_1949.index(tokenlist)
              word_index2 = tokenlist.index(word2)
              word_embeddings2 = list_token_embeddings_1949[tkindex2][word_index2]
              array2 = np.array(word_embeddings2)
              tokenlistje.append(array2)
            else:
              pass

        #for array in tokenlistje:
          #print("array in tokenlistje", array)
          #print("length of tokenlistje", len(array))
          #first = array[0]
          #second = array[1]
          df = pd.DataFrame(array1)
          df2 = pd.DataFrame(array2)
          #df = df.T
          #df2 = df2.T
          rho, p = spearmanr(df, df2)
          result_list = []
          result_list.append(word1)
          result_list.append(word2)
          result_list.append(float(rho))
          spearmans_set_list_1949.append(result_list)
          spearmans_list_1949.append(rho)

            
      
      
      
          #word_index1 = word1.index(tk)
          #word_embedding1 = list_token_embeddings[0][word_index1].reshape((1,100))
          #array1 = np.array(word_embedding1)

#print(len(spearmans_set_list_2000))

if len(spearmans_set_list_2000) > 0:
  spearmansdf_2000 = pd.DataFrame(spearmans_set_list_2000)
  spearmansdf_2000.columns = ["Word 1", "Word 2", "Spearman"]

  spearmansdf_2000 = spearmansdf_2000.sort_values(by=['Spearman'], ascending=False)

  spearmansdf_2000 = spearmansdf_2000.drop_duplicates(keep='first')


  indexlist = []
  for number in range(1, len(spearmansdf_2000) + 1):
    indexlist.append(number)
    
  spearmansdf_2000["index"] = indexlist
  #spearmansdf_2000

if len(spearmans_set_list_1949) > 0:
  spearmansdf_1949 = pd.DataFrame(spearmans_set_list_1949)
  spearmansdf_1949.columns = ["Word 1", "Word 2", "Spearman"]

  spearmansdf_1949 = spearmansdf_1949.sort_values(by=['Spearman'], ascending=False)

  spearmansdf_1949 = spearmansdf_1949.drop_duplicates(keep='first')


  indexlist = []
  for number in range(1, len(spearmansdf_1949) + 1):
    indexlist.append(number)
    
  spearmansdf_1949["index"] = indexlist
  #spearmansdf_1949

temp_list_res_2000 = []
temp_list_2000 = []
instances_list_2000 = []

for index, row in spearmansdf_2000.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_2000.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_2000.append(help_list)

for instance in temp_list_2000:
    #print(instance)
    for instance2 in temp_list_res_2000:
        #print(instance2)
        if instance == instance2:
            instances_list_2000.append(instance)
            
            
           
#instances_list_2000

temp_list_res_1949 = []
temp_list_1949 = []
instances_list_1949 = []

for index, row in spearmansdf_1949.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_1949.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_1949.append(help_list)

for instance in temp_list_1949:
    #print(instance)
    for instance2 in temp_list_res_1949:
        #print(instance2)
        if instance == instance2:
            instances_list_1949.append(instance)
            
            
           
#instances_list_1949

results_list_2000 = []
spearmans_list_2000 = []


for instance in instances_list_2000:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_2000.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_2000:
                    pass
                else:
                    spearmans_list_2000.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
                if index_res in results_list_2000:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_2000.append(index_res)
                tracker_2.clear()

results_list_1949 = []
spearmans_list_1949 = []


for instance in instances_list_1949:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_1949.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_1949:
                    pass
                else:
                    spearmans_list_1949.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
                if index_res in results_list_1949:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_1949.append(index_res)
                tracker_2.clear()

results_list_2000

spearmans_list_2000

instances_list_2000

spearmans_ranking_df_2000 = pd.DataFrame()
spearmans_ranking_df_2000["Eval index"] = results_list_2000[0:8]
spearmans_ranking_df_2000["Embedding index"] = spearmans_list_2000[0:8]
spearmans_ranking_df_2000["Words"] = instances_list_2000[0:8]
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
#spearmans_ranking_df_2000


spearmans_ranking_df_1949 = pd.DataFrame()
spearmans_ranking_df_1949["Eval index"] = results_list_1949
spearmans_ranking_df_1949["Embedding index"] = spearmans_list_1949
spearmans_ranking_df_1949["Words"] = instances_list_1949
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
#spearmans_ranking_df_1949

spearmans_ranking_df_1949

spearmans_ranking_df_2000

from sklearn.metrics import ndcg_score

my_r_1949 = spearmans_ranking_df_1949.corr(method="spearman")
spearmans_rank_correlation_df = pd.DataFrame()
spearmans_rank_correlation_df["SPR1949"] = [my_r_1949]

my_r_2000 = spearmans_ranking_df_2000.corr(method="spearman")
spearmans_rank_correlation_df["SPR2000"] = [my_r_2000]

dsgeval_1949 = list([spearmans_list_1949])
dsgemb_1949 = list([results_list_1949])

dcgresults_1949 = ndcg_score(dsgeval_1949, dsgemb_1949)

spearmans_rank_correlation_df["DCG1949"] = dcgresults_1949

dsgeval_2000 = list([spearmans_list_2000[0:8]])
dsgemb_2000 = list([results_list_2000])

dcgresults_2000 = ndcg_score(dsgeval_2000, dsgemb_2000)

spearmans_rank_correlation_df["DCG2000"] = dcgresults_2000



spearmans_rank_correlation_df.to_csv("/content/gdrive/My Drive/Thesis/Results/BERT/BERTDCGSPEARMAN.csv", index = False)



from scipy.spatial.distance import cosine

# Calculating the distance between the
# embeddings of 'bank' in all the
# given contexts of the word

list_of_distances = []
for text1, embed1 in zip(texts_2000, target_word_embeddings_2000):
    for text2, embed2 in zip(texts_1949, target_word_embeddings_1949):
        cos_dist = 1 - cosine(embed1, embed2)
        list_of_distances.append([text1, text2, cos_dist])


print(list_of_distances[0:100])

distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])

combinations = []

for word in words_1949:
    if word in words_2000:
        combinations.append(word)
  

## looking at words that appear both in 1949 and 2000
combinations_vectors_1949 = []
combinations_vectors_2000 = []

track_1949 = []
track_2000 = []

test_list = []


for word in combinations:
    for tokenlist in tokenized_list_1949:
      if word in tokenlist:
        for token in tokenlist:
          if token == word:
            if word not in track_1949:
              tkindex1 = tokenized_list_1949.index(tokenlist)
              word_index1 = tokenlist.index(word)
              word_embeddings1 = list_token_embeddings_1949[tkindex1][word_index1]
              combinations_vectors_1949.append(word_embeddings1)
              lsitje = [word, word_embeddings1[0]]
              test_list.append(lsitje)
              track_1949.append(word)
            else:
              pass
          else:
            pass




    for tokenlist2 in tokenized_list_2000:
      if word in tokenlist2:
        for token2 in tokenlist2:
          if token2 == word:
            if word not in track_2000:
              tkindex2 = tokenized_list_2000.index(tokenlist2)
              word_index2 = tokenlist2.index(word)
              word_embeddings2 = list_token_embeddings_2000[tkindex2][word_index2]
              combinations_vectors_2000.append(word_embeddings2)
              lsitje2 = [word, word_embeddings2[0] ]
              test_list.append(lsitje2)
              track_2000.append(word)
            else:
              pass
          else:
            pass


        
      else:
        pass

print(len(combinations))
print(len(combinations_vectors_1949))
print(len(combinations_vectors_2000))

print(combinations_vectors_2000[0][1])
print(combinations_vectors_1949[0][1])

myset = set(combinations)
#print(len(myset))

from scipy import spatial

combinationsdf = pd.DataFrame(combinations)
combinationsdf.columns = ["Words"]


simlist = []
counter = 0
for vector in combinations_vectors_1949:
  word = combinations[counter]
  sim = spatial.distance.cosine(vector, combinations_vectors_2000[counter])
  simlist.append(sim)
  counter += 1

combinationsdf["sim between 1949 & 2000"] = simlist

combinationsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/BERT/wordsimbetweentimeBERT.csv', index = False)

simarray = np.array(simlist)
sum = np.sum(simarray)
avg = np.mean(simarray)

nearest_neighbors_2000 = []
nearest_neighbors_words_2000 = []

for word in combinations:
    wordindex = combinations.index(word)
    wordvec = combinations_vectors_2000[wordindex]

    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    part1 = []
    part2 = []
    others = [x for x in combinations if x != word]

    for other in others:
        otherindex = combinations.index(other)
        othervec = combinations_vectors_2000[otherindex]
        othervecs.append(othervec)
        part1.append(other)
    
    
    for value in othervec:
        sim = spatial.distance.cosine(wordvec, value)
        sims.append(sim)
        part2.append(sim)



    counter = 0
    for i in part1:
      newsetlist = [i, part2[counter]]
      setlist.append(newsetlist)
      counter += 1
      
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_2000.append(setje)

    for n in near:
      for s in setlist:
        nmr = s[1]
        for m in n:
            if m == nmr:
              if s[0] not in neigbor_words:
                 neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_2000.append(makeset)
    
        
            #nearest_neighbors.append(sim)

#nearest_neighbors_words_2000[1]

nearest_neighbors_1949 = []
nearest_neighbors_words_1949 = []

for word in combinations:
    wordindex = combinations.index(word)
    wordvec = combinations_vectors_1949[wordindex]

    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    part1 = []
    part2 = []
    others = [x for x in combinations if x != word]

    for other in others:
        otherindex = combinations.index(other)
        othervec = combinations_vectors_1949[otherindex]
        othervecs.append(othervec)
        part1.append(other)
    
    
    for value in othervec:
        sim = spatial.distance.cosine(wordvec, value)
        sims.append(sim)
        part2.append(sim)



    counter = 0
    for i in part1:
      newsetlist = [i, part2[counter]]
      setlist.append(newsetlist)
      counter += 1
      
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_1949.append(setje)

    for n in near:
      for s in setlist:
        nmr = s[1]
        for m in n:
            if m == nmr:
              if s[0] not in neigbor_words:
                 neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_1949.append(makeset)
    
        
            #nearest_neighbors.append(sim)

#nearest_neighbors_words_1949[1]

nn_df_2000 = pd.DataFrame(nearest_neighbors_2000)
nn_df_2000_2 = pd.DataFrame(nearest_neighbors_words_2000)
nn_df_2000.to_csv('/content/gdrive/My Drive/Thesis/Results/BERT/nn_df_2000_BERT.csv', index = False)
nn_df_2000_2.to_csv("/content/gdrive/My Drive/Thesis/Results/BERT/nn_df_2000_words_BERT.csv", index = False)

nn_df_1949 = pd.DataFrame(nearest_neighbors_1949)
nn_df_1949_2 = pd.DataFrame(nearest_neighbors_words_1949)
nn_df_1949.to_csv('/content/gdrive/My Drive/Thesis/Results/BERT/nn_df_2000_BERT.csv', index = False)
nn_df_1949_2.to_csv("/content/gdrive/My Drive/Thesis/Results/BERT/nn_df_2000_words_BERT.csv", index = False)

def linear_align(base_embed, other_embed):
    """
        Align other embedding to base embedding using best linear transform.
        NOTE: Assumes indices are aligned
    """
    basevecs = base_embed
    othervecs = other_embed
    fixedvecs = othervecs.dot(np.linalg.pinv(othervecs)).dot(basevecs)
    return fixedvecs

counter = 0
aligned = []
for embedding in combinations_vectors_1949:
  align = linear_align(np.array(embedding).reshape(np.array(embedding).shape[0], 1), np.array(combinations_vectors_2000[counter]).reshape(np.array(combinations_vectors_2000[counter]).shape[0], 1))
  counter += 1
  aligned.append(align)

from scipy.spatial import distance
counter = 0

distances = []

for wordje in combinations:
    dist = distance.cosine(aligned[counter], combinations_vectors_2000[counter])
    distances.append(dist)
    counter +=1

resultscos = pd.DataFrame(distances, combinations)
resultscos
resultscos.to_csv("/content/gdrive/My Drive/Thesis/Results/BERT/resultscos.csv", index = False)



"""**Aligning the vector spaces**"""

with open('/content/gdrive/My Drive/Thesis/Data/Overig/Dolgopolsky.txt', 'r') as fp:
    dolgolist = fp.readlines()
    
dolgopolsky_list = []
for dol in dolgolist:
    new = dol.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    dolgopolsky_list.append(new6)
    

print(len(dolgopolsky_list))

print(len(combinations_vectors_1949))
print(len(combinations))
print(len(combinations_vectors_2000))

def smart_procrustes_align(base_embed, other_embed, post_normalize=True):
    base_embed = np.array(base_embed)
    base_embed = base_embed.reshape(base_embed.shape[0], 1)
    other_embed = np.array(other_embed)
    other_embed = other_embed.reshape(other_embed.shape[0], 1)
    
    m = other_embed.T.dot(base_embed)
    u, _, v = np.linalg.svd(m) 
    ortho = u.dot(v)
    final = other_embed.dot(ortho)
    return final

alinged0 = smart_procrustes_align(combinations_vectors_1949[0], combinations_vectors_2000[0])

from scipy import spatial
sim = 1 - spatial.distance.cosine(combinations_vectors_1949[0], alinged0)
print(sim)
print("word:", combinations[0])

from scipy import spatial

counter = 0
alinged_drift_results = {}
alinged_vecs = []

for word in combinations:
  aligned = smart_procrustes_align(combinations_vectors_1949[counter], combinations_vectors_2000[counter])
  dist  =  1 - spatial.distance.cosine(combinations_vectors_1949[counter], aligned)
  alinged_vecs.append(aligned)
  if word in alinged_drift_results:
    pass
  else:
    if combinations_vectors_1949[counter] != combinations_vectors_2000[counter]:
      alinged_drift_results[word] = dist
    counter += 1

alinged_drift_results_values = list(alinged_drift_results.values())

print(len(alinged_drift_results_values))

alinged_drift_results_values = [x for x in alinged_drift_results_values if x != 1]
alinged_drift_results_values = alinged_drift_results_values + alinged_drift_results_values

print(len(alinged_drift_results_values))

data_dict = alinged_drift_results
data_items = data_dict.items()
data_list = list(data_items)
aligned_resultsdf = pd.DataFrame(data_list)
average_token_drift_aligned = round(aligned_resultsdf[1].mean(),3)
print("token:", average_token_drift_aligned)
name_results_aligned = np.array(name_results_aligned)
print("name:", np.mean(name_results_aligned))
aligned_resultsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/BERT/Aligned_drift_results_BERT.csv', index = False)

name_results_aligned = []
overview_aligned = []
counter = 0
for word in combinations:
  if word in names:
    drift = aligned_resultsdf.iloc[counter][1]
    setje = [word, drift]
    overview_aligned.append(setje)
    name_results_aligned.append(drift)
    counter += 1

print(len(name_results_aligned))

name_results_aligned = name_results_aligned[0:29]

# Import the libraries
import matplotlib.pyplot as plt
import seaborn as sns

alinged_drift_results_values = np.array(alinged_drift_results_values)
name_results_aligned = np.array(name_results_aligned)

# seaborn histogram
sns.distplot(alinged_drift_results_values, hist=True, kde=False, color = 'gray',
             hist_kws={'edgecolor':'black'})

sns.distplot(name_results_aligned, hist=True, kde=False, color = 'black',
             hist_kws={'edgecolor':'black'})


# Add labels
plt.title('Semantic drift measured with aligned vector spaces BERT')
plt.xlabel('Semantic drift (cosine distance)')
plt.ylabel('Frequency')
ax = plt.gca()

plt.axvline(x=alinged_drift_results_values.mean(),
            color='red', linestyle = 'dashed')
plt.axvline(x=name_results_aligned.mean(),
            color = "blue")
plt.legend( ["General Mean", "Names mean", 'All word types', "Historical names"])


plt.savefig('/content/gdrive/My Drive/Thesis/Results/BERT/semantic_drict_dist_everythingBERT.png')

dolgo_aligned_results = []
dolgo_overview = []
counter = 0
for word in combinations:
  if word in dolgopolsky_list:
    drift = aligned_resultsdf.iloc[counter][1]
    setje = [word, drift]
    dolgo_overview.append(setje)
    dolgo_aligned_results.append(drift)
    counter += 1

dolgo_overview

######################

print(len(combinations))
print(len(combinations_vectors_1949))
print(len(combinations_vectors_2000))

combinations_vectors_1949arr = np.array(combinations_vectors_1949)
combinations_vectors_2000arr = np.array(combinations_vectors_2000)
combinations_vectors_1949 = combinations_vectors_1949
combinations_vectors_2000 = combinations_vectors_2000


NN_result_list_49 = []
for vector in combinations_vectors_1949:
  idx = combinations_vectors_1949.index(vector)
  target = vector
  sim_list = []
  for embed in combinations_vectors_1949:
    result = 1 - spatial.distance.cosine(target, embed)
    sim_list.append(result)
  target_word = combinations[idx]
  target_and_list = [target_word, sim_list]
  NN_result_list_49.append(target_and_list)

combinations_vectors_1949arr = np.array(combinations_vectors_1949)
combinations_vectors_2000arr = np.array(combinations_vectors_2000)
combinations_vectors_1949 = combinations_vectors_1949
combinations_vectors_2000 = combinations_vectors_2000


NN_result_list_20 = []
for vector in combinations_vectors_2000:
  idx = combinations_vectors_2000.index(vector)
  target = vector
  sim_list = []
  for embed in combinations_vectors_2000:
    result = 1 - spatial.distance.cosine(target, embed)
    sim_list.append(result)
  target_word = combinations[idx]
  target_and_list = [target_word, sim_list]
  NN_result_list_20.append(target_and_list)

print(len(NN_result_list_49[0][1]))
print(len(NN_result_list_20[0][1]))
print(len(NN_result_list_49_final[0]))

print(NN_result_list_49[0])
print(NN_result_list_49[1])

NN_result_list_49_final = []

for NN_result in NN_result_list_49:
  target = NN_result[0]
  alter = NN_result[1]
  new_resultlist = [x for x in alter if x > 0.1]
  new_resultlist = new_resultlist + new_resultlist
  new_result_listappend = [target, new_resultlist]
  NN_result_list_49_final.append(new_result_listappend)

NN_result_list_20_final = []

for NN_result in NN_result_list_20:
  target = NN_result[0]
  alter = NN_result[1]
  new_resultlist = [x for x in alter if x > 0.1]
  new_resultlist = new_resultlist + new_resultlist
  new_result_listappend = [target, new_resultlist]
  NN_result_list_20_final.append(new_result_listappend)

print(NN_result_list_49_final[0])

#NN_result_list_49[0]
NN_list_words_1949 = []

for bijna in NN_result_list_49_final:
  sims = bijna[1]
  sortedsims = sorted(sims)
  index_list = []
  combinations_list = []
  for i in sortedsims:
    if len(index_list) < 256:
      idx = sims.index(i)
      index_list.append(idx)
    else:
      pass
  for idxjes in index_list:
    combinations_list.append(combinations[idxjes])
  NN_list_words_1949.append(combinations_list)

#NN_result_list_49[0]
NN_list_words_2000 = []

for bijna in NN_result_list_20_final:
  sims = bijna[1]
  sortedsims = sorted(sims)
  index_list = []
  combinations_list = []
  for i in sortedsims:
    if len(index_list) < 256:
      idx = sims.index(i)
      index_list.append(idx)
    else:
      pass
  for idxjes in index_list:
    combinations_list.append(combinations[idxjes])
  NN_list_words_2000.append(combinations_list)

NN_results = {}
counter = 0
for word in combinations:
  firstlist = NN_list_words_1949[counter]
  secondlist = NN_list_words_2000[counter]
  thirdlist = len(set(firstlist)&set(secondlist))
  precentage = thirdlist / (len(firstlist))
  drift = 1 - precentage
  if word in NN_results:
    pass
  else:
    NN_results[word] = drift
  counter += 1

data_dict = NN_results
data_items = data_dict.items()
data_list = list(data_items)
NN_resultsdf = pd.DataFrame(data_list)
average_token_drift_NN = NN_resultsdf[1].mean()
print(average_token_drift_NN)
NN_resultsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/BERT/NN_drift_results_BERT.csv', index = False)

overall_drift_NN = np.array(NN_resultsdf[1])

name_results_NN = []
overview_NN = []
counter = 0
for word in combinations:
  if word in names:
    drift = NN_resultsdf.iloc[counter][1]
    setje = [word, drift]
    overview_NN.append(setje)
    name_results_NN.append(drift)
    counter += 1

overview_NN

print(name_results_NN)
print(len(name_results_NN))
namedriftarray_NN = np.array(name_results_NN)
avg_name_drift_NN = np.mean(namedriftarray_NN)
print(avg_name_drift_NN)

# Import the libraries
import matplotlib.pyplot as plt
import seaborn as sns

overall_drift_NN = np.array(NN_resultsdf[1][0:239])
namedriftarray_NN = np.array(name_results_NN[28:50])

# seaborn histogram
sns.distplot(overall_drift_NN, hist=True, kde=False, color = 'gray',
             hist_kws={'edgecolor':'black'})

sns.distplot(namedriftarray_NN, hist=True, kde=False, color = 'black',
             hist_kws={'edgecolor':'black'})


# Add labels
plt.title('Semantic drift Measured with nearest neighbors BERT')
plt.xlabel('Semantic drift (nearest neighbors)')
plt.ylabel('Frequency')
plt.axvline(x=average_token_drift_NN,
            color='red', linestyle = 'dashed')
plt.axvline(x=avg_name_drift_NN,
            color = "blue")
plt.legend( ["General Mean", "Names mean", 'All word types', "Historical names"], loc='upper left')


plt.savefig('/content/gdrive/My Drive/Thesis/Results/BERT/semantic_drict_dist_everythingBERTNN.png')

dolgo_aligned_results = []
dolgo_overview = []
counter = 0
for word in combinations:
  if word in dolgopolsky_list:
    drift = NN_resultsdf.iloc[counter][1]
    setje = [word, drift]
    dolgo_overview.append(setje)
    dolgo_aligned_results.append(drift)
    counter += 1

dolgo_overview

######################

## Nearest neighbours per word:
neigbours_1949 = []

for word in combinations:
  nearest = list(sg_model_1949.wv.most_similar(word, topn = 250))
  nearset = [word, nearest]
  neigbours_1949.append(nearset)

neigbours_2000 = []

for word in combinations:
  nearest = list(sg_model_2000.wv.most_similar(word, topn = 250))
  nearset = [word, nearest]
  neigbours_2000.append(nearset)

neighborlist_2000 = []
for neigbors in neigbours_2000:
  for  n in neigbors:
    counter = 0
    for near in n:
      if len(near) > 1:
        neighborlist_2000.append(near[0])

neighborlist_1949 = []
for neigbors in neigbours_1949:
  neighbors = []
  for n in neigbors:
    counter = 0
    for near in n:
      if len(near) > 1:
        neighbors.append(list(near[0]))
  neighborlist_1949.append(neighbors)


neighbor_1949_complete = []
for entry in neighborlist_1949:
  appendedletters_1949 = []
  for letter in entry:
    new = "".join(letter)
    appendedletters_1949.append(new)
  neighbor_1949_complete.append(appendedletters_1949)

neighborlist_2000 = []
for neigbors in neigbours_2000:
  neighbors = []
  for n in neigbors:
    counter = 0
    for near in n:
      if len(near) > 1:
        neighbors.append(list(near[0]))
  neighborlist_2000.append(neighbors)


neighbor_2000_complete = []
for entry in neighborlist_2000:
  appendedletters_2000 = []
  for letter in entry:
    new = "".join(letter)
    appendedletters_2000.append(new)
  neighbor_2000_complete.append(appendedletters_2000)

print(len(neighborlist_2000))
print(len(neighbor_1949_complete))
print(len(combinations))

print(combinations_vectors_1949[0][1])
print(combinations_vectors_2000[0][1])

combinations_vectors_1949arr = []
combinations_vectors_2000arr = []


for i in combinations_vectors_1949:
  new_i = np.array(i)
  combinations_vectors_1949arr.append(new_i)


for y in combinations_vectors_2000:
  new_y = np.array(y)
  combinations_vectors_2000arr.append(new_y)

combinations_vectors_1949arr = np.array(combinations_vectors_1949arr)
combinations_vectors_2000arr = np.array(combinations_vectors_2000arr)

combinations_vectors_1949arr[0].shape

combinations_vectors_2000arr[0].shape

print(combinations_vectors_1949arr[0][1])
print(combinations_vectors_2000arr[0][1])

from scipy import spatial

counter = 0
alinged_drift_results = {}

for word in combinations:
  aligned = smart_procrustes_align(combinations_vectors_1949arr[counter].reshape(combinations_vectors_1949arr[counter].shape[0], 1), combinations_vectors_2000arr[counter].reshape(combinations_vectors_2000arr[counter].shape[0], 1))
  dist  =  spatial.distance.cosine(combinations_vectors_2000arr[counter].reshape(combinations_vectors_2000arr[counter].shape[0], 1), aligned)
  if word in alinged_drift_results:
    pass
  else:
    alinged_drift_results[word] = dist
  counter += 1

alinged_drift_results

data_dict = alinged_drift_results
data_items = data_dict.items()
data_list = list(data_items)
aligned_resultsdf = pd.DataFrame(data_list)
average_token_drift_aligned = round(aligned_resultsdf[1].mean(),3)
print(average_token_drift_aligned)
aligned_resultsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/BERT/Aligned_drift_results_BERT.csv', index = False)



"""**Nearest Neighbor Technique**"""

neighborlist_2000 = []
for neigbors in neigbours_2000:
  neighbors = []
  for n in neigbors:
    counter = 0
    for near in n:
      if len(near) > 1:
        neighbors.append(list(near[0]))
  neighborlist_2000.append(neighbors)


neighbor_2000_complete = []
for entry in neighborlist_2000:
  appendedletters_2000 = []
  for letter in entry:
    new = "".join(letter)
    appendedletters_2000.append(new)
  neighbor_2000_complete.append(appendedletters_2000)

print(len(neighborlist_2000))
print(len(neighbor_1949_complete))
print(len(combinations))

NN_results = {}
counter = 0
for word in combinations:
  firstlist = neighbor_1949_complete[counter]
  secondlist = neighbor_2000_complete[counter]
  thirdlist = len(set(firstlist)&set(secondlist))
  precentage = thirdlist / (len(firstlist))
  drift = 1 - precentage
  if word in NN_results:
    pass
  else:
    NN_results[word] = drift
  counter += 1

data_dict = NN_results
data_items = data_dict.items()
data_list = list(data_items)
NN_resultsdf = pd.DataFrame(data_list)
average_token_drift_NN = NN_resultsdf[1].mean()
print(average_token_drift_NN)
NN_resultsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/NN_drift_results_W2V.csv', index = False)

name_results_NN = []
overview_NN = []
counter = 0
for word in combinations:
  if word in names:
    drift = NN_resultsdf.iloc[counter][1]
    setje = [word, drift]
    overview_NN.append(setje)
    name_results_NN.append(drift)
    counter += 1

overview_NN

print(name_results_NN)
print(len(name_results_NN))
namedriftarray_NN = np.array(name_results_NN)
avg_name_drift_NN = np.mean(namedriftarray_NN)
print(avg_name_drift_NN)

print(len(tokenized_list_1949))
print(len(list_token_embeddings_1949))

simalarityaligned_list = []

for vector in aligned:
  othervecs = aligned
  othersims = []
  wordpairslist = []
  counter = 0
  for other in othervecs:
    sim = spatial.distance.cosine(vector, other)
    othersims.append(sim)

  simalarityaligned_list.append(othersims)

print(len(simalarityaligned_list))
print(len(combinations))

nearest_neighbors_aligned = []
nearest_neighbors_words_aligned = []

for word in combinations:
    wordindex = combinations.index(word)
    wordvec = aligned[wordindex]

    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    part1 = []
    part2 = []
    others = [x for x in combinations if x != word]

    for other in others:
        otherindex = combinations.index(other)
        othervec = aligned[otherindex]
        othervecs.append(othervec)
        part1.append(other)
    
    
    for value in othervec:
        sim = spatial.distance.cosine(wordvec, value)
        sims.append(sim)
        part2.append(sim)



    counter = 0
    for i in part1:
      newsetlist = [i, part2[counter]]
      setlist.append(newsetlist)
      counter += 1
      
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_aligned.append(setje)

    for n in near:
      for s in setlist:
        nmr = s[1]
        for m in n:
            if m == nmr:
              if s[0] not in neigbor_words:
                 neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_aligned.append(makeset)
    
        
            #nearest_neighbors.append(sim)

!pip install bert-embedding mxnet-cu100

import mxnet as mx
from bert_embedding import BertEmbedding

from scipy.spatial.distance import cosine

# Calculating the distance between the
# embeddings of 'bank' in all the
# given contexts of the word

list_of_distances = []
for tokenized_textje, embed1 in zip(tokenized_text_list_1949, embeddingtargets):
    for text2, embed2 in zip(tokenized_text_list_1949, embeddingtargets):
        cos_dist = 1 - cosine(embed1, embed2)
        list_of_distances.append([text1, text2, cos_dist])

distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])

!pip install pytorch-pretrained-bert

# Commented out IPython magic to ensure Python compatibility.
import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM

# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows
import logging
#logging.basicConfig(level=logging.INFO)

import matplotlib.pyplot as plt
# % matplotlib inline

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokenized_sentences = []

for text in texts_1949:
  marked_text = "[CLS] " + text + " [SEP]"

  # Tokenize our sentence with the BERT tokenizer.
  tokenized_text = tokenizer.tokenize(marked_text)
  tokenized_sentences.append(tokenized_text)

list(tokenizer.vocab.keys())[5000:5020]

tokenized_text_list_2 = []
indexed_tokens_list = []

for text in texts_1949:

  # Add the special tokens.
  marked_text = "[CLS] " + text + " [SEP]"

  # Split the sentence into tokens.
  tokenized_text = tokenizer.tokenize(marked_text)
  tokenized_text_list_2.append(tokenized_text)

  # Map the token strings to their vocabulary indeces.
  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
  indexed_tokens_list.append(indexed_tokens)

# Mark each of the 22 tokens as belonging to sentence "1".
segments_ids_list = []
counter = 1
for tokenize in tokenized_text_list_2:
  segments_ids = [counter] * len(tokenize)
  segments_ids_list.append(segments_ids)
  counter += 1

print(len(segments_ids_list))
print(len(tokenized_text_list_2))

# Convert inputs to PyTorch tensors
tokens_tensor_list = []
segments_tensor_list = []

for index in indexed_tokens_list:
  tokens_tensor = torch.tensor([indexed_tokens])
  tokens_tensor_list.append(tokens_tensor)

for segment in segments_ids_list:
  segments_tensors = torch.tensor([segments_ids])
  segments_tensor_list.append(segments_tensors)

# Load pre-trained model (weights)
model = BertModel.from_pretrained('bert-base-uncased')

# Put the model in "evaluation" mode, meaning feed-forward operation.
model.eval()



# Predict hidden states features for each layer
encoded_layers_list = []
counter = 0
with torch.no_grad():
  for token in tokens_tensor_list:
    encoded_layers = model(token, segments_tensor_list[counter])
    encoded_layers_list.append(encoded_layers)
    counter += 1

prencoded_layers_list

# Concatenate the tensors for all layers. We use `stack` here to
# create a new dimension in the tensor.
token_embeddings_list = []
for encode in encoded_layers_list:
  token_embeddings = torch.stack(encode[0][0], dim=0)
  token_embeddings_list.append(token_embeddings)

# Remove dimension 1, the "batches".
token_embeddings = torch.squeeze(token_embeddings, dim=1)

token_embeddings.size()

# Swap dimensions 0 and 1.
token_embeddings = token_embeddings.permute(1,0,2)

token_embeddings.size()

# Stores the token vectors, with shape [22 x 3,072]
token_vecs_cat = []

# `token_embeddings` is a [22 x 12 x 768] tensor.

# For each token in the sentence...
for token in token_embeddings:
    
    # `token` is a [12 x 768] tensor

    # Concatenate the vectors (that is, append them together) from the last 
    # four layers.
    # Each layer vector is 768 values, so `cat_vec` is length 3,072.
    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)
    
    # Use `cat_vec` to represent `token`.
    token_vecs_cat.append(cat_vec)

print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))

# Stores the token vectors, with shape [22 x 768]
token_vecs_sum = []

# `token_embeddings` is a [22 x 12 x 768] tensor.

# For each token in the sentence...
for token in token_embeddings:

    # `token` is a [12 x 768] tensor

    # Sum the vectors from the last four layers.
    sum_vec = torch.sum(token[-4:], dim=0)
    
    # Use `sum_vec` to represent `token`.
    token_vecs_sum.append(sum_vec)

print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))

# `encoded_layers` has shape [12 x 1 x 22 x 768]

# `token_vecs` is a tensor with shape [22 x 768]
token_vecs = encoded_layers[11][0]

# Calculate the average of all 22 token vectors.
sentence_embedding = torch.mean(token_vecs, dim=0)

print ("Our final sentence embedding vector of shape:", sentence_embedding.size())

for i, token_str in enumerate(tokenized_text):
  print (i, token_str)

print('First 5 vector values for each instance of "bank".')
print('')
print("bank vault   ", str(token_vecs_sum[6][:5]))
print("bank robber  ", str(token_vecs_sum[10][:5]))
print("river bank   ", str(token_vecs_sum[19][:5]))

from scipy.spatial.distance import cosine

# Calculate the cosine similarity between the word bank 
# in "bank robber" vs "river bank" (different meanings).
diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])

# Calculate the cosine similarity between the word bank
# in "bank robber" vs "bank vault" (same meaning).
same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])

print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)
print('Vector similarity for *different* meanings:  %.2f' % diff_bank)

"""ALIGNED AND NN RESULTS // ROBUSTNESS CHECK"""

with open('/content/gdrive/My Drive/Thesis/Data/Overig/Dolgopolsky.txt', 'r') as fp:
    dolgolist = fp.readlines()
    
dolgopolsky_list = []
for dol in dolgolist:
    new = dol.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    dolgopolsky_list.append(new6)
    

print(len(dolgopolsky_list))

#Negative values analysis:
negative_val_dict19 = {}
negative_val_dict20 = {}
negative_val_alinged_dict = {}

counter = 0
for word in combinations:
  ninety_vec = combinations_vectors_1949[counter]
  twenty_vec = combinations_vectors_2000[counter]
  aligned_vec = alinged_vecs[counter]
  neg_count_nine = len(list(filter(lambda x: (x < 0), ninety_vec[0])))
  neg_count_twen = len(list(filter(lambda x: (x < 0), twenty_vec[0])))
  neg_count_align = len(list(filter(lambda x: (x < 0), aligned_vec[0])))
  negative_val_dict19[word] = neg_count_nine
  negative_val_dict20[word] = neg_count_twen
  negative_val_alinged_dict[word] = neg_count_align
  counter += 1

negative_val_dict19_arr = np.array(list(negative_val_dict19.values()))
negative_val_dict20_arr = np.array(list(negative_val_dict20.values()))
negative_val_dictali_arr = np.array(list(negative_val_alinged_dict.values()))

print("average negative values 1949:", np.mean(negative_val_dict19_arr))
print("average negative values 2000:", np.mean(negative_val_dict20_arr))
print("average negative values aligned:", np.mean(negative_val_dictali_arr))

