# -*- coding: utf-8 -*-
"""Bert_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bxy1raiNR4W_Kv3F_F1o1CyL6GQuqyjI
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install torch
# !pip install transformers

from transformers import BertTokenizer, BertModel
import pandas as pd
import numpy as np
import nltk
import torch

# Loading the pre-trained BERT model
###################################
# Embeddings will be derived from
# the outputs of this model
model = BertModel.from_pretrained('bert-base-uncased',
                                  output_hidden_states = True,
                                  )

# Setting up the tokenizer
###################################
# This is the same tokenizer that
# was used in the model to generate 
# embeddings to ensure consistency
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

try:
  from google.colab import drive
  IN_COLAB=True
except:
  IN_COLAB=False

if IN_COLAB:
  print("We're running Colab")

# Commented out IPython magic to ensure Python compatibility.
if IN_COLAB:
  # Mount the Google Drive at mount
  mount='/content/gdrive'
  print("Colab: mounting Google drive on ", mount)

  drive.mount(mount)

  # Switch to the directory on the Google Drive that you want to use
  import os
  drive_root = mount + "/My Drive/Thesis/Code/Descriptives"
  
  # Create drive_root if it doesn't exist
  create_drive_root = True
  if create_drive_root:
    print("\nColab: making sure ", drive_root, " exists.")
    os.makedirs(drive_root, exist_ok=True)
  
  # Change to the directory
  print("\nColab: Changing directory to ", drive_root)
#   %cd $drive_root

import pandas as pd

df_1949 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed1949.csv")
df_2000 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed2000.csv")

texts_list_2000 = []
for ind in df_2000.index:
     toevoegen = df_2000['0'][ind]
     texts_list_2000.append(toevoegen)
    

texts_list_1949 = []
for ind in df_1949.index:
     toevoegen = df_1949['0'][ind]
     texts_list_1949.append(toevoegen)

import io
import pandas as pd

#Processing the name dataset
    
with open('/content/gdrive/My Drive/Thesis/Data/Overig/Famous people.txt', 'r') as fp:
    nameslist = fp.readlines()
    
names = []
for name in nameslist:
    new = name.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    names.append(new6)

dfnames = pd.DataFrame(names)

#BERT uses a fixed-size window that limits the amount of
#text that can be input to the model at one time. The model maximum window size, or maximum sequence
#length, is fixed during pre-training, with 512 wordpieces a common choice

#Make sure the name data is usubale (1-gram last name)
two_gram_names = []

for small_t_gram in names:
    two_names = small_t_gram.split(" ")
    if len(two_names[-1]) > 2:
        two_gram_names.append(two_names[-1])
    else:
        two_gram_names.append(two_names[-2])
        

two_gram_names[0] = "muhammad"

print(two_gram_names)

with open('Names_processed.txt', 'w') as filehandle:
    for listitem in two_gram_names:
        filehandle.write('%s\n' % listitem)
#print(len(two_gram_names))



for name in two_gram_names:
  texts_list_2000.append(name)
  texts_list_1949.append(name)
  
#for eval in eval_words:
#  texts_list.append(eval)

# Text corpus
##############
# These sentences show the different
# forms of the word 'bank' to show the
# value of contextualized embeddings

texts_2000 = texts_list_2000
texts_1949 = texts_list_1949

def bert_text_preparation(text, tokenizer):
    """Preparing the input for BERT
    
    Takes a string argument and performs
    pre-processing like adding special tokens,
    tokenization, tokens to ids, and tokens to
    segment ids. All tokens are mapped to seg-
    ment id = 1.
    
    Args:
        text (str): Text to be converted
        tokenizer (obj): Tokenizer object
            to convert text into BERT-re-
            adable tokens and ids
        
    Returns:
        list: List of BERT-readable tokens
        obj: Torch tensor with token ids
        obj: Torch tensor segment ids
    
    
    """
    marked_text = "[CLS] " + text + " [SEP]"
    tokenized_text = tokenizer.tokenize(marked_text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    segments_ids = [1]*len(indexed_tokens)

    # Convert inputs to PyTorch tensors
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    return tokenized_text, tokens_tensor, segments_tensors
    
def get_bert_embeddings(tokens_tensor, segments_tensors, model):
    """Get embeddings from an embedding model
    
    Args:
        tokens_tensor (obj): Torch tensor size [n_tokens]
            with token ids for each token in text
        segments_tensors (obj): Torch tensor size [n_tokens]
            with segment ids for each token in text
        model (obj): Embedding model to generate embeddings
            from token and segment ids
    
    Returns:
        list: List of list of floats of size
            [n_tokens, n_embedding_dimensions]
            containing embeddings for each token
    
    """
    
    # Gradient calculation id disabled
    # Model is in inference mode
    with torch.no_grad():
        outputs = model(tokens_tensor, segments_tensors)
        # Removing the first hidden state
        # The first state is the input state
        hidden_states = outputs[2][1:]

    # Getting embeddings from the final BERT layer
    token_embeddings = hidden_states[-1]
    # Collapsing the tensor into 1-dimension
    token_embeddings = torch.squeeze(token_embeddings, dim=0)
    # Converting torchtensors to lists
    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]

    return list_token_embeddings

new_test_list_2000 = []
for text in texts_2000:
  new_test = bert_text_preparation(text, tokenizer)
  new_test_list_2000.append(new_test)


new_test_list_1949 = []
for text in texts_1949:
  new_test = bert_text_preparation(text, tokenizer)
  new_test_list_1949.append(new_test)

tokenized_text_list_2000 = []
tokens_tensor_list_2000 = []
segments_tensors_list_2000 = []
for text in texts_2000:
  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)
  tokenized_text_list_2000.append(tokenized_text)
  tokens_tensor_list_2000.append(tokens_tensor)
  segments_tensors_list_2000.append(segments_tensors)



tokenized_text_list_1949 = []
tokens_tensor_list_1949 = []
segments_tensors_list_1949 = []
for text in texts_1949:
  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)
  tokenized_text_list_1949.append(tokenized_text)
  tokens_tensor_list_1949.append(tokens_tensor)
  segments_tensors_list_1949.append(segments_tensors)

target_word_embeddings_2000 = []
name_embedding_list_2000 = []
tokenized_list_2000 = []
list_token_embeddings_2000 = []

for text in texts_2000:
  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)
  tokenized_list_2000.append(tokenized_text) 
  token_embeddings_2000 = get_bert_embeddings(tokens_tensor, segments_tensors, model)
  list_token_embeddings_2000.append(token_embeddings_2000)


print(len(tokenized_list_2000))



target_word_embeddings_1949 = []
name_embedding_list_1949 = []
tokenized_list_1949 = []
list_token_embeddings_1949 = []

for text in texts_1949:
  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)
  tokenized_list_1949.append(tokenized_text) 
  token_embeddings_1949 = get_bert_embeddings(tokens_tensor, segments_tensors, model)
  list_token_embeddings_1949.append(token_embeddings_1949)


print(len(tokenized_list_1949))

d = {'Tokens':tokenized_list_2000,'Word Embeddings':list_token_embeddings_2000}

emb_token_2000 =pd.DataFrame(d)

f = {'Tokens':tokenized_list_1949,'Word Embeddings':list_token_embeddings_1949}

emb_token_1949 =pd.DataFrame(f)

print(list_token_embeddings_1949[0])
print(len(list_token_embeddings_1949))
print(len(texts_1949))

##Making the wordlist:

words_2000 = []


for index, row in emb_token_2000.iterrows():
    sent = row["Tokens"]
    for word in sent:
      if word in words_2000:
        pass
      else:
        words_2000.append(word)


words_1949 = []


for index, row in emb_token_1949.iterrows():
    sent = row["Tokens"]
    for word in sent:
      if word in words_1949:
        pass
      else:
        words_1949.append(word)

print(len(words_2000))
print(len(words_1949))

results_df = pd.read_csv("/content/gdrive/My Drive/Thesis/Data/Overig/One_evaluation_set.csv")

#results_df

results_df = results_df.sort_values(by=['Rating'], ascending=False)

indexlist = []
for number in range(1, len(results_df) + 1):
    indexlist.append(number)
    
results_df["index"] = indexlist
print("length evalset before processing:", len(results_df))

setlist = []
for index, row in results_df.iterrows():
    temp_set = []
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    temp_set.append(word1)
    temp_set.append(word2)
    if temp_set in setlist:
        results_df = results_df.drop([index])
    else:
        setlist.append(temp_set)


print("length of evalset after processing:", len(results_df))

results_df[196:199]

eval_words = []
eval_sets = []
for index, row in results_df.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    set_list = []
    set_list.append(word1)
    set_list.append(word2)
    eval_sets.append(set_list)
    eval_words.append(word1)
    eval_words.append(word2)

from scipy.stats import spearmanr

spearmans_list_2000 = []
spearmans_set_list_2000 = []

for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in words_2000 and word2 in words_2000:
      for tokenlist in tokenized_list_2000:
        tokenlistje = []
        if word1 in tokenlist and word2 in tokenlist:
          for token in tokenlist:
            if token == word1:
              tkindex1 = tokenized_list_2000.index(tokenlist)
              word_index1 = tokenlist.index(word1)
              word_embeddings1 = list_token_embeddings_2000[tkindex1][word_index1]
              array1 = np.array(word_embeddings1)
              tokenlistje.append(array1)
            if token == word2:
              tkindex2 = tokenized_list_2000.index(tokenlist)
              word_index2 = tokenlist.index(word2)
              word_embeddings2 = list_token_embeddings_2000[tkindex2][word_index2]
              array2 = np.array(word_embeddings2)
              tokenlistje.append(array2)
            else:
              pass

        #for array in tokenlistje:
          #print("array in tokenlistje", array)
          #print("length of tokenlistje", len(array))
          #first = array[0]
          #second = array[1]
          df = pd.DataFrame(array1)
          df2 = pd.DataFrame(array2)
          #df = df.T
          #df2 = df2.T
          rho, p = spearmanr(df, df2)
          result_list = []
          result_list.append(word1)
          result_list.append(word2)
          result_list.append(float(rho))
          spearmans_set_list_2000.append(result_list)
          spearmans_list_2000.append(rho)

            
      
      
      
          #word_index1 = word1.index(tk)
          #word_embedding1 = list_token_embeddings[0][word_index1].reshape((1,100))
          #array1 = np.array(word_embedding1)

spearmans_list_1949 = []
spearmans_set_list_1949 = []

for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in words_1949 and word2 in words_1949:
      for tokenlist in tokenized_list_1949:
        tokenlistje = []
        if word1 in tokenlist and word2 in tokenlist:
          for token in tokenlist:
            if token == word1:
              tkindex1 = tokenized_list_1949.index(tokenlist)
              word_index1 = tokenlist.index(word1)
              word_embeddings1 = list_token_embeddings_1949[tkindex1][word_index1]
              array1 = np.array(word_embeddings1)
              tokenlistje.append(array1)
            if token == word2:
              tkindex2 = tokenized_list_1949.index(tokenlist)
              word_index2 = tokenlist.index(word2)
              word_embeddings2 = list_token_embeddings_1949[tkindex2][word_index2]
              array2 = np.array(word_embeddings2)
              tokenlistje.append(array2)
            else:
              pass

        #for array in tokenlistje:
          #print("array in tokenlistje", array)
          #print("length of tokenlistje", len(array))
          #first = array[0]
          #second = array[1]
          df = pd.DataFrame(array1)
          df2 = pd.DataFrame(array2)
          #df = df.T
          #df2 = df2.T
          rho, p = spearmanr(df, df2)
          result_list = []
          result_list.append(word1)
          result_list.append(word2)
          result_list.append(float(rho))
          spearmans_set_list_1949.append(result_list)
          spearmans_list_1949.append(rho)

            
      
      
      
          #word_index1 = word1.index(tk)
          #word_embedding1 = list_token_embeddings[0][word_index1].reshape((1,100))
          #array1 = np.array(word_embedding1)

#print(len(spearmans_set_list_2000))

if len(spearmans_set_list_2000) > 0:
  spearmansdf_2000 = pd.DataFrame(spearmans_set_list_2000)
  spearmansdf_2000.columns = ["Word 1", "Word 2", "Spearman"]

  spearmansdf_2000 = spearmansdf_2000.sort_values(by=['Spearman'], ascending=False)

  spearmansdf_2000 = spearmansdf_2000.drop_duplicates(keep='first')


  indexlist = []
  for number in range(1, len(spearmansdf_2000) + 1):
    indexlist.append(number)
    
  spearmansdf_2000["index"] = indexlist
  #spearmansdf_2000

#spearmansdf_2000

if len(spearmans_set_list_1949) > 0:
  spearmansdf_1949 = pd.DataFrame(spearmans_set_list_1949)
  spearmansdf_1949.columns = ["Word 1", "Word 2", "Spearman"]

  spearmansdf_1949 = spearmansdf_1949.sort_values(by=['Spearman'], ascending=False)

  spearmansdf_1949 = spearmansdf_1949.drop_duplicates(keep='first')


  indexlist = []
  for number in range(1, len(spearmansdf_1949) + 1):
    indexlist.append(number)
    
  spearmansdf_1949["index"] = indexlist
  #spearmansdf_1949

temp_list_res_2000 = []
temp_list_2000 = []
instances_list_2000 = []

for index, row in spearmansdf_2000.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_2000.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_2000.append(help_list)

for instance in temp_list_2000:
    #print(instance)
    for instance2 in temp_list_res_2000:
        #print(instance2)
        if instance == instance2:
            instances_list_2000.append(instance)
            
            
           
#instances_list_2000

temp_list_res_1949 = []
temp_list_1949 = []
instances_list_1949 = []

for index, row in spearmansdf_1949.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_1949.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_1949.append(help_list)

for instance in temp_list_1949:
    #print(instance)
    for instance2 in temp_list_res_1949:
        #print(instance2)
        if instance == instance2:
            instances_list_1949.append(instance)
            
            
           
#instances_list_1949

results_list_2000 = []
spearmans_list_2000 = []


for instance in instances_list_2000:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_2000.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_2000:
                    pass
                else:
                    spearmans_list_2000.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
                if index_res in results_list_2000:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_2000.append(index_res)
                tracker_2.clear()

results_list_1949 = []
spearmans_list_1949 = []


for instance in instances_list_1949:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_1949.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_1949:
                    pass
                else:
                    spearmans_list_1949.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
                if index_res in results_list_1949:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_1949.append(index_res)
                tracker_2.clear()

results_list_2000

spearmans_list_2000

instances_list_2000

spearmans_ranking_df_2000 = pd.DataFrame()
spearmans_ranking_df_2000["Eval index"] = results_list_2000
spearmans_ranking_df_2000["Embedding index"] = spearmans_list_2000[0:7]
spearmans_ranking_df_2000["Words"] = instances_list_2000[0:7]
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
#spearmans_ranking_df_2000


spearmans_ranking_df_1949 = pd.DataFrame()
spearmans_ranking_df_1949["Eval index"] = results_list_1949
spearmans_ranking_df_1949["Embedding index"] = spearmans_list_1949
spearmans_ranking_df_1949["Words"] = instances_list_1949
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
#spearmans_ranking_df_1949

spearmans_ranking_df_1949

spearmans_ranking_df_2000

from sklearn.metrics import ndcg_score

my_r_1949 = spearmans_ranking_df_1949.corr(method="spearman")
spearmans_rank_correlation_df = pd.DataFrame()
spearmans_rank_correlation_df["SPR1949"] = [my_r_1949]

my_r_2000 = spearmans_ranking_df_2000.corr(method="spearman")
spearmans_rank_correlation_df["SPR2000"] = [my_r_2000]

dsgeval_1949 = list([spearmans_list_1949])
dsgemb_1949 = list([results_list_1949])

dcgresults_1949 = ndcg_score(dsgeval_1949, dsgemb_1949)

spearmans_rank_correlation_df["DCG1949"] = dcgresults_1949

dsgeval_2000 = list([spearmans_list_2000[0:7]])
dsgemb_2000 = list([results_list_2000])

dcgresults_2000 = ndcg_score(dsgeval_2000, dsgemb_2000)

spearmans_rank_correlation_df["DCG2000"] = dcgresults_2000



spearmans_rank_correlation_df.to_csv("/content/gdrive/My Drive/Thesis/Results/BERT/BERTDCGSPEARMAN.csv", index = False)



#spearmans_rank_correlation_BERT_2000.to_csv('spearmans_rank_correlation_BERT_2000.csv', index = False)
#spearmans_rank_correlation_BERT_1949.to_csv('spearmans_rank_correlation_BERT_1949.csv', index = False)

from scipy.spatial.distance import cosine

# Calculating the distance between the
# embeddings of 'bank' in all the
# given contexts of the word

list_of_distances = []
for text1, embed1 in zip(texts_2000, target_word_embeddings_2000):
    for text2, embed2 in zip(texts_1949, target_word_embeddings_1949):
        cos_dist = 1 - cosine(embed1, embed2)
        list_of_distances.append([text1, text2, cos_dist])


print(list_of_distances[0:100])

distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])

combinations = []

for word in words_1949:
    if word in words_2000:
        combinations.append(word)
  

## looking at words that appear both in 1949 and 2000
combinations_vectors_1949 = []
combinations_vectors_2000 = []

track_1949 = []
track_2000 = []

test_list = []


for word in combinations:
    for tokenlist in tokenized_list_1949:
      if word in tokenlist:
        for token in tokenlist:
          if token == word:
            if word not in track_1949:
              tkindex1 = tokenized_list_1949.index(tokenlist)
              word_index1 = tokenlist.index(word)
              word_embeddings1 = list_token_embeddings_1949[tkindex1][word_index1]
              combinations_vectors_1949.append(word_embeddings1)
              lsitje = [word, word_embeddings1[0]]
              test_list.append(lsitje)
              track_1949.append(word)
            else:
              pass
          else:
            pass




    for tokenlist2 in tokenized_list_2000:
      if word in tokenlist2:
        for token2 in tokenlist2:
          if token2 == word:
            if word not in track_2000:
              tkindex2 = tokenized_list_2000.index(tokenlist2)
              word_index2 = tokenlist2.index(word)
              word_embeddings2 = list_token_embeddings_2000[tkindex2][word_index2]
              combinations_vectors_2000.append(word_embeddings2)
              lsitje2 = [word, word_embeddings2[0] ]
              test_list.append(lsitje2)
              track_2000.append(word)
            else:
              pass
          else:
            pass


        
      else:
        pass

myset = set(combinations)
#print(len(myset))

from scipy import spatial

combinationsdf = pd.DataFrame(combinations)
combinationsdf.columns = ["Words"]


simlist = []
counter = 0
for vector in combinations_vectors_1949:
  word = combinations[counter]
  sim = spatial.distance.cosine(vector, combinations_vectors_2000[counter])
  simlist.append(sim)
  counter += 1

combinationsdf["sim between 1949 & 2000"] = simlist

combinationsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/BERT/wordsimbetweentimeBERT.csv', index = False)

simarray = np.array(simlist)
sum = np.sum(simarray)
avg = np.mean(simarray)

nearest_neighbors_2000 = []
nearest_neighbors_words_2000 = []

for word in combinations:
    wordindex = combinations.index(word)
    wordvec = combinations_vectors_2000[wordindex]

    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    part1 = []
    part2 = []
    others = [x for x in combinations if x != word]

    for other in others:
        otherindex = combinations.index(other)
        othervec = combinations_vectors_2000[otherindex]
        othervecs.append(othervec)
        part1.append(other)
    
    
    for value in othervec:
        sim = spatial.distance.cosine(wordvec, value)
        sims.append(sim)
        part2.append(sim)



    counter = 0
    for i in part1:
      newsetlist = [i, part2[counter]]
      setlist.append(newsetlist)
      counter += 1
      
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_2000.append(setje)

    for n in near:
      for s in setlist:
        nmr = s[1]
        for m in n:
            if m == nmr:
              if s[0] not in neigbor_words:
                 neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_2000.append(makeset)
    
        
            #nearest_neighbors.append(sim)

#nearest_neighbors_words_2000[1]

nearest_neighbors_1949 = []
nearest_neighbors_words_1949 = []

for word in combinations:
    wordindex = combinations.index(word)
    wordvec = combinations_vectors_1949[wordindex]

    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    part1 = []
    part2 = []
    others = [x for x in combinations if x != word]

    for other in others:
        otherindex = combinations.index(other)
        othervec = combinations_vectors_1949[otherindex]
        othervecs.append(othervec)
        part1.append(other)
    
    
    for value in othervec:
        sim = spatial.distance.cosine(wordvec, value)
        sims.append(sim)
        part2.append(sim)



    counter = 0
    for i in part1:
      newsetlist = [i, part2[counter]]
      setlist.append(newsetlist)
      counter += 1
      
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_1949.append(setje)

    for n in near:
      for s in setlist:
        nmr = s[1]
        for m in n:
            if m == nmr:
              if s[0] not in neigbor_words:
                 neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_1949.append(makeset)
    
        
            #nearest_neighbors.append(sim)

#nearest_neighbors_words_1949[1]

nn_df_2000 = pd.DataFrame(nearest_neighbors_2000)
nn_df_2000_2 = pd.DataFrame(nearest_neighbors_words_2000)
nn_df_2000.to_csv('/content/gdrive/My Drive/Thesis/Results/BERT/nn_df_2000_BERT.csv', index = False)
nn_df_2000_2.to_csv("/content/gdrive/My Drive/Thesis/Results/BERT/nn_df_2000_words_BERT.csv", index = False)

nn_df_1949 = pd.DataFrame(nearest_neighbors_1949)
nn_df_1949_2 = pd.DataFrame(nearest_neighbors_words_1949)
nn_df_1949.to_csv('/content/gdrive/My Drive/Thesis/Results/BERT/nn_df_2000_BERT.csv', index = False)
nn_df_1949_2.to_csv("/content/gdrive/My Drive/Thesis/Results/BERT/nn_df_2000_words_BERT.csv", index = False)

def linear_align(base_embed, other_embed):
    """
        Align other embedding to base embedding using best linear transform.
        NOTE: Assumes indices are aligned
    """
    basevecs = base_embed
    othervecs = other_embed
    fixedvecs = othervecs.dot(np.linalg.pinv(othervecs)).dot(basevecs)
    return fixedvecs

counter = 0
aligned = []
for embedding in combinations_vectors_1949:
  align = linear_align(np.array(embedding).reshape(np.array(embedding).shape[0], 1), np.array(combinations_vectors_2000[counter]).reshape(np.array(combinations_vectors_2000[counter]).shape[0], 1))
  counter += 1
  aligned.append(align)

from scipy.spatial import distance
counter = 0

distances = []

for wordje in combinations:
    dist = distance.cosine(aligned[counter], combinations_vectors_2000[counter])
    distances.append(dist)
    counter +=1

resultscos = pd.DataFrame(distances, combinations)
resultscos
resultscos.to_csv("/content/gdrive/My Drive/Thesis/Results/BERT/resultscos.csv", index = False)

def smart_procrustes_align_gensim(base_embed, other_embed, words=None):
	"""Procrustes align two gensim word2vec models (to allow for comparison between same word across models).
	Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.
		(With help from William. Thank you!)
	First, intersect the vocabularies (see `intersection_align_gensim` documentation).
	Then do the alignment on the other_embed model.
	Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.
	Return other_embed.
	If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).
	"""

	# make sure vocabulary and indices are aligned
	in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)
    
	# get the embedding matrices
	base_vecs =  in_base_embed.wv
	other_vecs = in_other_embed.wv

	# just a matrix dot product with numpy
	m = other_vecs.T.dot(base_vecs) 
	# SVD method from numpy
	u, _, v = np.linalg.svd(m)
	# another matrix operation
	ortho = u.dot(v) 
	# Replace original array with modified one
	# i.e. multiplying the embedding matrix (syn0norm)by "ortho"
	other_embed = other_embed = (other_embed).dot(ortho)
	return other_embed
	
def intersection_align_gensim(m1,m2, words=None):
	"""
	Intersect two gensim word2vec models, m1 and m2.
	Only the shared vocabulary between them is kept.
	If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.
	Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).
	These indices correspond to the new syn0 and syn0norm objects in both gensim models:
		-- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0
		-- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2
	The .vocab dictionary is also updated for each model, preserving the count but updating the index.
	"""

	# Get the vocab for each model
	vocab_m1 = set(m1.wv.key_to_index.keys())
	vocab_m2 = set(m2.wv.key_to_index.keys())

	# Find the common vocabulary
	common_vocab = vocab_m1&vocab_m2
	if words: common_vocab&=set(words)

	# If no alignment necessary because vocab is identical...
	if not vocab_m1-common_vocab and not vocab_m2-common_vocab:
		return (m1,m2)

	# Otherwise sort by frequency (summed for both)
	common_vocab = list(common_vocab)
	common_vocab.sort()

	# Then for each model...
	for m in [m1,m2]:
		# Replace old syn0norm array with new one (with common vocab)
		indices = [m1.wv.key_to_index[w] for w in common_vocab]
		old_arr = m.wv
		new_arr = np.array([old_arr[index] for index in indices])
		m.syn0norm = m.syn0 = new_arr

		# Replace old vocab dictionary with new one (with common vocab)
		# and old index2word with new one
		m.index2word = common_vocab
		old_vocab = m.wv.key_to_index
		new_vocab = {}
		for new_index,word in enumerate(common_vocab):
			old_vocab_obj= old_vocab[word]
			new_vocab[word] = common_vocab[new_index]
		m.vocab = new_vocab

	return (m1,m2)

resultscos = pd.DataFrame(distances, combinations)
resultscos
resultscos.to_csv("/content/gdrive/My Drive/Thesis/Results/BERT/resultscosBERT.csv", index = False)

print(len(tokenized_list_1949))
print(len(list_token_embeddings_1949))

simalarityaligned_list = []

for vector in aligned:
  othervecs = aligned
  othersims = []
  wordpairslist = []
  counter = 0
  for other in othervecs:
    sim = spatial.distance.cosine(vector, other)
    othersims.append(sim)

  simalarityaligned_list.append(othersims)

print(len(simalarityaligned_list))
print(len(combinations))

newresult_list = []
counter = 0

for vector in simalarityaligned_list:
  newresult = [vector, combinations[counter]]
  counter += 1
  newresult_list.append(newresult)

washington_list = []

for result in newresult_list:
  if 'washington' in result:
    washington_list.append(result)

washington_list[-1][1]
washington_list[-1].pop()
print(washington_list)

washdf = pd.DataFrame(washington_list[0]).T

washdf["words"] = combinations

washdf.sort_values(by=0, ascending=False)[0:90]

nearest_neighbors_aligned = []
nearest_neighbors_words_aligned = []

for word in combinations:
    wordindex = combinations.index(word)
    wordvec = aligned[wordindex]

    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    part1 = []
    part2 = []
    others = [x for x in combinations if x != word]

    for other in others:
        otherindex = combinations.index(other)
        othervec = aligned[otherindex]
        othervecs.append(othervec)
        part1.append(other)
    
    
    for value in othervec:
        sim = spatial.distance.cosine(wordvec, value)
        sims.append(sim)
        part2.append(sim)



    counter = 0
    for i in part1:
      newsetlist = [i, part2[counter]]
      setlist.append(newsetlist)
      counter += 1
      
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_aligned.append(setje)

    for n in near:
      for s in setlist:
        nmr = s[1]
        for m in n:
            if m == nmr:
              if s[0] not in neigbor_words:
                 neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_aligned.append(makeset)
    
        
            #nearest_neighbors.append(sim)

nearest_neighbors_aligned

nn_df_align = pd.DataFrame(nearest_neighbors_aligned)
nn_df_align_2 = pd.DataFrame(nearest_neighbors_words_aligned)
nn_df_align.to_csv('/content/gdrive/My Drive/Thesis/Results/BERT/nn_df_alignBERT.csv', index = False)
nn_df_align_2.to_csv("/content/gdrive/My Drive/Thesis/Results/BERT/nn_df_align_wordsBERT.csv", index = False)

!pip install bert-embedding mxnet-cu100

import mxnet as mx
from bert_embedding import BertEmbedding

ctx = mx.gpu(0)
bert = BertEmbedding(ctx=ctx)

# Getting embeddings for the target
# word in all given contexts
# Find the position 'bank' in list of tokens
embeddingtargets = []
name_need = ["stalin", "fleming", "washington"]
for name in name_need:
  for tokenized_textje in tokenized_text_list_1949:
    if name in tokenized_textje:
      word_index = tokenized_textje.index(name)
      # Get the embedding for bank
      word_embedding = list_token_embeddings_1949[word_index]
      if word_embedding not in embeddingtargets:
        embeddingtargets.append(word_embedding)

print(len(embeddingtargets))

from scipy.spatial.distance import cosine

# Calculating the distance between the
# embeddings of 'bank' in all the
# given contexts of the word

list_of_distances = []
for tokenized_textje, embed1 in zip(tokenized_text_list_1949, embeddingtargets):
    for text2, embed2 in zip(tokenized_text_list_1949, embeddingtargets):
        cos_dist = 1 - cosine(embed1, embed2)
        list_of_distances.append([text1, text2, cos_dist])

distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])

!pip install pytorch-pretrained-bert

# Commented out IPython magic to ensure Python compatibility.
import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM

# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows
import logging
#logging.basicConfig(level=logging.INFO)

import matplotlib.pyplot as plt
# % matplotlib inline

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokenized_sentences = []

for text in texts_1949:
  marked_text = "[CLS] " + text + " [SEP]"

  # Tokenize our sentence with the BERT tokenizer.
  tokenized_text = tokenizer.tokenize(marked_text)
  tokenized_sentences.append(tokenized_text)

list(tokenizer.vocab.keys())[5000:5020]

tokenized_text_list_2 = []
indexed_tokens_list = []

for text in texts_1949:

  # Add the special tokens.
  marked_text = "[CLS] " + text + " [SEP]"

  # Split the sentence into tokens.
  tokenized_text = tokenizer.tokenize(marked_text)
  tokenized_text_list_2.append(tokenized_text)

  # Map the token strings to their vocabulary indeces.
  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
  indexed_tokens_list.append(indexed_tokens)

# Mark each of the 22 tokens as belonging to sentence "1".
segments_ids_list = []
counter = 1
for tokenize in tokenized_text_list_2:
  segments_ids = [counter] * len(tokenize)
  segments_ids_list.append(segments_ids)
  counter += 1

print(len(segments_ids_list))
print(len(tokenized_text_list_2))

# Convert inputs to PyTorch tensors
tokens_tensor_list = []
segments_tensor_list = []

for index in indexed_tokens_list:
  tokens_tensor = torch.tensor([indexed_tokens])
  tokens_tensor_list.append(tokens_tensor)

for segment in segments_ids_list:
  segments_tensors = torch.tensor([segments_ids])
  segments_tensor_list.append(segments_tensors)

# Load pre-trained model (weights)
model = BertModel.from_pretrained('bert-base-uncased')

# Put the model in "evaluation" mode, meaning feed-forward operation.
model.eval()



# Predict hidden states features for each layer
encoded_layers_list = []
counter = 0
with torch.no_grad():
  for token in tokens_tensor_list:
    encoded_layers = model(token, segments_tensor_list[counter])
    encoded_layers_list.append(encoded_layers)
    counter += 1

prencoded_layers_list

# Concatenate the tensors for all layers. We use `stack` here to
# create a new dimension in the tensor.
token_embeddings_list = []
for encode in encoded_layers_list:
  token_embeddings = torch.stack(encode[0][0], dim=0)
  token_embeddings_list.append(token_embeddings)

# Remove dimension 1, the "batches".
token_embeddings = torch.squeeze(token_embeddings, dim=1)

token_embeddings.size()

# Swap dimensions 0 and 1.
token_embeddings = token_embeddings.permute(1,0,2)

token_embeddings.size()

# Stores the token vectors, with shape [22 x 3,072]
token_vecs_cat = []

# `token_embeddings` is a [22 x 12 x 768] tensor.

# For each token in the sentence...
for token in token_embeddings:
    
    # `token` is a [12 x 768] tensor

    # Concatenate the vectors (that is, append them together) from the last 
    # four layers.
    # Each layer vector is 768 values, so `cat_vec` is length 3,072.
    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)
    
    # Use `cat_vec` to represent `token`.
    token_vecs_cat.append(cat_vec)

print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))

# Stores the token vectors, with shape [22 x 768]
token_vecs_sum = []

# `token_embeddings` is a [22 x 12 x 768] tensor.

# For each token in the sentence...
for token in token_embeddings:

    # `token` is a [12 x 768] tensor

    # Sum the vectors from the last four layers.
    sum_vec = torch.sum(token[-4:], dim=0)
    
    # Use `sum_vec` to represent `token`.
    token_vecs_sum.append(sum_vec)

print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))

# `encoded_layers` has shape [12 x 1 x 22 x 768]

# `token_vecs` is a tensor with shape [22 x 768]
token_vecs = encoded_layers[11][0]

# Calculate the average of all 22 token vectors.
sentence_embedding = torch.mean(token_vecs, dim=0)

print ("Our final sentence embedding vector of shape:", sentence_embedding.size())

for i, token_str in enumerate(tokenized_text):
  print (i, token_str)

print('First 5 vector values for each instance of "bank".')
print('')
print("bank vault   ", str(token_vecs_sum[6][:5]))
print("bank robber  ", str(token_vecs_sum[10][:5]))
print("river bank   ", str(token_vecs_sum[19][:5]))

from scipy.spatial.distance import cosine

# Calculate the cosine similarity between the word bank 
# in "bank robber" vs "river bank" (different meanings).
diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])

# Calculate the cosine similarity between the word bank
# in "bank robber" vs "bank vault" (same meaning).
same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])

print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)
print('Vector similarity for *different* meanings:  %.2f' % diff_bank)

