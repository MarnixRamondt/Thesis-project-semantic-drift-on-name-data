# -*- coding: utf-8 -*-
"""Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fc1E8jAnWthRawhMICJzFBPULSP7leSa
"""

try:
  from google.colab import drive
  IN_COLAB=True
except:
  IN_COLAB=False

if IN_COLAB:
  print("We're running Colab")

# Commented out IPython magic to ensure Python compatibility.
if IN_COLAB:
  # Mount the Google Drive at mount
  mount='/content/gdrive'
  print("Colab: mounting Google drive on ", mount)

  drive.mount(mount)

  # Switch to the directory on the Google Drive that you want to use
  import os
  drive_root = mount + "/My Drive/Thesis/Code/Descriptives"
  
  # Create drive_root if it doesn't exist
  create_drive_root = True
  if create_drive_root:
    print("\nColab: making sure ", drive_root, " exists.")
    os.makedirs(drive_root, exist_ok=True)
  
  # Change to the directory
  print("\nColab: Changing directory to ", drive_root)
#   %cd $drive_root

!pip install nltk
!pip install gensim

import nltk
import gensim
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.manifold import TSNE

df_1949 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed1949.csv")
df_2000 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed2000.csv")

# Convert a dataframe to the list of rows i.e. list of lists
List_1949 = df_1949.to_numpy().tolist()

List_2000 = df_2000.to_numpy().tolist()

running_text_1949 = []
for listje in List_1949:
    new_list = []
    words = listje[0].split(" ")
    new_list.append(words)
    running_text_1949.append(words)

running_text_2000 = []
for listje in List_2000:
    new_list = []
    words = listje[0].split(" ")
    new_list.append(words)
    running_text_2000.append(words)

!pip install --upgrade gensim

#Skip-gram
sg_model_1949 = gensim.models.Word2Vec(running_text_1949, min_count = 4, vector_size = 100,
                                             window = 4, sg = 1, negative = 5)

sg_model_2000 = gensim.models.Word2Vec(running_text_2000, min_count = 4, vector_size = 100,
                                             window = 4, sg = 1, negative = 5)



#Printing the vocabulary list (only for subset otherwise runtimes)
words_1949 = list(sg_model_1949.wv.key_to_index)
words_2000 = list(sg_model_2000.wv.key_to_index)

#print(words_1949)

#print(words_2000)



#If you want to look at the vector of a specific word:
arrayword_emb_1949 = []
arrayword_emb_2000 = []

for word in words_1949:
    vec = sg_model_1949.wv[word].reshape((1,100))
    arrayword_emb_1949.append(vec)
    
for word in words_2000:
    vec = sg_model_2000.wv[word].reshape((1,100))
    arrayword_emb_2000.append(vec)

import numpy as np

total_emb_1949 = np.array(arrayword_emb_1949)
total_emb_2000 = np.array(arrayword_emb_2000)


combinations = []
for word in words_1949:
    if word in words_2000:
        combinations.append(word)
  

## looking at words that appear both in 1949 and 2000
combinations_vectors_1949 = []
combinations_vectors_2000 = []
for word in combinations:
    vec = sg_model_2000.wv[word].reshape((1,100))
    vec2 = sg_model_1949.wv[word].reshape((1,100))
    combinations_vectors_1949.append(vec2)
    combinations_vectors_2000.append(vec)

tsne = TSNE(n_components=2)

vocab_1949 = list(sg_model_1949.wv.key_to_index)
test_vocab_1949 = words_1949
X_1949 = sg_model_1949.wv[vocab_1949]

vocab_2000 = list(sg_model_2000.wv.key_to_index)
test_vocab_2000 = words_2000
X_2000 = sg_model_2000.wv[vocab_2000]

X_tsne_1949 = tsne.fit_transform(X_1949)
X_tsne_2000 = tsne.fit_transform(X_2000)

wordvecs_1949 = pd.DataFrame(X_tsne_1949, index=vocab_1949, columns=['x', 'y'])

wordvecs_2000 = pd.DataFrame(X_tsne_2000, index=vocab_2000, columns=['x', 'y'])

import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

ax.scatter(wordvecs_1949['x'], wordvecs_1949['y'])

for word, pos in wordvecs_1949.iterrows():
    ax.annotate(word, pos)
    

plt.savefig('/content/gdrive/My Drive/Thesis/Results/W2V/Word-embeddingsW2V_1949window4.png')
plt.show()

import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

ax.scatter(wordvecs_2000['x'], wordvecs_2000['y'])
for word, pos in wordvecs_2000.iterrows():
    ax.annotate(word, pos)
    
plt.savefig('/content/gdrive/My Drive/Thesis/Results/W2V/Word-embeddingsW2V_2000window4.png')
plt.show()

with open('/content/gdrive/My Drive/Thesis/Data/Overig/moving_words.txt', 'r') as fp:
    moving = fp.readlines()

moving_list = []
for move in moving:
    new = move.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    moving_list.append(new6)

moving_word = []
for move in moving_list:
  if move in words_1949 and move in words_2000:
    moving_word.append(move)

for moving in moving_word:
  words_2000.append(move)
  words_1949.append(move)

results_df = pd.read_csv("/content/gdrive/My Drive/Thesis/Data/Overig/One_evaluation_set.csv")

results_df

results_df = results_df.sort_values(by=['Rating'], ascending=False)

indexlist = []
for number in range(1, len(results_df) + 1):
    indexlist.append(number)
    
results_df["index"] = indexlist
print("length evalset before processing:", len(results_df))

setlist = []
for index, row in results_df.iterrows():
    temp_set = []
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    temp_set.append(word1)
    temp_set.append(word2)
    if temp_set in setlist:
        results_df = results_df.drop([index])
    else:
        setlist.append(temp_set)


print("length of evalset after processing:", len(results_df))



eval_words = []
eval_sets = []
for index, row in results_df.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    set_list = []
    set_list.append(word1)
    set_list.append(word2)
    eval_sets.append(set_list)
    eval_words.append(word1)
    eval_words.append(word2)

from scipy.stats import spearmanr
import numpy as np

spearmans_list_2000 = []
spearmans_set_list_2000 = []

for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in words_2000 and word2 in words_2000:
        word1_vec = sg_model_2000.wv[word1].reshape((1, 100))
        array1 = np.array(word1_vec)
        word2_vec = sg_model_2000.wv[word2].reshape((1,100))
        array2 = np.array(word2_vec)
        df = pd.DataFrame(word1_vec)
        df2 = pd.DataFrame(word2_vec)
        df = df.T
        df2 = df2.T
        rho, p = spearmanr(df[0:100], df2[0:100])
        result_list = []
        result_list.append(word1)
        result_list.append(word2)
        result_list.append(rho)
        spearmans_set_list_2000.append(result_list)
        spearmans_list_2000.append(rho)

from scipy.stats import spearmanr
import numpy as np

spearmans_list_1949 = []
spearmans_set_list_1949 = []

for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in words_1949 and word2 in words_1949:
        word1_vec = sg_model_1949.wv[word1].reshape((1, 100))
        array1 = np.array(word1_vec)
        word2_vec = sg_model_1949.wv[word2].reshape((1,100))
        array2 = np.array(word2_vec)
        df = pd.DataFrame(word1_vec)
        df2 = pd.DataFrame(word2_vec)
        df = df.T
        df2 = df2.T
        rho, p = spearmanr(df[0:100], df2[0:100])
        result_list = []
        result_list.append(word1)
        result_list.append(word2)
        result_list.append(rho)
        spearmans_set_list_1949.append(result_list)
        spearmans_list_1949.append(rho)

spearmansdf_2000 = pd.DataFrame(spearmans_set_list_2000)
spearmansdf_2000.columns = ["Word 1", "Word 2", "Spearman"]

spearmansdf_2000 = spearmansdf_2000.sort_values(by=['Spearman'], ascending=False)


indexlist_2000 = []
for number in range(1, len(spearmansdf_2000) + 1):
    indexlist_2000.append(number)
    
spearmansdf_2000["index"] = indexlist_2000
#spearmansdf_2000

spearmansdf_1949 = pd.DataFrame(spearmans_set_list_1949)
spearmansdf_1949.columns = ["Word 1", "Word 2", "Spearman"]

spearmansdf_1949 = spearmansdf_1949.sort_values(by=['Spearman'], ascending=False)


indexlist_1949 = []
for number in range(1, len(spearmansdf_1949) + 1):
    indexlist_1949.append(number)
    
spearmansdf_1949["index"] = indexlist_1949
#spearmansdf

temp_list_res_2000 = []
temp_list_2000 = []
instances_list_2000 = []

for index, row in spearmansdf_2000.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_2000.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_2000.append(help_list)
for instance in temp_list_2000:
    #print(instance)
    for instance2 in temp_list_res_2000:
        #print(instance2)
        if instance == instance2:
            instances_list_2000.append(instance)
            
            
           
#instances_list

temp_list_res_1949 = []
temp_list_1949 = []
instances_list_1949 = []

for index, row in spearmansdf_1949.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_1949.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_1949.append(help_list)

for instance in temp_list_1949:
    #print(instance)
    for instance2 in temp_list_res_1949:
        #print(instance2)
        if instance == instance2:
            instances_list_1949.append(instance)
            
            
           
#instances_list

results_list_2000 = []
spearmans_list_2000 = []


for instance in instances_list_2000:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_2000.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_2000:
                    pass
                else:
                    spearmans_list_2000.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
                if index_res in results_list_2000:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_2000.append(index_res)
                tracker_2.clear()

results_list_1949 = []
spearmans_list_1949 = []


for instance in instances_list_1949:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_1949.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_1949:
                    pass
                else:
                    spearmans_list_1949.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
            else:
              
                if index_res in results_list_1949:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_1949.append(index_res)
                tracker_2.clear()

print(instances_list_1949)

print(spearmans_list_1949)
print(results_list_1949)

spearmans_ranking_df_1949 = pd.DataFrame()
spearmans_ranking_df_1949["Eval index"] = results_list_1949[0:5]
spearmans_ranking_df_1949["Embedding index"] = spearmans_list_1949
spearmans_ranking_df_1949["Words"] = instances_list_1949
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
spearmans_ranking_df_1949["corr"] = spearmansdf_1949["Spearman"]
spearmans_ranking_df_1949

spearmans_ranking_df_1949 = pd.DataFrame()
spearmans_ranking_df_1949["Eval index"] = results_list_1949[0:5]
spearmans_ranking_df_1949["Embedding index"] = spearmans_list_1949
spearmans_ranking_df_1949["Words"] = instances_list_1949
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
#spearmans_ranking_df_1949

from scipy import stats
from sklearn.metrics import ndcg_score

evalind_1949 = spearmans_ranking_df_1949["Eval index"]
embind_1949 = spearmans_ranking_df_1949["Embedding index"]
dsgeval_1949 = list([evalind_1949])
dsgemb_1949 = list([embind_1949])

spearmansresult_1949 = stats.spearmanr(embind_1949, evalind_1949)
dcgresults_1949 = ndcg_score(dsgeval_1949, dsgemb_1949)

spearmans_ranking_df_2000 = pd.DataFrame()
spearmans_ranking_df_2000["Eval index"] = results_list_2000
spearmans_ranking_df_2000["Embedding index"] = spearmans_list_2000
spearmans_ranking_df_2000["Words"] = instances_list_2000
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
spearmans_ranking_df_2000

sorted(results_list_2000)

from scipy import stats
from sklearn.metrics import ndcg_score

evalind_2000 = spearmans_ranking_df_2000["Eval index"]
embind_2000 = spearmans_ranking_df_2000["Embedding index"]
dsgeval_2000 = list([evalind_2000])
dsgemb_2000 = list([embind_2000])


spearmansresult_2000 = stats.spearmanr(evalind_2000, embind_2000)
dcgresults_2000 = ndcg_score(dsgeval_2000, dsgemb_2000)

spearmansdf = pd.DataFrame()
spearmansdf["SPR2000"] = spearmansresult_2000
spearmansdf["SPR1949"] = spearmansresult_1949
spearmansdf["DCG2000"] = dcgresults_2000
spearmansdf["DSG1949"] = dcgresults_1949

spearmansdf

spearmansdf.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/spearmans_rank_corr_W2V_2000_1949window4.csv', index = False)



## looking at words that appear both in 1949 and 2000
combinations_vectors_1949 = []
combinations_vectors_2000 = []
for word in combinations:
    vec = sg_model_2000.wv[word].reshape((1,100))
    vec2 = sg_model_1949.wv[word].reshape((1,100))
    combinations_vectors_1949.append(vec2)
    combinations_vectors_2000.append(vec)

from scipy import spatial

combinationsdf = pd.DataFrame(combinations)
combinationsdf.columns = ["Words"]


simlist = []
for word in combinations:
    vec = sg_model_2000.wv[word].reshape((1,100))
    vec2 = sg_model_1949.wv[word].reshape((1,100))
    sim = spatial.distance.cosine(vec, vec2)
    simlist.append(sim)

combinationsdf["sim between 1949 & 2000"] = simlist

combinationsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/wordsimbetweentimewindow4.csv', index = False)

nearest_neighbors_1949 = []
nearest_neighbors_words_1949 = []
for word in words_1949:
    wordvec = sg_model_1949.wv[word].reshape((1,100))
    others = [x for x in words_1949 if x != word]
    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    for other in others:

        
        othervec = sg_model_1949.wv[other].reshape((1,100))
        othervecs.append(othervec)
    
        for value in othervec:
            sim = spatial.distance.cosine(wordvec, value)
            sims.append(sim)
            setje2 = [other, sim]
            setlist.append(setje2)
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_1949.append(setje)
    for n in near:
        for s in setlist:
            nmr = s[1]
            for m in n:
                if m == nmr:
                    neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_1949.append(makeset)
        
            #nearest_neighbors.append(sim)

nearest_neighbors_2000 = []
nearest_neighbors_words_2000 = []
for word in words_2000:
    wordvec = sg_model_2000.wv[word].reshape((1,100))
    others = [x for x in words_2000 if x != word]
    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    for other in others:

        
        othervec = sg_model_2000.wv[other].reshape((1,100))
        othervecs.append(othervec)
    
        for value in othervec:
            sim = spatial.distance.cosine(wordvec, value)
            sims.append(sim)
            setje2 = [other, sim]
            setlist.append(setje2)
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_2000.append(setje)
    for n in near:
        for s in setlist:
            nmr = s[1]
            for m in n:
                if m == nmr:
                    neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_2000.append(makeset)
        
            #nearest_neighbors.append(sim)

nn_df_2000 = pd.DataFrame(nearest_neighbors_2000)
nn_df_2000_2 = pd.DataFrame(nearest_neighbors_words_2000)
nn_df_2000.to_csv('nn_df_2000W2Vwindow3.csv', index = False)
nn_df_2000_2.to_csv("nn_df_2000_wordsW2Vwindow2.csv", index = False)

nn_df_1949 = pd.DataFrame(nearest_neighbors_1949)
nn_df_1949_2 = pd.DataFrame(nearest_neighbors_words_1949)
nn_df_1949.to_csv('nn_df_1949W2Vwindow3.csv', index = False)
nn_df_1949_2.to_csv("nn_df_1949_wordsW2Vwindow2.csv", index = False)

def linear_align(base_embed, other_embed):
    """
        Align other embedding to base embedding using best linear transform.
        NOTE: Assumes indices are aligned
    """
    basevecs = base_embed
    othervecs = other_embed
    fixedvecs = othervecs.dot(np.linalg.pinv(othervecs)).dot(basevecs)
    return fixedvecs

def smart_procrustes_align_gensim(base_embed, other_embed, words=None):
	"""Procrustes align two gensim word2vec models (to allow for comparison between same word across models).
	Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.
		(With help from William. Thank you!)
	First, intersect the vocabularies (see `intersection_align_gensim` documentation).
	Then do the alignment on the other_embed model.
	Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.
	Return other_embed.
	If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).
	"""

	# make sure vocabulary and indices are aligned
	in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)
    
	# get the embedding matrices
	base_vecs =  in_base_embed.wv
	other_vecs = in_other_embed.wv

	# just a matrix dot product with numpy
	m = other_vecs.T.dot(base_vecs) 
	# SVD method from numpy
	u, _, v = np.linalg.svd(m)
	# another matrix operation
	ortho = u.dot(v) 
	# Replace original array with modified one
	# i.e. multiplying the embedding matrix (syn0norm)by "ortho"
	other_embed = other_embed = (other_embed).dot(ortho)
	return other_embed
	
def intersection_align_gensim(m1,m2, words=None):
	"""
	Intersect two gensim word2vec models, m1 and m2.
	Only the shared vocabulary between them is kept.
	If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.
	Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).
	These indices correspond to the new syn0 and syn0norm objects in both gensim models:
		-- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0
		-- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2
	The .vocab dictionary is also updated for each model, preserving the count but updating the index.
	"""

	# Get the vocab for each model
	vocab_m1 = set(m1.wv.key_to_index.keys())
	vocab_m2 = set(m2.wv.key_to_index.keys())

	# Find the common vocabulary
	common_vocab = vocab_m1&vocab_m2
	if words: common_vocab&=set(words)

	# If no alignment necessary because vocab is identical...
	if not vocab_m1-common_vocab and not vocab_m2-common_vocab:
		return (m1,m2)

	# Otherwise sort by frequency (summed for both)
	common_vocab = list(common_vocab)
	common_vocab.sort()

	# Then for each model...
	for m in [m1,m2]:
		# Replace old syn0norm array with new one (with common vocab)
		indices = [m1.wv.key_to_index[w] for w in common_vocab]
		old_arr = m.wv
		new_arr = np.array([old_arr[index] for index in indices])
		m.syn0norm = m.syn0 = new_arr

		# Replace old vocab dictionary with new one (with common vocab)
		# and old index2word with new one
		m.index2word = common_vocab
		old_vocab = m.wv.key_to_index
		new_vocab = {}
		for new_index,word in enumerate(common_vocab):
			old_vocab_obj= old_vocab[word]
			new_vocab[word] = common_vocab[new_index]
		m.vocab = new_vocab

	return (m1,m2)

new_1949, new_2000 = intersection_align_gensim(sg_model_1949, sg_model_2000)

counter = 0
aligned_2 = []
for comb in combinations:
    new_align = linear_align(new_1949.wv[counter].reshape(1,100), new_2000.wv[counter
                                                                                          ].reshape(1,100))
    aligned_2.append(new_align)
    counter += 1

from scipy.spatial import distance
counter = 0

distances = []
alignednns = {}

for wordje in combinations:
  if wordje in alignednns:
    counter += 1
  else:
    dist = distance.cosine(aligned_2[counter], combinations_vectors_2000[counter])
    distances.append(dist)
    alignednns[wordje] = dist
    counter +=1

from operator import itemgetter
N = 100

res = dict(sorted(alignednns.items(), key = itemgetter(1), reverse = True)[:N])
  
# printing result
resultsaligned = str(res)

#open text file
text_file = open("/content/gdrive/My Drive/Thesis/Results/W2V/ResultsalingedNNW2Vsize4.txt", "w")
 
#write string to file
text_file.write(resultsaligned)
 
#close file
text_file.close()

print(len(aligned_2))
print(len(combinations))
print(len(distances))

resultscos = pd.DataFrame(distances, combinations)

resultscos.to_csv("/content/gdrive/My Drive/Thesis/Results/W2V/resultscosW2Vwindow4.csv", index = True)

alignednns = {}

counter = 0
for wordje in combinations:
  if wordje in alignednns:
    counter += 1
  else:
    alignednns[wordje] = distances[counter]
    counter += 1

print(aligned_2[0])
print(combinations[0])

print(len(aligned_2))
print(len(combinations))

print(len(aligned_2))

simalarityaligned_list = []

for vector in aligned_2:
  othervecs = aligned_2
  othersims = []
  wordpairslist = []
  counter = 0
  for other in othervecs:
    sim = spatial.distance.cosine(vector, other)
    othersims.append(sim)

  simalarityaligned_list.append(othersims)

print(len(simalarityaligned_list))
print(len(simalarityaligned_list[0]))

print(combinations[0])

newresult_list = []
counter = 0
for vector in simalarityaligned_list:
  
  newresult = [vector, combinations[counter]]
  counter += 1
  newresult_list.append(newresult)

washington_list = []

for result in newresult_list:
  if 'fleming' in result:
    washington_list.append(result)



washington_list[-1][1]
washington_list[-1].pop()
print(washington_list)



washdf = pd.DataFrame(washington_list[0]).T

washdf["words"] = combinations

washdf.sort_values(by=0, ascending=False)[0:30]

print(len(simalarityaligned_list))
print(simalarityaligned_list)

aligned_neigbors = []
aligned_neigbors_words = []
for word in combinations:
    indx = combinations.index(word)
    wordvec = aligned_2[indx].reshape((1,100))
    others = [x for x in combinations if x != word]
    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    for other in others:

        otherindx = combinations.index(other)
        othervec = aligned_2[otherindx].reshape((1,100))
        othervecs.append(othervec)
    
        for value in othervec:
            sim = spatial.distance.cosine(wordvec, value)
            sims.append(sim)
            setje2 = [other, sim]
            setlist.append(setje2)
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    aligned_neigbors.append(setje)
    for n in near:
        for s in setlist:
            nmr = s[1]
            for m in n:
                if m == nmr:
                    neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    aligned_neigbors_words.append(makeset)
        
            #nearest_neighbors.append(sim)

nn_df_align = pd.DataFrame(aligned_neigbors)
nn_df_align_2 = pd.DataFrame(aligned_neigbors_words)
nn_df_align.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/nn_df_alignW2Vwindow4.csv', index = False)
nn_df_align_2.to_csv("/content/gdrive/My Drive/Thesis/Results/W2V/nn_df_align_wordsW2Vwindow4.csv", index = False)



## Nearest neighbours per word:
neigbours_1949 = []

for word in words_1949:
  nearest = list(sg_model_1949.wv.most_similar(word))
  nearset = [word, nearest]
  neigbours_1949.append(nearset)



neigbours_2000 = []

for word in words_2000:
  nearest = list(sg_model_2000.wv.most_similar(word))
  nearset = [word, nearest]
  neigbours_2000.append(nearset)

neigbours_2000

neigborwords2000 = pd.DataFrame(neigbours_2000)
neigborwords1949 = pd.DataFrame(neigbours_1949)
neigborwords2000.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/neigborwords2000W2Vsize4.csv', index = False)
neigborwords1949.to_csv("/content/gdrive/My Drive/Thesis/Results/W2V/neigborwords1949W2Vsize4.csv", index = False)

