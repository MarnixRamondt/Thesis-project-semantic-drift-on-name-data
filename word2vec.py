# -*- coding: utf-8 -*-
"""Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fc1E8jAnWthRawhMICJzFBPULSP7leSa
"""

try:
  from google.colab import drive
  IN_COLAB=True
except:
  IN_COLAB=False

if IN_COLAB:
  print("We're running Colab")

!pip install --upgrade gensim

# Commented out IPython magic to ensure Python compatibility.
if IN_COLAB:
  # Mount the Google Drive at mount
  mount='/content/gdrive'
  print("Colab: mounting Google drive on ", mount)

  drive.mount(mount)

  # Switch to the directory on the Google Drive that you want to use
  import os
  drive_root = mount + "/My Drive/Thesis/Code/Descriptives"
  
  # Create drive_root if it doesn't exist
  create_drive_root = True
  if create_drive_root:
    print("\nColab: making sure ", drive_root, " exists.")
    os.makedirs(drive_root, exist_ok=True)
  
  # Change to the directory
  print("\nColab: Changing directory to ", drive_root)
#   %cd $drive_root

!pip install nltk

import nltk
import gensim
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.manifold import TSNE

df_1949 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed1949.csv")
df_2000 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed2000.csv")

print(len(df_1949))
print(len(df_2000))

# Convert a dataframe to the list of rows i.e. list of lists
List_1949 = df_1949.to_numpy().tolist()

List_2000 = df_2000.to_numpy().tolist()

running_text_1949 = []
for listje in List_1949:
    new_list = []
    words = listje[0].split(" ")
    new_list.append(words)
    running_text_1949.append(words)

running_text_2000 = []
for listje in List_2000:
    new_list = []
    words = listje[0].split(" ")
    new_list.append(words)
    running_text_2000.append(words)



#Skip-gram
sg_model_1949 = gensim.models.Word2Vec(running_text_1949, min_count = 4, vector_size = 100,
                                             window = 4, sg = 1, negative = 5)

sg_model_2000 = gensim.models.Word2Vec(running_text_2000, min_count = 4, vector_size = 100,
                                             window = 4, sg = 1, negative = 5)



#Printing the vocabulary list (only for subset otherwise runtimes)
words_1949 = list(sg_model_1949.wv.key_to_index)
words_2000 = list(sg_model_2000.wv.key_to_index)

#print(words_1949)

#print(words_2000)

#loading the name data
#Processing the name dataset
    
with open('/content/gdrive/My Drive/Thesis/Data/Overig/1000Names.txt', 'r') as fp:
    nameslist = fp.readlines()
    
names = []
for name in nameslist:
    new = name.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    names.append(new6)
    

print(len(names))

with open('/content/gdrive/My Drive/Thesis/Data/Overig/Dolgopolsky.txt', 'r') as fp:
    dolgolist = fp.readlines()
    
dolgopolsky_list = []
for dol in dolgolist:
    new = dol.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    dolgopolsky_list.append(new6)
    

print(len(dolgopolsky_list))



#If you want to look at the vector of a specific word:
arrayword_emb_1949 = []
arrayword_emb_2000 = []

for word in words_1949:
    vec = sg_model_1949.wv[word].reshape((1,100))
    arrayword_emb_1949.append(vec)
    
for word in words_2000:
    vec = sg_model_2000.wv[word].reshape((1,100))
    arrayword_emb_2000.append(vec)

import numpy as np

total_emb_1949 = np.array(arrayword_emb_1949)
total_emb_2000 = np.array(arrayword_emb_2000)


combinations = []
for word in words_1949:
    if word in words_2000:
        combinations.append(word)
  

## looking at words that appear both in 1949 and 2000
combinations_vectors_1949 = []
combinations_vectors_2000 = []
for word in combinations:
    vec = sg_model_2000.wv[word].reshape((1,100))
    vec2 = sg_model_1949.wv[word].reshape((1,100))
    combinations_vectors_1949.append(vec2)
    combinations_vectors_2000.append(vec)

print(len(combinations_vectors_1949))
print(len(combinations_vectors_2000))

#Orthogonal Procrustes conform (Hamilton, 2016)
def smart_procrustes_align(base_embed, other_embed, post_normalize=True):
    m = other_embed.T.dot(base_embed)
    u, _, v = np.linalg.svd(m) 
    ortho = u.dot(v)
    final = other_embed.dot(ortho)
    return final

alinged0 = smart_procrustes_align(combinations_vectors_1949[0], combinations_vectors_2000[0])



from scipy import spatial

counter = 0
alinged_drift_results = {}
alinged_vecs = []

aligned = smart_procrustes_align(combinations_vectors_1949, combinations_vectors_2000)
alinged_vecs.append(aligned)
dist  =  spatial.distance.cosine(combinations_vectors_2000, aligned)
if word in alinged_drift_results:
  pass
else:
  alinged_drift_results[word] = dist
counter += 1

alinged_drift_results_values = list(alinged_drift_results.values())

data_dict = alinged_drift_results
data_items = data_dict.items()
data_list = list(data_items)
aligned_resultsdf = pd.DataFrame(data_list)
average_token_drift_aligned = round(aligned_resultsdf[1].mean(),3)
print(average_token_drift_aligned)
aligned_resultsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/Aligned_drift_results_W2V.csv', index = False)

print(len(aligned_resultsdf[1]))

name_results_aligned = []
overview_aligned = []
counter = 0
for word in combinations:
  if word in names:
    drift = aligned_resultsdf.iloc[counter][1]
    setje = [word, drift]
    overview_aligned.append(setje)
    name_results_aligned.append(drift)
    counter += 1

overview_aligned

dolgo_aligned_results = []
dolgo_overview = []
counter = 0
for word in combinations:
  if word in dolgopolsky_list:
    drift = aligned_resultsdf.iloc[counter][1]
    setje = [word, drift]
    dolgo_overview.append(setje)
    dolgo_aligned_results.append(drift)
    counter += 1

dolgo_overview

import seaborn as sns

color_blindtab = sns.color_palette("colorblind", 20)

print(len(alinged_drift_results_values))

# Import the libraries
import matplotlib.pyplot as plt
import seaborn as sns

alinged_drift_results_values = np.array(alinged_drift_results_values)
name_results_aligned = np.array(name_results_aligned)

# seaborn histogram
sns.distplot(alinged_drift_results_values, hist=True, kde=False, color = color_blindtab[7],
             hist_kws={'edgecolor':'black'})

sns.distplot(name_results_aligned, hist=True, kde=False, color ='black',
             hist_kws={'edgecolor':'black'})


# Add labels
plt.title('Semantic drift Measured with aligned vector spaces SGNS')
plt.xlabel('Semantic drift (cosine distance)')
plt.ylabel('Frequency')
plt.axvline(x=alinged_drift_results_values.mean(),
            color='red', linestyle='dashed')
plt.axvline(x=name_results_aligned.mean(),
            color = "blue")
plt.legend( ["General Mean", "Names mean", 'All word types', "Historical names"])


plt.savefig('/content/gdrive/My Drive/Thesis/Results/W2V/semantic_drict_dist_everythingW2V.png')

print(len(name_results_aligned))
namedriftarray = np.array(name_results_aligned)
avg_name_drift = np.mean(namedriftarray)
alinged_drift_results_values = np.array(alinged_drift_results_values)
avg_overall_drift_aligned = np.mean(alinged_drift_results_values)
print(avg_overall_drift_aligned)
print(avg_name_drift)

tsne = TSNE(n_components=2)

vocab_1949 = list(sg_model_1949.wv.key_to_index)
test_vocab_1949 = words_1949
X_1949 = sg_model_1949.wv[vocab_1949]

vocab_2000 = list(sg_model_2000.wv.key_to_index)
test_vocab_2000 = words_2000
X_2000 = sg_model_2000.wv[vocab_2000]

X_tsne_1949 = tsne.fit_transform(X_1949)
X_tsne_2000 = tsne.fit_transform(X_2000)

wordvecs_1949 = pd.DataFrame(X_tsne_1949, index=vocab_1949, columns=['x', 'y'])

wordvecs_2000 = pd.DataFrame(X_tsne_2000, index=vocab_2000, columns=['x', 'y'])

import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

ax.scatter(wordvecs_1949['x'], wordvecs_1949['y'])

for word, pos in wordvecs_1949.iterrows():
    ax.annotate(word, pos)
    

plt.savefig('/content/gdrive/My Drive/Thesis/Results/W2V/Word-embeddingsW2V_1949window4.png')
plt.show()

import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

ax.scatter(wordvecs_2000['x'], wordvecs_2000['y'])
for word, pos in wordvecs_2000.iterrows():
    ax.annotate(word, pos)
    
plt.savefig('/content/gdrive/My Drive/Thesis/Results/W2V/Word-embeddingsW2V_2000window4.png')
plt.show()

with open('/content/gdrive/My Drive/Thesis/Data/Overig/moving_words.txt', 'r') as fp:
    moving = fp.readlines()

moving_list = []
for move in moving:
    new = move.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    moving_list.append(new6)

moving_word = []
for move in moving_list:
  if move in words_1949 and move in words_2000:
    moving_word.append(move)

for moving in moving_word:
  words_2000.append(move)
  words_1949.append(move)

results_df = pd.read_csv("/content/gdrive/My Drive/Thesis/Data/Overig/One_evaluation_set.csv")

results_df

results_df = results_df.sort_values(by=['Rating'], ascending=False)

indexlist = []
for number in range(1, len(results_df) + 1):
    indexlist.append(number)
    
results_df["index"] = indexlist
print("length evalset before processing:", len(results_df))

setlist = []
for index, row in results_df.iterrows():
    temp_set = []
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    temp_set.append(word1)
    temp_set.append(word2)
    if temp_set in setlist:
        results_df = results_df.drop([index])
    else:
        setlist.append(temp_set)


print("length of evalset after processing:", len(results_df))

eval_words = []
eval_sets = []
for index, row in results_df.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    set_list = []
    set_list.append(word1)
    set_list.append(word2)
    eval_sets.append(set_list)
    eval_words.append(word1)
    eval_words.append(word2)

from scipy.stats import spearmanr
import numpy as np

spearmans_list_2000 = []
spearmans_set_list_2000 = []

for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in words_2000 and word2 in words_2000:
        word1_vec = sg_model_2000.wv[word1].reshape((1, 100))
        array1 = np.array(word1_vec)
        word2_vec = sg_model_2000.wv[word2].reshape((1,100))
        array2 = np.array(word2_vec)
        df = pd.DataFrame(word1_vec)
        df2 = pd.DataFrame(word2_vec)
        df = df.T
        df2 = df2.T
        rho, p = spearmanr(df[0:100], df2[0:100])
        result_list = []
        result_list.append(word1)
        result_list.append(word2)
        result_list.append(rho)
        spearmans_set_list_2000.append(result_list)
        spearmans_list_2000.append(rho)

from scipy.stats import spearmanr
import numpy as np

spearmans_list_1949 = []
spearmans_set_list_1949 = []

for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in words_1949 and word2 in words_1949:
        word1_vec = sg_model_1949.wv[word1].reshape((1, 100))
        array1 = np.array(word1_vec)
        word2_vec = sg_model_1949.wv[word2].reshape((1,100))
        array2 = np.array(word2_vec)
        df = pd.DataFrame(word1_vec)
        df2 = pd.DataFrame(word2_vec)
        df = df.T
        df2 = df2.T
        rho, p = spearmanr(df[0:100], df2[0:100])
        result_list = []
        result_list.append(word1)
        result_list.append(word2)
        result_list.append(rho)
        spearmans_set_list_1949.append(result_list)
        spearmans_list_1949.append(rho)

spearmansdf_2000 = pd.DataFrame(spearmans_set_list_2000)
spearmansdf_2000.columns = ["Word 1", "Word 2", "Spearman"]

spearmansdf_2000 = spearmansdf_2000.sort_values(by=['Spearman'], ascending=False)


indexlist_2000 = []
for number in range(1, len(spearmansdf_2000) + 1):
    indexlist_2000.append(number)
    
spearmansdf_2000["index"] = indexlist_2000
#spearmansdf_2000

spearmansdf_1949 = pd.DataFrame(spearmans_set_list_1949)
spearmansdf_1949.columns = ["Word 1", "Word 2", "Spearman"]

spearmansdf_1949 = spearmansdf_1949.sort_values(by=['Spearman'], ascending=False)


indexlist_1949 = []
for number in range(1, len(spearmansdf_1949) + 1):
    indexlist_1949.append(number)
    
spearmansdf_1949["index"] = indexlist_1949
#spearmansdf

temp_list_res_2000 = []
temp_list_2000 = []
instances_list_2000 = []

for index, row in spearmansdf_2000.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_2000.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_2000.append(help_list)
for instance in temp_list_2000:
    #print(instance)
    for instance2 in temp_list_res_2000:
        #print(instance2)
        if instance == instance2:
            instances_list_2000.append(instance)
            
            
           
#instances_list

temp_list_res_1949 = []
temp_list_1949 = []
instances_list_1949 = []

for index, row in spearmansdf_1949.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_1949.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_1949.append(help_list)

for instance in temp_list_1949:
    #print(instance)
    for instance2 in temp_list_res_1949:
        #print(instance2)
        if instance == instance2:
            instances_list_1949.append(instance)
            
            
           
#instances_list

results_list_2000 = []
spearmans_list_2000 = []


for instance in instances_list_2000:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_2000.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_2000:
                    pass
                else:
                    spearmans_list_2000.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
                if index_res in results_list_2000:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_2000.append(index_res)
                tracker_2.clear()

results_list_1949 = []
spearmans_list_1949 = []


for instance in instances_list_1949:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_1949.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_1949:
                    pass
                else:
                    spearmans_list_1949.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
            else:
              
                if index_res in results_list_1949:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_1949.append(index_res)
                tracker_2.clear()

print(instances_list_1949)

print(spearmans_list_1949)
print(results_list_1949)

spearmans_ranking_df_1949 = pd.DataFrame()
spearmans_ranking_df_1949["Eval index"] = results_list_1949[0:5]
spearmans_ranking_df_1949["Embedding index"] = spearmans_list_1949[0:5]
spearmans_ranking_df_1949["Words"] = instances_list_1949[0:5]
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
spearmans_ranking_df_1949["corr"] = spearmansdf_1949["Spearman"]
spearmans_ranking_df_1949

spearmans_ranking_df_1949 = pd.DataFrame()
spearmans_ranking_df_1949["Eval index"] = results_list_1949[0:5]
spearmans_ranking_df_1949["Embedding index"] = spearmans_list_1949[0:5]
spearmans_ranking_df_1949["Words"] = instances_list_1949[0:5]
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
#spearmans_ranking_df_1949

from scipy import stats
from sklearn.metrics import ndcg_score

evalind_1949 = spearmans_ranking_df_1949["Eval index"]
embind_1949 = spearmans_ranking_df_1949["Embedding index"]
dsgeval_1949 = list([evalind_1949])
dsgemb_1949 = list([embind_1949])

spearmansresult_1949 = stats.spearmanr(embind_1949, evalind_1949)
dcgresults_1949 = ndcg_score(dsgeval_1949, dsgemb_1949)

spearmans_ranking_df_2000 = pd.DataFrame()
spearmans_ranking_df_2000["Eval index"] = results_list_2000
spearmans_ranking_df_2000["Embedding index"] = spearmans_list_2000
spearmans_ranking_df_2000["Words"] = instances_list_2000
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
spearmans_ranking_df_2000

from scipy import stats
from sklearn.metrics import ndcg_score

evalind_2000 = spearmans_ranking_df_2000["Eval index"]
embind_2000 = spearmans_ranking_df_2000["Embedding index"]
dsgeval_2000 = list([evalind_2000])
dsgemb_2000 = list([embind_2000])


spearmansresult_2000 = stats.spearmanr(evalind_2000, embind_2000)
dcgresults_2000 = ndcg_score(dsgeval_2000, dsgemb_2000)

spearmansdf = pd.DataFrame()
spearmansdf["SPR2000"] = spearmansresult_2000
spearmansdf["SPR1949"] = spearmansresult_1949
spearmansdf["DCG2000"] = dcgresults_2000
spearmansdf["DSG1949"] = dcgresults_1949

spearmansdf

spearmansdf.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/spearmans_rank_corr_W2V_2000_1949window4.csv', index = False)

## looking at words that appear both in 1949 and 2000
combinations_vectors_1949 = []
combinations_vectors_2000 = []
for word in combinations:
    vec = sg_model_2000.wv[word].reshape((1,100))
    vec2 = sg_model_1949.wv[word].reshape((1,100))
    combinations_vectors_1949.append(vec2)
    combinations_vectors_2000.append(vec)

from scipy import spatial

combinationsdf = pd.DataFrame(combinations)
combinationsdf.columns = ["Words"]


simlist = []
for word in combinations:
    vec = sg_model_2000.wv[word].reshape((1,100))
    vec2 = sg_model_1949.wv[word].reshape((1,100))
    sim = spatial.distance.cosine(vec, vec2)
    simlist.append(sim)

combinationsdf["sim between 1949 & 2000"] = simlist

combinationsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/wordsimbetweentimewindow4.csv', index = False)

nearest_neighbors_1949 = []
nearest_neighbors_words_1949 = []
for word in words_1949:
    wordvec = sg_model_1949.wv[word].reshape((1,100))
    others = [x for x in words_1949 if x != word]
    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    for other in others:

        
        othervec = sg_model_1949.wv[other].reshape((1,100))
        othervecs.append(othervec)
    
        for value in othervec:
            sim = spatial.distance.cosine(wordvec, value)
            sims.append(sim)
            setje2 = [other, sim]
            setlist.append(setje2)
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_1949.append(setje)
    for n in near:
        for s in setlist:
            nmr = s[1]
            for m in n:
                if m == nmr:
                    neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_1949.append(makeset)
        
            #nearest_neighbors.append(sim)

nearest_neighbors_2000 = []
nearest_neighbors_words_2000 = []
for word in words_2000:
    wordvec = sg_model_2000.wv[word].reshape((1,100))
    others = [x for x in words_2000 if x != word]
    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    for other in others:

        
        othervec = sg_model_2000.wv[other].reshape((1,100))
        othervecs.append(othervec)
    
        for value in othervec:
            sim = spatial.distance.cosine(wordvec, value)
            sims.append(sim)
            setje2 = [other, sim]
            setlist.append(setje2)
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_2000.append(setje)
    for n in near:
        for s in setlist:
            nmr = s[1]
            for m in n:
                if m == nmr:
                    neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_2000.append(makeset)
        
            #nearest_neighbors.append(sim)

nn_df_2000 = pd.DataFrame(nearest_neighbors_2000)
nn_df_2000_2 = pd.DataFrame(nearest_neighbors_words_2000)
nn_df_2000.to_csv('nn_df_2000W2Vwindow3.csv', index = False)
nn_df_2000_2.to_csv("nn_df_2000_wordsW2Vwindow2.csv", index = False)

nn_df_1949 = pd.DataFrame(nearest_neighbors_1949)
nn_df_1949_2 = pd.DataFrame(nearest_neighbors_words_1949)
nn_df_1949.to_csv('nn_df_1949W2Vwindow3.csv', index = False)
nn_df_1949_2.to_csv("nn_df_1949_wordsW2Vwindow2.csv", index = False)

"""DOLGO LIST ANALYSIS"""

dolgo_aligned_results = []
dolgo_overview = []
counter = 0
for word in combinations:
  if word in dolgopolsky_list:
    drift = aligned_resultsdf.iloc[counter][1]
    setje = [word, drift]
    dolgo_overview.append(setje)
    dolgo_aligned_results.append(drift)
    counter += 1

dolgo_overview

## Nearest neighbours per word:
neigbours_1949 = []

for word in combinations:
  nearest = list(sg_model_1949.wv.most_similar(word, topn = 250))
  nearset = [word, nearest]
  neigbours_1949.append(nearset)

neigbours_2000 = []

for word in combinations:
  nearest = list(sg_model_2000.wv.most_similar(word, topn = 250))
  nearset = [word, nearest]
  neigbours_2000.append(nearset)

neighborlist_2000 = []
for neigbors in neigbours_2000:
  for  n in neigbors:
    counter = 0
    for near in n:
      if len(near) > 1:
        neighborlist_2000.append(near[0])

neighborlist_1949 = []
for neigbors in neigbours_1949:
  neighbors = []
  for n in neigbors:
    counter = 0
    for near in n:
      if len(near) > 1:
        neighbors.append(list(near[0]))
  neighborlist_1949.append(neighbors)


neighbor_1949_complete = []
for entry in neighborlist_1949:
  appendedletters_1949 = []
  for letter in entry:
    new = "".join(letter)
    appendedletters_1949.append(new)
  neighbor_1949_complete.append(appendedletters_1949)

neighborlist_2000 = []
for neigbors in neigbours_2000:
  neighbors = []
  for n in neigbors:
    counter = 0
    for near in n:
      if len(near) > 1:
        neighbors.append(list(near[0]))
  neighborlist_2000.append(neighbors)


neighbor_2000_complete = []
for entry in neighborlist_2000:
  appendedletters_2000 = []
  for letter in entry:
    new = "".join(letter)
    appendedletters_2000.append(new)
  neighbor_2000_complete.append(appendedletters_2000)

print(len(neighborlist_2000))
print(len(neighbor_1949_complete))
print(len(combinations))

NN_results = {}
counter = 0
for word in combinations:
  firstlist = neighbor_1949_complete[counter]
  secondlist = neighbor_2000_complete[counter]
  thirdlist = len(set(firstlist)&set(secondlist))
  precentage = thirdlist / (len(firstlist))
  drift = 1 - precentage
  if word in NN_results:
    pass
  else:
    NN_results[word] = drift
  counter += 1



data_dict = NN_results
data_items = data_dict.items()
data_list = list(data_items)
NN_resultsdf = pd.DataFrame(data_list)
average_token_drift_NN = NN_resultsdf[1].mean()
print(average_token_drift_NN)
NN_resultsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/NN_drift_results_W2V.csv', index = False)

overall_drift_NN = np.array(NN_resultsdf[1])

name_results_NN = []
overview_NN = []
counter = 0
for word in combinations:
  if word in names:
    drift = NN_resultsdf.iloc[counter][1]
    setje = [word, drift]
    overview_NN.append(setje)
    name_results_NN.append(drift)
    counter += 1

overview_NN

print(name_results_NN)
print(len(name_results_NN))
namedriftarray_NN = np.array(name_results_NN)
avg_name_drift_NN = np.mean(namedriftarray_NN)
print(avg_name_drift_NN)

# Import the libraries
import matplotlib.pyplot as plt
import seaborn as sns

overall_drift_NN = np.array(NN_resultsdf[1])
namedriftarray_NN = np.array(name_results_NN)

# seaborn histogram
sns.distplot(overall_drift_NN, hist=True, kde=False, color = 'gray',
             hist_kws={'edgecolor':'black'})

sns.distplot(namedriftarray_NN, hist=True, kde=False, color = 'black',
             hist_kws={'edgecolor':'black'})


# Add labels
plt.title('Semantic drift measured with nearest neighbors SGNS')
plt.xlabel('Semantic drift (nearest neigbors)')
plt.ylabel('Frequency')
plt.axvline(x=overall_drift_NN.mean(),
            color='red', linestyle='dashed')
plt.axvline(x=namedriftarray_NN.mean(),
            color = "blue")
plt.legend( ["General Mean", "Names mean", 'All word types', "Historical names"])


plt.savefig('/content/gdrive/My Drive/Thesis/Results/W2V/semantic_drict_dist_everythingW2VNN.png')

neigborwords2000 = pd.DataFrame(neigbours_2000)
neigborwords1949 = pd.DataFrame(neigbours_1949)
neigborwords2000.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/neigborwords2000W2Vsize4.csv', index = False)
neigborwords1949.to_csv("/content/gdrive/My Drive/Thesis/Results/W2V/neigborwords1949W2Vsize4.csv", index = False)

## Dolgo list NN:


dolgo_NN = []
overview_NN_dolgo = []
counter = 0
for word in combinations:
  if word in dolgopolsky_list:
    drift = NN_resultsdf.iloc[counter][1]
    setje = [word, drift]
    overview_NN_dolgo.append(setje)
    dolgo_NN.append(drift)
    counter += 1

overview_NN_dolgo

print(len(combinations))

indices = list(range(0,238))
print(indices)
print(len(indices))



#Negative values analysis:
negative_val_dict19 = {}
negative_val_dict20 = {}
negative_val_alinged_dict = {}

counter = 0
for word in combinations:
  ninety_vec = combinations_vectors_1949[counter]
  twenty_vec = combinations_vectors_2000[counter]
  aligned_vec = alinged_vecs[counter]
  neg_count_nine = len(list(filter(lambda x: (x < 0), ninety_vec[0])))
  neg_count_twen = len(list(filter(lambda x: (x < 0), twenty_vec[0])))
  neg_count_align = len(list(filter(lambda x: (x < 0), aligned_vec[0])))
  negative_val_dict19[word] = neg_count_nine
  negative_val_dict20[word] = neg_count_twen
  negative_val_alinged_dict[word] = neg_count_align
  counter += 1

negative_val_dict19_arr = np.array(list(negative_val_dict19.values()))
negative_val_dict20_arr = np.array(list(negative_val_dict20.values()))
negative_val_dictali_arr = np.array(list(negative_val_alinged_dict.values()))

print("average negative values 1949:", np.mean(negative_val_dict19_arr))
print("average negative values 2000:", np.mean(negative_val_dict20_arr))
print("average negative values aligned:", np.mean(negative_val_dictali_arr))