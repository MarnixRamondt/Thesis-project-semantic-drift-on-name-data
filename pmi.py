# -*- coding: utf-8 -*-
"""PMI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/178N-OivKbE-8P9sZEpXaRJR-7xkN0LT8
"""

try:
  from google.colab import drive
  IN_COLAB=True
except:
  IN_COLAB=False

if IN_COLAB:
  print("We're running Colab")

# Commented out IPython magic to ensure Python compatibility.
if IN_COLAB:
  # Mount the Google Drive at mount
  mount='/content/gdrive'
  print("Colab: mounting Google drive on ", mount)

  drive.mount(mount)

  # Switch to the directory on the Google Drive that you want to use
  import os
  drive_root = mount + "/My Drive/Thesis/Code/Models"
  
  # Create drive_root if it doesn't exist
  create_drive_root = True
  if create_drive_root:
    print("\nColab: making sure ", drive_root, " exists.")
    os.makedirs(drive_root, exist_ok=True)
  
  # Change to the directory
  print("\nColab: Changing directory to ", drive_root)
#   %cd $drive_root

import pandas as pd
df_1949 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed1949.csv")
df_2000 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed2000.csv")

# Convert a dataframe to the list of rows i.e. list of lists
List_1949 = df_1949.to_numpy().tolist()

List_2000 = df_2000.to_numpy().tolist()

from collections import Counter
import itertools

import nltk
from nltk.corpus import stopwords
import numpy as np
import pandas as pd
from scipy import sparse
from scipy.sparse import linalg 
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity

list_1949 = []
for i in List_1949:
    for text in i:
        j = text.split(" ")
        list_1949.append(j)
        
list_2000 = []
for i in List_2000:
    for text in i:
        j = text.split(" ")
        list_2000.append(j)

tok2indx_1949 = dict()
unigram_counts_1949 = Counter()
for ii, ngram in enumerate(list_1949):
    if ii % 200000 == 0:
        print(f'finished {ii/len(list_1949):.2%} of ngrams')
    for token in ngram:
        unigram_counts_1949[token] += 1
        if token not in tok2indx_1949:
            tok2indx_1949[token] = len(tok2indx_1949)
            
tok2indx_1949 = {indx:tok for tok,indx in tok2indx_1949.items()}
print('done')
print('vocabulary size: {}'.format(len(unigram_counts_1949)))
print('most common: {}'.format(unigram_counts_1949.most_common(10)))

tok2indx_2000 = dict()
unigram_counts_2000 = Counter()
for ii, ngram in enumerate(list_2000):
    if ii % 200000 == 0:
        print(f'finished {ii/len(ngram):.2%} of ngrams')
    for token in ngram:
        unigram_counts_2000[token] += 1
        if token not in tok2indx_2000:
            tok2indx_2000[token] = len(tok2indx_2000)
            
tok2indx_2000 = {indx:tok for tok,indx in tok2indx_2000.items()}
print('done')
print('vocabulary size: {}'.format(len(unigram_counts_2000)))
print('most common: {}'.format(unigram_counts_2000.most_common(10)))

tok2indx_switch_1949 = dict((y,x) for x,y in tok2indx_1949.items())
vocab_1949 = list(tok2indx_switch_1949.keys())


tok2indx_switch_2000 = dict((y,x) for x,y in tok2indx_2000.items())
vocab_2000 = list(tok2indx_switch_2000.keys())

# note add dynammic window hyperparameter
back_window = 4
front_window = 4
skipgram_counts_1949 = Counter()

for igram, ngram in enumerate(list_1949):
    for ifw, fw in enumerate(ngram):
        
        icw_min = max(0, ifw - back_window)
        icw_max = min(len(ngram) - 1, ifw + front_window)
        
        icws = [ii for ii in range(icw_min, icw_max + 1) if ii != ifw]
        
        for icw in icws:
            skipgram = (ngram[ifw], ngram[icw])
            skipgram_counts_1949[skipgram] += 1    
    if igram % 200000 == 0:
        print(f'finished {igram/len(list_1949):.2%} of ngrams')
        
print('done')
print('number of skipgrams: {}'.format(len(skipgram_counts_1949)))
print('most common: {}'.format(skipgram_counts_1949.most_common(10)))

# note add dynammic window hyperparameter
back_window = 4
front_window = 4
skipgram_counts_2000 = Counter()

for igram, ngram in enumerate(list_2000):
    for ifw, fw in enumerate(ngram):
        
        icw_min = max(0, ifw - back_window)
        icw_max = min(len(ngram) - 1, ifw + front_window)
        
        icws = [ii for ii in range(icw_min, icw_max + 1) if ii != ifw]
        
        for icw in icws:
            skipgram = (ngram[ifw], ngram[icw])
            skipgram_counts_2000[skipgram] += 1    
    if igram % 200000 == 0:
        print(f'finished {igram/len(list_2000):.2%} of ngrams')
        
print('done')
print('number of skipgrams: {}'.format(len(skipgram_counts_2000)))
print('most common: {}'.format(skipgram_counts_2000.most_common(10)))

row_indxs_1949 = []
col_indxs_1949 = []
dat_values_1949 = []
ii = 0

for (tok1, tok2), sg_count in skipgram_counts_1949.items():
    ii += 1
    if ii % 1000000 == 0:
        print(f'finished {ii/len(skipgram_counts_1949):.2%} of skipgrams')
    
    tok2indx_switch = dict((y,x) for x,y in tok2indx_1949.items())
    tok1_indx = tok2indx_switch[tok1]
    tok2_indx = tok2indx_switch[tok2]
        
    row_indxs_1949.append(tok1_indx)
    col_indxs_1949.append(tok2_indx)
    dat_values_1949.append(sg_count)
    
wwcnt_mat_1949 = sparse.csr_matrix((dat_values_1949, (row_indxs_1949, col_indxs_1949)))
print('done')

row_indxs_2000 = []
col_indxs_2000 = []
dat_values_2000 = []
ii = 0

for (tok1, tok2), sg_count in skipgram_counts_2000.items():
    ii += 1
    if ii % 1000000 == 0:
        print(f'finished {ii/len(skipgram_counts_2000):.2%} of skipgrams')
    tok2indx_switch = dict((y,x) for x,y in tok2indx_2000.items())
    tok1_indx = tok2indx_switch[tok1]
    tok2_indx = tok2indx_switch[tok2]
        
    row_indxs_2000.append(tok1_indx)
    col_indxs_2000.append(tok2_indx)
    dat_values_2000.append(sg_count)
    
wwcnt_mat_2000 = sparse.csr_matrix((dat_values_2000, (row_indxs_2000, col_indxs_2000)))
print('done')

def ww_sim(word, mat, topn=10):
    """Calculate topn most similar words to word"""
    indx = tok2indx[word]
    if isinstance(mat, sparse.csr_matrix):
        v1 = mat.getrow(indx)
    else:
        v1 = mat[indx:indx+1, :]
    sims = cosine_similarity(mat, v1).flatten()
    sindxs = np.argsort(-sims)
    sim_word_scores = [(indx2tok[sindx], sims[sindx]) for sindx in sindxs[0:topn]]
    return sim_word_scores

print('done')

wwcnt_norm_mat_1949 = normalize(wwcnt_mat_1949, norm='l2', axis=1)
print('done')

wwcnt_norm_mat_2000 = normalize(wwcnt_mat_2000, norm='l2', axis=1)
print('done')

num_skipgrams_1949 = wwcnt_mat_1949.sum()
assert(sum(skipgram_counts_1949.values())==num_skipgrams_1949)

# for creating sparse matrices
row_indxs_1949 = []
col_indx_1949 = []

# pmi: pointwise mutual information
pmi_dat_values_1949 = []
# ppmi: positive pointwise mutual information
ppmi_dat_values_1949 = []
# spmi: smoothed pointwise mutual information
spmi_dat_values_1949 = []
# sppmi: smoothed positive pointwise mutual information
sppmi_dat_values_1949 = []

# Sum over words and contexts
sum_over_words_1949 = np.array(wwcnt_mat_1949.sum(axis=0)).flatten()
sum_over_contexts_1949 = np.array(wwcnt_mat_1949.sum(axis=1)).flatten()

# Smoothing
# According to [Levy, Goldberg & Dagan, 2015], the smoothing operation 
# should be done on the context 
alpha = 0.75
nca_denom_1949 = np.sum(sum_over_contexts_1949**alpha)
# sum_over_words_alpha = sum_over_words**alpha
sum_over_contexts_alpha_1949 = sum_over_contexts_1949**alpha


ii = 0
for (tok1, tok2), sg_count in skipgram_counts_1949.items():
    ii += 1
    if ii % 1000000 == 0:
        print(f'finished {ii/len(skipgram_counts_1949):.2%} of skipgrams')
    tok2indx_switch = dict((y,x) for x,y in tok2indx_1949.items())
    tok1_indx = tok2indx_switch[tok1]
    tok2_indx = tok2indx_switch[tok2]
    
    nwc = sg_count
    Pwc = nwc / num_skipgrams_1949

    nw = sum_over_contexts_1949[tok1_indx]
    Pw = nw / num_skipgrams_1949
    
    nc = sum_over_words_1949[tok2_indx]
    Pc = nc / num_skipgrams_1949
    
    pmi = np.log2(Pwc/(Pw*Pc))
    ppmi = max(pmi, 0)
    
#   nca = sum_over_words_alpha[tok2_indx]
    nca = sum_over_contexts_alpha_1949[tok2_indx]
    Pca = nca / nca_denom_1949

    spmi = np.log2(Pwc/(Pw*Pca))
    sppmi = max(spmi, 0)
    
    row_indxs_1949.append(tok1_indx)
    col_indx_1949.append(tok2_indx)
    pmi_dat_values_1949.append(pmi)
    ppmi_dat_values_1949.append(ppmi)
    spmi_dat_values_1949.append(spmi)
    sppmi_dat_values_1949.append(sppmi)

pmi_mat_1949 = sparse.csr_matrix((pmi_dat_values_1949, (row_indxs_1949, col_indx_1949)))
ppmi_mat_1949 = sparse.csr_matrix((ppmi_dat_values_1949, (row_indxs_1949, col_indx_1949)))
spmi_mat_1949 = sparse.csr_matrix((spmi_dat_values_1949, (row_indxs_1949, col_indx_1949)))
sppmi_mat_1949 = sparse.csr_matrix((sppmi_dat_values_1949, (row_indxs_1949, col_indx_1949)))

print('done')

pmi_use = pmi_mat_1949
embedding_size = 6
uu, ss, vv = linalg.svds(pmi_use, embedding_size)

num_skipgrams_2000 = wwcnt_mat_2000.sum()
assert(sum(skipgram_counts_2000.values())==num_skipgrams_2000)

# for creating sparse matrices
row_indxs_2000 = []
col_indx_2000 = []

# pmi: pointwise mutual information
pmi_dat_values_2000 = []
# ppmi: positive pointwise mutual information
ppmi_dat_values_2000 = []
# spmi: smoothed pointwise mutual information
spmi_dat_values_2000 = []
# sppmi: smoothed positive pointwise mutual information
sppmi_dat_values_2000 = []

# Sum over words and contexts
sum_over_words_2000 = np.array(wwcnt_mat_2000.sum(axis=0)).flatten()
sum_over_contexts_2000 = np.array(wwcnt_mat_2000.sum(axis=1)).flatten()

# Smoothing
# According to [Levy, Goldberg & Dagan, 2015], the smoothing operation 
# should be done on the context 
alpha = 0.75
nca_denom_2000 = np.sum(sum_over_contexts_2000**alpha)
# sum_over_words_alpha = sum_over_words**alpha
sum_over_contexts_alpha_2000 = sum_over_contexts_2000**alpha


ii = 0
for (tok1, tok2), sg_count in skipgram_counts_2000.items():
    ii += 1
    if ii % 1000000 == 0:
        print(f'finished {ii/len(skipgram_counts_2000):.2%} of skipgrams')
    tok2indx_switch = dict((y,x) for x,y in tok2indx_2000.items())
    tok1_indx = tok2indx_switch[tok1]
    tok2_indx = tok2indx_switch[tok2]
    
    nwc = sg_count
    Pwc = nwc / num_skipgrams_2000

    nw = sum_over_contexts_2000[tok1_indx]
    Pw = nw / num_skipgrams_2000
    
    nc = sum_over_words_2000[tok2_indx]
    Pc = nc / num_skipgrams_2000
    
    pmi = np.log2(Pwc/(Pw*Pc))
    ppmi = max(pmi, 0)
    
#   nca = sum_over_words_alpha[tok2_indx]
    nca = sum_over_contexts_alpha_2000[tok2_indx]
    Pca = nca / nca_denom_2000

    spmi = np.log2(Pwc/(Pw*Pca))
    sppmi = max(spmi, 0)
    
    row_indxs_2000.append(tok1_indx)
    col_indx_2000.append(tok2_indx)
    pmi_dat_values_2000.append(pmi)
    ppmi_dat_values_2000.append(ppmi)
    spmi_dat_values_2000.append(spmi)
    sppmi_dat_values_2000.append(sppmi)

pmi_mat_2000 = sparse.csr_matrix((pmi_dat_values_2000, (row_indxs_2000, col_indx_2000)))
ppmi_mat_2000 = sparse.csr_matrix((ppmi_dat_values_2000, (row_indxs_2000, col_indx_2000)))
spmi_mat_2000 = sparse.csr_matrix((spmi_dat_values_2000, (row_indxs_2000, col_indx_2000)))
sppmi_mat_2000 = sparse.csr_matrix((sppmi_dat_values_2000, (row_indxs_2000, col_indx_2000)))

print('done')

print(ppmi_mat_1949.shape)

pmi_use_1949 = ppmi_mat_1949
embedding_size_1949 = 7
uu_1949, ss_1949, vv_1949 = linalg.svds(pmi_use_1949, embedding_size_1949)

print(uu_1949.shape)
print(ss_1949.shape)
print(vv_1949.shape)

pmi_use_2000 = ppmi_mat_2000
embedding_size_2000 = 7
uu_2000, ss_2000, vv_2000 = linalg.svds(pmi_use_2000, embedding_size_2000)

unorm_1949 = uu_1949 / np.sqrt(np.sum(uu_1949*uu_1949, axis=1, keepdims=True))
vnorm_1949 = vv_1949 / np.sqrt(np.sum(vv_1949*vv_1949, axis=0, keepdims=True))
#word_vecs = unorm
#word_vecs = vnorm.T
word_vecs_1949 = uu_1949 + vv_1949.T
word_vecs_norm_1949 = word_vecs_1949 / np.sqrt(np.sum(word_vecs_1949*word_vecs_1949, axis=1, keepdims=True))

unorm_2000 = uu_2000 / np.sqrt(np.sum(uu_2000*uu_2000, axis=1, keepdims=True))
vnorm_2000 = vv_2000 / np.sqrt(np.sum(vv_2000*vv_2000, axis=0, keepdims=True))
#word_vecs = unorm
#word_vecs = vnorm.T
word_vecs_2000 = uu_2000 + vv_2000.T
word_vecs_norm_2000 = word_vecs_2000 / np.sqrt(np.sum(word_vecs_2000*word_vecs_2000, axis=1, keepdims=True))

def word_sim_report(word, sim_mat):
    sim_word_scores = ww_sim(word, word_vecs)
    for sim_word, sim_score in sim_word_scores:
        print(sim_word, sim_score)
        word_headlines = [hl for hl in headlines if sim_word in hl and word in hl][0:5]
        for headline in word_headlines:
            print(f'    {headline}')

#1949
pmiarray_1949 = pmi_mat_1949.toarray()
pmidf_1949 = pd.DataFrame(pmiarray_1949)


word_embedding_1949 = []

counter = 0
for word in vocab_1949:
    wordvec = word_vecs_norm_1949[counter]
    word_embedding_1949.append(wordvec)
    counter += 1
    
#2000
pmiarray_2000 = pmi_mat_2000.toarray()
pmidf_2000 = pd.DataFrame(pmiarray_2000)


word_embedding_2000 = []

counter = 0
for word in vocab_2000:
    wordvec = word_vecs_norm_2000[counter]
    word_embedding_2000.append(wordvec)
    counter += 1

results_df = pd.read_csv("/content/gdrive/My Drive/Thesis/Data/Overig/One_evaluation_set.csv")

results_df

results_df = results_df.sort_values(by=['Rating'], ascending=False)

indexlist = []
for number in range(1, len(results_df) + 1):
    indexlist.append(number)
    
results_df["index"] = indexlist
print("length evalset before processing:", len(results_df))

setlist = []
for index, row in results_df.iterrows():
    temp_set = []
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    temp_set.append(word1)
    temp_set.append(word2)
    if temp_set in setlist:
        results_df = results_df.drop([index])
    else:
        setlist.append(temp_set)


print("length of evalset after processing:", len(results_df))

#results_df[196:199]

eval_words = []
eval_sets = []
for index, row in results_df.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    set_list = []
    set_list.append(word1)
    set_list.append(word2)
    eval_sets.append(set_list)
    eval_words.append(word1)
    eval_words.append(word2)

from scipy.stats import spearmanr
import numpy as np

spearmans_list_1949 = []
spearmans_set_list_1949 = []


for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in vocab_1949 and word2 in vocab_1949:
        
        tokindex = vocab_1949.index(word1)                
        wordvec1 = word_vecs_norm_1949[tokindex]
        array1 = np.array(wordvec1)
        
        tokindex2 = vocab_1949.index(word2)                
        wordvec2 = word_vecs_norm_1949[tokindex2]
        array2 = np.array(wordvec2)

        df = pd.DataFrame(wordvec1)
        df2 = pd.DataFrame(wordvec2)
        #df = df.T
        #df2 = df2.T
        
        rho, p = spearmanr(df, df2)

        result_list = []
        result_list.append(word1)
        result_list.append(word2)
        result_list.append(rho)
        
        spearmans_set_list_1949.append(result_list)
        spearmans_list_1949.append(rho)

from scipy.stats import spearmanr
import numpy as np

spearmans_list_2000 = []
spearmans_set_list_2000 = []


for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in vocab_2000 and word2 in vocab_2000:
        
        tokindex = vocab_2000.index(word1)                
        wordvec1 = word_vecs_norm_2000[tokindex]
        array1 = np.array(wordvec1)
        
        tokindex2 = vocab_2000.index(word2)                
        wordvec2 = word_vecs_norm_2000[tokindex2]
        array2 = np.array(wordvec2)

        df = pd.DataFrame(wordvec1)
        df2 = pd.DataFrame(wordvec2)
        #df = df.T
        #df2 = df2.T
        
        rho, p = spearmanr(df, df2)

        result_list = []
        result_list.append(word1)
        result_list.append(word2)
        result_list.append(rho)
        
        spearmans_set_list_2000.append(result_list)
        spearmans_list_2000.append(rho)

if len(spearmans_set_list_1949) > 0:
    spearmansdf_1949 = pd.DataFrame(spearmans_set_list_1949)
    spearmansdf_1949.columns = ["Word 1", "Word 2", "Spearman"]

    spearmansdf_1949 = spearmansdf_1949.sort_values(by=['Spearman'], ascending=False)


    indexlist = []
    for number in range(1, len(spearmansdf_1949) + 1):
        indexlist.append(number)
    
    spearmansdf_1949["index"] = indexlist
    spearmansdf_1949

if len(spearmans_set_list_2000) > 0:
    spearmansdf_2000 = pd.DataFrame(spearmans_set_list_2000)
    spearmansdf_2000.columns = ["Word 1", "Word 2", "Spearman"]

    spearmansdf_2000 = spearmansdf_2000.sort_values(by=['Spearman'], ascending=False)


    indexlist = []
    for number in range(1, len(spearmansdf_2000) + 1):
        indexlist.append(number)
    
    spearmansdf_2000["index"] = indexlist
    spearmansdf_2000

temp_list_res_1949 = []
temp_list_1949 = []
instances_list_1949 = []

for index, row in spearmansdf_1949.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_1949.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_1949.append(help_list)
    
for instance in temp_list_1949:
    #print(instance)
    for instance2 in temp_list_res_1949:
        #print(instance2)
        if instance == instance2:
            instances_list_1949.append(instance)
            
            
           
#instances_list_1949

temp_list_res_2000 = []
temp_list_2000 = []
instances_list_2000 = []

for index, row in spearmansdf_2000.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_2000.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_2000.append(help_list)
    
for instance in temp_list_2000:
    #print(instance)
    for instance2 in temp_list_res_2000:
        #print(instance2)
        if instance == instance2:
            instances_list_2000.append(instance)
            
            
           
#instances_list_2000

results_list_1949 = []
spearmans_list_1949 = []


for instance in instances_list_1949:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_1949.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_1949:
                    pass
                else:
                    spearmans_list_1949.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
                if index_res in results_list_1949:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_1949.append(index_res)
                tracker_2.clear()

results_list_2000 = []
spearmans_list_2000 = []


for instance in instances_list_2000:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_2000.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_2000:
                    pass
                else:
                    spearmans_list_2000.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
                if index_res in results_list_2000:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_2000.append(index_res)
                tracker_2.clear()

spearmans_ranking_df_1949 = pd.DataFrame()
spearmans_ranking_df_1949["Eval index"] = results_list_1949
spearmans_ranking_df_1949["Embedding index"] = spearmans_list_1949
spearmans_ranking_df_1949["Words"] = instances_list_1949
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
spearmans_ranking_df_1949

spearmans_ranking_df_2000 = pd.DataFrame()
spearmans_ranking_df_2000["Eval index"] = results_list_2000
spearmans_ranking_df_2000["Embedding index"] = spearmans_list_2000
spearmans_ranking_df_2000["Words"] = instances_list_2000
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
spearmans_ranking_df_2000

results_list_2000
sorted(results_list_2000)

spearmans_ranking_df_1949

spearmans_ranking_df_2000["Eval index"] = results_list_2000
spearmans_ranking_df_2000["Embedding index"] = spearmans_list_2000



from sklearn.metrics import ndcg_score

my_r_1949 = spearmans_ranking_df_1949.corr(method="spearman")
spearmans_rank_correlation_df = pd.DataFrame()
spearmans_rank_correlation_df["SPR1949"] = [my_r_1949]

my_r_2000 = spearmans_ranking_df_2000.corr(method="spearman")
spearmans_rank_correlation_df["SPR2000"] = [my_r_2000]

dsgeval_1949 = list([spearmans_list_1949])
dsgemb_1949 = list([results_list_1949])

dcgresults_1949 = ndcg_score(dsgeval_1949, dsgemb_1949)

spearmans_rank_correlation_df["DCG1949"] = dcgresults_1949

dsgeval_2000 = list([spearmans_list_2000])
dsgemb_2000 = list([results_list_2000])

dcgresults_2000 = ndcg_score(dsgeval_2000, dsgemb_2000)

spearmans_rank_correlation_df["DCG2000"] = dcgresults_2000



spearmans_rank_correlation_df.to_csv("/content/gdrive/My Drive/Thesis/Results/PMI/SpearmansDCGPMIwindow4.csv", index = False)


#spearmans_rank_correlation_1949.to_csv('spearmans_rank_corr_PMI_1949.csv', index = False)
#spearmans_rank_correlation_2000.to_csv('spearmans_rank_corr_PMI_2000.csv', index = False)





## looking at words that appear both in 1949 and 2000
combinations = []

for word in vocab_1949:
    if word in vocab_2000:
        combinations.append(word)
  
combinations_vectors_1949 = []
combinations_vectors_2000 = []


for word in combinations:
        
    tokindex = vocab_1949.index(word)                
    wordvec1 = word_vecs_norm_1949[tokindex]
    combinations_vectors_1949.append(wordvec1)
    
    tokindex2 = vocab_2000.index(word)                
    wordvec2 = word_vecs_norm_2000[tokindex2]
    combinations_vectors_2000.append(wordvec2)

from scipy import spatial

combinationsdf = pd.DataFrame(combinations)
combinationsdf.columns = ["Words"]


simlist = []

counter = 0
for word in combinations:
                   
    wordvec1 = combinations_vectors_1949[counter]
    
    wordvec2 = combinations_vectors_2000[counter]
    
    sim = spatial.distance.cosine(wordvec1, wordvec2)
    simlist.append(sim)
    counter += 1

combinationsdf["sim between 1949 & 2000"] = simlist

combinationsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/PMI/wordsimbetweentime_PMIwindow4.csv', index = False)

nearest_neighbors_1949 = []
nearest_neighbors_words_1949 = []

counter = 0
for word in combinations:
    
    wordvec = combinations_vectors_1949[counter]
    counter += 1
    
    
    others = [x for x in combinations if x != word]
    
    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    
    for other in others:

        index = combinations.index(other)
        othervec = combinations_vectors_1949[index]
        othervecs.append(othervec)
    
    for value in othervecs:
        sim = spatial.distance.cosine(wordvec, value)
        sims.append(sim)
        setje2 = [other, sim]
        setlist.append(setje2)
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_1949.append(setje)
    for n in near:
        for s in setlist:
            nmr = s[1]
            for m in n:
                if m == nmr:
                    neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_1949.append(makeset)
        
            #nearest_neighbors.append(sim)

nn_df_1949 = pd.DataFrame(nearest_neighbors_1949)
nn_df_1949_2 = pd.DataFrame(nearest_neighbors_words_1949)
nn_df_1949.to_csv('/content/gdrive/My Drive/Thesis/Results/PMI/nn_df_1949PMIwindow4.csv', index = False)
nn_df_1949_2.to_csv("/content/gdrive/My Drive/Thesis/Results/PMI/nn_df_1949_wordsPMIwindow4.csv", index = False)



nearest_neighbors_2000 = []
nearest_neighbors_words_2000 = []

counter = 0
for word in combinations:
    
    wordvec = combinations_vectors_2000[counter]
    counter += 1
    
    
    others = [x for x in combinations if x != word]
    
    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    
    for other in others:

        index = combinations.index(other)
        othervec = combinations_vectors_1949[index]
        othervecs.append(othervec)
    
    for value in othervecs:
        sim = spatial.distance.cosine(wordvec, value)
        sims.append(sim)
        setje2 = [other, sim]
        setlist.append(setje2)
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_2000.append(setje)
    for n in near:
        for s in setlist:
            nmr = s[1]
            for m in n:
                if m == nmr:
                  if s[0] not in neigbor_words:
                      neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_2000.append(makeset)
        
            #nearest_neighbors.append(sim)

nn_df_2000 = pd.DataFrame(nearest_neighbors_2000)
nn_df_2000_2 = pd.DataFrame(nearest_neighbors_words_2000)
nn_df_2000.to_csv('/content/gdrive/My Drive/Thesis/Results/PMI/nn_df_2000PMIwindow4.csv', index = False)
nn_df_2000_2.to_csv("/content/gdrive/My Drive/Thesis/Results/PMI/nn_df_2000_wordsPMIwindow4.csv", index = False)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from scipy.sparse.linalg import svds
# U, _, _ = svds(pmi_use_1949, k=20)

norms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))
U /= np.maximum(norms, 1e-7)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from itertools import combinations
# 
# cx = Counter()
# cxy = Counter()
# for text in list_1949:
#     for x in text:
#       cx[x] += 1
#       for x, y in map(sorted, combinations(text, 2)):
#           cxy[(x, y)] += 1

# Commented out IPython magic to ensure Python compatibility.
# %%time
# x2i, i2x = {}, {}
# for i, x in enumerate(cx.keys()):
#     x2i[x] = i
#     i2x[i] = x

k = 10
pmi_new_nn_method_1949 = []
for x in vocab_1949:
    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.
    s = ''
    # Compile the list of nearest neighbor descriptions.
    # Argpartition is faster than argsort and meets our needs.
    for i in np.argpartition(-1 * dd, k + 1)[:k + 1]:
        if i2x[i] == x: 
          continue
        s += '(%s, %.3lf) ' % (i2x[i], dd[i])
    #print('%s, %d\n %s' % (x, cx[x], s))
    #print('-' * 10)
    pmi_new_nn_method_1949.append('%s, %d\n %s' % (x, cx[x], s))



# Commented out IPython magic to ensure Python compatibility.
# %%time
# from scipy.sparse.linalg import svds
# U, _, _ = svds(pmi_use_2000, k=20)

norms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))
U /= np.maximum(norms, 1e-7)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from itertools import combinations
# 
# cx = Counter()
# cxy = Counter()
# for text in list_2000:
#     for x in text:
#       cx[x] += 1
#       for x, y in map(sorted, combinations(text, 2)):
#           cxy[(x, y)] += 1

# Commented out IPython magic to ensure Python compatibility.
# %%time
# x2i, i2x = {}, {}
# for i, x in enumerate(cx.keys()):
#     x2i[x] = i
#     i2x[i] = x

k = 10
pmi_new_nn_method_2000 = []
for x in vocab_2000:
    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.
    s = ''
    # Compile the list of nearest neighbor descriptions.
    # Argpartition is faster than argsort and meets our needs.
    for i in np.argpartition(-1 * dd, k + 1)[:k + 1]:
        if i2x[i] == x: 
          continue
        s += '(%s, %.3lf) ' % (i2x[i], dd[i])
    #print('%s, %d\n %s' % (x, cx[x], s))
    #print('-' * 10)
    pmi_new_nn_method_2000.append('%s, %d\n %s' % (x, cx[x], s))

pmi_new_nn_method_1949

pmi_new_nn_method_2000

f= open("/content/gdrive/My Drive/Thesis/Results/PMI/NewtechniqueNNPMIsize41949.txt","w+")
f.write(str(pmi_new_nn_method_1949))
f.close



f= open("/content/gdrive/My Drive/Thesis/Results/PMI/NewtechniqueNNPMIsize42000.txt","w+")
f.write(str(pmi_new_nn_method_2000))
f.close

