# -*- coding: utf-8 -*-
"""PMI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/178N-OivKbE-8P9sZEpXaRJR-7xkN0LT8
"""

try:
  from google.colab import drive
  IN_COLAB=True
except:
  IN_COLAB=False

if IN_COLAB:
  print("We're running Colab")

# Commented out IPython magic to ensure Python compatibility.
if IN_COLAB:
  # Mount the Google Drive at mount
  mount='/content/gdrive'
  print("Colab: mounting Google drive on ", mount)

  drive.mount(mount)

  # Switch to the directory on the Google Drive that you want to use
  import os
  drive_root = mount + "/My Drive/Thesis/Code/Models"
  
  # Create drive_root if it doesn't exist
  create_drive_root = True
  if create_drive_root:
    print("\nColab: making sure ", drive_root, " exists.")
    os.makedirs(drive_root, exist_ok=True)
  
  # Change to the directory
  print("\nColab: Changing directory to ", drive_root)
#   %cd $drive_root

import pandas as pd
df_1949 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed1949.csv")
df_2000 = pd.read_csv("/content/gdrive/My Drive/Thesis/Code/Pre-processing/Processed2000.csv")

# Convert a dataframe to the list of rows i.e. list of lists
List_1949 = df_1949.to_numpy().tolist()

List_2000 = df_2000.to_numpy().tolist()

from collections import Counter
import itertools

import nltk
from nltk.corpus import stopwords
import numpy as np
import pandas as pd
from scipy import sparse
from scipy.sparse import linalg 
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity

list_1949 = []
for i in List_1949:
    for text in i:
        j = text.split(" ")
        list_1949.append(j)
        
list_2000 = []
for i in List_2000:
    for text in i:
        j = text.split(" ")
        list_2000.append(j)

tok2indx_1949 = dict()
unigram_counts_1949 = Counter()
for ii, ngram in enumerate(list_1949):
    if ii % 200000 == 0:
        print(f'finished {ii/len(list_1949):.2%} of ngrams')
    for token in ngram:
        unigram_counts_1949[token] += 1
        if token not in tok2indx_1949:
            tok2indx_1949[token] = len(tok2indx_1949)
            
tok2indx_1949 = {indx:tok for tok,indx in tok2indx_1949.items()}
print('done')
print('vocabulary size: {}'.format(len(unigram_counts_1949)))
print('most common: {}'.format(unigram_counts_1949.most_common(10)))

tok2indx_2000 = dict()
unigram_counts_2000 = Counter()
for ii, ngram in enumerate(list_2000):
    if ii % 200000 == 0:
        print(f'finished {ii/len(ngram):.2%} of ngrams')
    for token in ngram:
        unigram_counts_2000[token] += 1
        if token not in tok2indx_2000:
            tok2indx_2000[token] = len(tok2indx_2000)
            
tok2indx_2000 = {indx:tok for tok,indx in tok2indx_2000.items()}
print('done')
print('vocabulary size: {}'.format(len(unigram_counts_2000)))
print('most common: {}'.format(unigram_counts_2000.most_common(10)))

tok2indx_switch_1949 = dict((y,x) for x,y in tok2indx_1949.items())
vocab_1949 = list(tok2indx_switch_1949.keys())


tok2indx_switch_2000 = dict((y,x) for x,y in tok2indx_2000.items())
vocab_2000 = list(tok2indx_switch_2000.keys())

# note add dynammic window hyperparameter
back_window = 4
front_window = 4
skipgram_counts_1949 = Counter()

for igram, ngram in enumerate(list_1949):
    for ifw, fw in enumerate(ngram):
        
        icw_min = max(0, ifw - back_window)
        icw_max = min(len(ngram) - 1, ifw + front_window)
        
        icws = [ii for ii in range(icw_min, icw_max + 1) if ii != ifw]
        
        for icw in icws:
            skipgram = (ngram[ifw], ngram[icw])
            skipgram_counts_1949[skipgram] += 1    
    if igram % 200000 == 0:
        print(f'finished {igram/len(list_1949):.2%} of ngrams')
        
print('done')
print('number of skipgrams: {}'.format(len(skipgram_counts_1949)))
print('most common: {}'.format(skipgram_counts_1949.most_common(10)))

# note add dynammic window hyperparameter
back_window = 4
front_window = 4
skipgram_counts_2000 = Counter()

for igram, ngram in enumerate(list_2000):
    for ifw, fw in enumerate(ngram):
        
        icw_min = max(0, ifw - back_window)
        icw_max = min(len(ngram) - 1, ifw + front_window)
        
        icws = [ii for ii in range(icw_min, icw_max + 1) if ii != ifw]
        
        for icw in icws:
            skipgram = (ngram[ifw], ngram[icw])
            skipgram_counts_2000[skipgram] += 1    
    if igram % 200000 == 0:
        print(f'finished {igram/len(list_2000):.2%} of ngrams')
        
print('done')
print('number of skipgrams: {}'.format(len(skipgram_counts_2000)))
print('most common: {}'.format(skipgram_counts_2000.most_common(10)))

row_indxs_1949 = []
col_indxs_1949 = []
dat_values_1949 = []
ii = 0

for (tok1, tok2), sg_count in skipgram_counts_1949.items():
    ii += 1
    if ii % 1000000 == 0:
        print(f'finished {ii/len(skipgram_counts_1949):.2%} of skipgrams')
    
    tok2indx_switch = dict((y,x) for x,y in tok2indx_1949.items())
    tok1_indx = tok2indx_switch[tok1]
    tok2_indx = tok2indx_switch[tok2]
        
    row_indxs_1949.append(tok1_indx)
    col_indxs_1949.append(tok2_indx)
    dat_values_1949.append(sg_count)
    
wwcnt_mat_1949 = sparse.csr_matrix((dat_values_1949, (row_indxs_1949, col_indxs_1949)))
print('done')

row_indxs_2000 = []
col_indxs_2000 = []
dat_values_2000 = []
ii = 0

for (tok1, tok2), sg_count in skipgram_counts_2000.items():
    ii += 1
    if ii % 1000000 == 0:
        print(f'finished {ii/len(skipgram_counts_2000):.2%} of skipgrams')
    tok2indx_switch = dict((y,x) for x,y in tok2indx_2000.items())
    tok1_indx = tok2indx_switch[tok1]
    tok2_indx = tok2indx_switch[tok2]
        
    row_indxs_2000.append(tok1_indx)
    col_indxs_2000.append(tok2_indx)
    dat_values_2000.append(sg_count)
    
wwcnt_mat_2000 = sparse.csr_matrix((dat_values_2000, (row_indxs_2000, col_indxs_2000)))
print('done')

def ww_sim(word, mat, topn=10):
    """Calculate topn most similar words to word"""
    indx = tok2indx[word]
    if isinstance(mat, sparse.csr_matrix):
        v1 = mat.getrow(indx)
    else:
        v1 = mat[indx:indx+1, :]
    sims = cosine_similarity(mat, v1).flatten()
    sindxs = np.argsort(-sims)
    sim_word_scores = [(indx2tok[sindx], sims[sindx]) for sindx in sindxs[0:topn]]
    return sim_word_scores

print('done')

wwcnt_norm_mat_1949 = normalize(wwcnt_mat_1949, norm='l2', axis=1)
print('done')

wwcnt_norm_mat_2000 = normalize(wwcnt_mat_2000, norm='l2', axis=1)
print('done')

num_skipgrams_1949 = wwcnt_mat_1949.sum()
assert(sum(skipgram_counts_1949.values())==num_skipgrams_1949)

# for creating sparse matrices
row_indxs_1949 = []
col_indx_1949 = []

# pmi: pointwise mutual information
pmi_dat_values_1949 = []
# ppmi: positive pointwise mutual information
ppmi_dat_values_1949 = []
# spmi: smoothed pointwise mutual information
spmi_dat_values_1949 = []
# sppmi: smoothed positive pointwise mutual information
sppmi_dat_values_1949 = []

# Sum over words and contexts
sum_over_words_1949 = np.array(wwcnt_mat_1949.sum(axis=0)).flatten()
sum_over_contexts_1949 = np.array(wwcnt_mat_1949.sum(axis=1)).flatten()

# Smoothing
# According to [Levy, Goldberg & Dagan, 2015], the smoothing operation 
# should be done on the context 
alpha = 0.75
nca_denom_1949 = np.sum(sum_over_contexts_1949**alpha)
# sum_over_words_alpha = sum_over_words**alpha
sum_over_contexts_alpha_1949 = sum_over_contexts_1949**alpha


ii = 0
for (tok1, tok2), sg_count in skipgram_counts_1949.items():
    ii += 1
    if ii % 1000000 == 0:
        print(f'finished {ii/len(skipgram_counts_1949):.2%} of skipgrams')
    tok2indx_switch = dict((y,x) for x,y in tok2indx_1949.items())
    tok1_indx = tok2indx_switch[tok1]
    tok2_indx = tok2indx_switch[tok2]
    
    nwc = sg_count
    Pwc = nwc / num_skipgrams_1949

    nw = sum_over_contexts_1949[tok1_indx]
    Pw = nw / num_skipgrams_1949
    
    nc = sum_over_words_1949[tok2_indx]
    Pc = nc / num_skipgrams_1949
    
    pmi = np.log2(Pwc/(Pw*Pc))
    ppmi = max(pmi, 0)
    
#   nca = sum_over_words_alpha[tok2_indx]
    nca = sum_over_contexts_alpha_1949[tok2_indx]
    Pca = nca / nca_denom_1949

    spmi = np.log2(Pwc/(Pw*Pca))
    sppmi = max(spmi, 0)
    
    row_indxs_1949.append(tok1_indx)
    col_indx_1949.append(tok2_indx)
    pmi_dat_values_1949.append(pmi)
    ppmi_dat_values_1949.append(ppmi)
    spmi_dat_values_1949.append(spmi)
    sppmi_dat_values_1949.append(sppmi)

pmi_mat_1949 = sparse.csr_matrix((pmi_dat_values_1949, (row_indxs_1949, col_indx_1949)))
ppmi_mat_1949 = sparse.csr_matrix((ppmi_dat_values_1949, (row_indxs_1949, col_indx_1949)))
spmi_mat_1949 = sparse.csr_matrix((spmi_dat_values_1949, (row_indxs_1949, col_indx_1949)))
sppmi_mat_1949 = sparse.csr_matrix((sppmi_dat_values_1949, (row_indxs_1949, col_indx_1949)))

print('done')

pmi_use = spmi_mat_1949
embedding_size = 6
uu, ss, vv = linalg.svds(pmi_use, embedding_size)

num_skipgrams_2000 = wwcnt_mat_2000.sum()
assert(sum(skipgram_counts_2000.values())==num_skipgrams_2000)

# for creating sparse matrices
row_indxs_2000 = []
col_indx_2000 = []

# pmi: pointwise mutual information
pmi_dat_values_2000 = []
# ppmi: positive pointwise mutual information
ppmi_dat_values_2000 = []
# spmi: smoothed pointwise mutual information
spmi_dat_values_2000 = []
# sppmi: smoothed positive pointwise mutual information
sppmi_dat_values_2000 = []

# Sum over words and contexts
sum_over_words_2000 = np.array(wwcnt_mat_2000.sum(axis=0)).flatten()
sum_over_contexts_2000 = np.array(wwcnt_mat_2000.sum(axis=1)).flatten()

# Smoothing
# According to [Levy, Goldberg & Dagan, 2015], the smoothing operation 
# should be done on the context 
alpha = 0.75
nca_denom_2000 = np.sum(sum_over_contexts_2000**alpha)
# sum_over_words_alpha = sum_over_words**alpha
sum_over_contexts_alpha_2000 = sum_over_contexts_2000**alpha


ii = 0
for (tok1, tok2), sg_count in skipgram_counts_2000.items():
    ii += 1
    if ii % 1000000 == 0:
        print(f'finished {ii/len(skipgram_counts_2000):.2%} of skipgrams')
    tok2indx_switch = dict((y,x) for x,y in tok2indx_2000.items())
    tok1_indx = tok2indx_switch[tok1]
    tok2_indx = tok2indx_switch[tok2]
    
    nwc = sg_count
    Pwc = nwc / num_skipgrams_2000

    nw = sum_over_contexts_2000[tok1_indx]
    Pw = nw / num_skipgrams_2000
    
    nc = sum_over_words_2000[tok2_indx]
    Pc = nc / num_skipgrams_2000
    
    pmi = np.log2(Pwc/(Pw*Pc))
    ppmi = max(pmi, 0)
    
#   nca = sum_over_words_alpha[tok2_indx]
    nca = sum_over_contexts_alpha_2000[tok2_indx]
    Pca = nca / nca_denom_2000

    spmi = np.log2(Pwc/(Pw*Pca))
    sppmi = max(spmi, 0)
    
    row_indxs_2000.append(tok1_indx)
    col_indx_2000.append(tok2_indx)
    pmi_dat_values_2000.append(pmi)
    ppmi_dat_values_2000.append(ppmi)
    spmi_dat_values_2000.append(spmi)
    sppmi_dat_values_2000.append(sppmi)

pmi_mat_2000 = sparse.csr_matrix((pmi_dat_values_2000, (row_indxs_2000, col_indx_2000)))
ppmi_mat_2000 = sparse.csr_matrix((ppmi_dat_values_2000, (row_indxs_2000, col_indx_2000)))
spmi_mat_2000 = sparse.csr_matrix((spmi_dat_values_2000, (row_indxs_2000, col_indx_2000)))
sppmi_mat_2000 = sparse.csr_matrix((sppmi_dat_values_2000, (row_indxs_2000, col_indx_2000)))

print('done')

print(ppmi_mat_1949.shape)

pmi_use_1949 = sppmi_mat_1949
embedding_size_1949 = 7
uu_1949, ss_1949, vv_1949 = linalg.svds(pmi_use_1949, embedding_size_1949)

print(uu_1949.shape)
print(ss_1949.shape)
print(vv_1949.shape)

pmi_use_2000 = sppmi_mat_2000
embedding_size_2000 = 7
uu_2000, ss_2000, vv_2000 = linalg.svds(pmi_use_2000, embedding_size_2000)

unorm_1949 = uu_1949 / np.sqrt(np.sum(uu_1949*uu_1949, axis=1, keepdims=True))
vnorm_1949 = vv_1949 / np.sqrt(np.sum(vv_1949*vv_1949, axis=0, keepdims=True))
#word_vecs = unorm
#word_vecs = vnorm.T
word_vecs_1949 = uu_1949 + vv_1949.T
word_vecs_norm_1949 = word_vecs_1949 / np.sqrt(np.sum(word_vecs_1949*word_vecs_1949, axis=1, keepdims=True))

unorm_2000 = uu_2000 / np.sqrt(np.sum(uu_2000*uu_2000, axis=1, keepdims=True))
vnorm_2000 = vv_2000 / np.sqrt(np.sum(vv_2000*vv_2000, axis=0, keepdims=True))
#word_vecs = unorm
#word_vecs = vnorm.T
word_vecs_2000 = uu_2000 + vv_2000.T
word_vecs_norm_2000 = word_vecs_2000 / np.sqrt(np.sum(word_vecs_2000*word_vecs_2000, axis=1, keepdims=True))

def word_sim_report(word, sim_mat):
    sim_word_scores = ww_sim(word, word_vecs)
    for sim_word, sim_score in sim_word_scores:
        print(sim_word, sim_score)
        word_headlines = [hl for hl in headlines if sim_word in hl and word in hl][0:5]
        for headline in word_headlines:
            print(f'    {headline}')

#1949
pmiarray_1949 = pmi_mat_1949.toarray()
pmidf_1949 = pd.DataFrame(pmiarray_1949)


word_embedding_1949 = []

counter = 0
for word in vocab_1949:
    wordvec = word_vecs_norm_1949[counter]
    word_embedding_1949.append(wordvec)
    counter += 1
    
#2000
pmiarray_2000 = pmi_mat_2000.toarray()
pmidf_2000 = pd.DataFrame(pmiarray_2000)


word_embedding_2000 = []

counter = 0
for word in vocab_2000:
    wordvec = word_vecs_norm_2000[counter]
    word_embedding_2000.append(wordvec)
    counter += 1

results_df = pd.read_csv("/content/gdrive/My Drive/Thesis/Data/Overig/One_evaluation_set.csv")

results_df

results_df = results_df.sort_values(by=['Rating'], ascending=False)

indexlist = []
for number in range(1, len(results_df) + 1):
    indexlist.append(number)
    
results_df["index"] = indexlist
print("length evalset before processing:", len(results_df))

setlist = []
for index, row in results_df.iterrows():
    temp_set = []
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    temp_set.append(word1)
    temp_set.append(word2)
    if temp_set in setlist:
        results_df = results_df.drop([index])
    else:
        setlist.append(temp_set)


print("length of evalset after processing:", len(results_df))

#results_df[196:199]

eval_words = []
eval_sets = []
for index, row in results_df.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    set_list = []
    set_list.append(word1)
    set_list.append(word2)
    eval_sets.append(set_list)
    eval_words.append(word1)
    eval_words.append(word2)

from scipy.stats import spearmanr
import numpy as np

spearmans_list_1949 = []
spearmans_set_list_1949 = []


for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in vocab_1949 and word2 in vocab_1949:
        
        tokindex = vocab_1949.index(word1)                
        wordvec1 = word_vecs_norm_1949[tokindex]
        array1 = np.array(wordvec1)
        
        tokindex2 = vocab_1949.index(word2)                
        wordvec2 = word_vecs_norm_1949[tokindex2]
        array2 = np.array(wordvec2)

        df = pd.DataFrame(wordvec1)
        df2 = pd.DataFrame(wordvec2)
        #df = df.T
        #df2 = df2.T
        
        rho, p = spearmanr(df, df2)

        result_list = []
        result_list.append(word1)
        result_list.append(word2)
        result_list.append(rho)
        
        spearmans_set_list_1949.append(result_list)
        spearmans_list_1949.append(rho)

from scipy.stats import spearmanr
import numpy as np

spearmans_list_2000 = []
spearmans_set_list_2000 = []


for wordset in eval_sets:
    word1 = wordset[0]
    word2 = wordset[1]
    if word1 in vocab_2000 and word2 in vocab_2000:
        
        tokindex = vocab_2000.index(word1)                
        wordvec1 = word_vecs_norm_2000[tokindex]
        array1 = np.array(wordvec1)
        
        tokindex2 = vocab_2000.index(word2)                
        wordvec2 = word_vecs_norm_2000[tokindex2]
        array2 = np.array(wordvec2)

        df = pd.DataFrame(wordvec1)
        df2 = pd.DataFrame(wordvec2)
        #df = df.T
        #df2 = df2.T
        
        rho, p = spearmanr(df, df2)

        result_list = []
        result_list.append(word1)
        result_list.append(word2)
        result_list.append(rho)
        
        spearmans_set_list_2000.append(result_list)
        spearmans_list_2000.append(rho)

if len(spearmans_set_list_1949) > 0:
    spearmansdf_1949 = pd.DataFrame(spearmans_set_list_1949)
    spearmansdf_1949.columns = ["Word 1", "Word 2", "Spearman"]

    spearmansdf_1949 = spearmansdf_1949.sort_values(by=['Spearman'], ascending=False)


    indexlist = []
    for number in range(1, len(spearmansdf_1949) + 1):
        indexlist.append(number)
    
    spearmansdf_1949["index"] = indexlist
    spearmansdf_1949

if len(spearmans_set_list_2000) > 0:
    spearmansdf_2000 = pd.DataFrame(spearmans_set_list_2000)
    spearmansdf_2000.columns = ["Word 1", "Word 2", "Spearman"]

    spearmansdf_2000 = spearmansdf_2000.sort_values(by=['Spearman'], ascending=False)


    indexlist = []
    for number in range(1, len(spearmansdf_2000) + 1):
        indexlist.append(number)
    
    spearmansdf_2000["index"] = indexlist
    spearmansdf_2000

temp_list_res_1949 = []
temp_list_1949 = []
instances_list_1949 = []

for index, row in spearmansdf_1949.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_1949.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_1949.append(help_list)
    
for instance in temp_list_1949:
    #print(instance)
    for instance2 in temp_list_res_1949:
        #print(instance2)
        if instance == instance2:
            instances_list_1949.append(instance)
            
            
           
#instances_list_1949

temp_list_res_2000 = []
temp_list_2000 = []
instances_list_2000 = []

for index, row in spearmansdf_2000.iterrows():
    word1 = row["Word 1"]
    word2 = row["Word 2"]
    index = row["index"]
    help_list2 = []
    help_list2.append(word1)
    help_list2.append(word2)
    temp_list_2000.append(help_list2)
    

for index, row in results_df.iterrows():
    word1_res = row["Word 1"]
    word2_res = row["Word 2"]
    index_res = row["index"]
    help_list = []
    help_list.append(word1_res)
    help_list.append(word2_res)
    temp_list_res_2000.append(help_list)
    
for instance in temp_list_2000:
    #print(instance)
    for instance2 in temp_list_res_2000:
        #print(instance2)
        if instance == instance2:
            instances_list_2000.append(instance)
            
            
           
#instances_list_2000

results_list_1949 = []
spearmans_list_1949 = []


for instance in instances_list_1949:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_1949.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_1949:
                    pass
                else:
                    spearmans_list_1949.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
                if index_res in results_list_1949:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_1949.append(index_res)
                tracker_2.clear()

results_list_2000 = []
spearmans_list_2000 = []


for instance in instances_list_2000:
    #print(instance)
    tracker = []
    tracker_2 =[]
    for index, row in spearmansdf_2000.iterrows():
        word1_end = row["Word 1"]
        word2_end = row["Word 2"]
        temp_list_ending = []
        temp_list_ending.append(word1_end)
        temp_list_ending.append(word2_end)
        if temp_list_ending in tracker:
            continue
        else:
            tracker.append(temp_list_ending)
           # print("tracker", tracker)
            if word1_end in instance and word2_end in instance:
                index_spear = row["index"]
                #print("index_spear:", index_spear, "instance:", instance)
                if index_spear in spearmans_list_2000:
                    pass
                else:
                    spearmans_list_2000.append(index_spear)
                tracker.clear()
    
    for index, row in results_df.iterrows():
        word1_end2 = row["Word 1"]
        word2_end2 = row["Word 2"]
        temp_list_ending_2 = []
        temp_list_ending_2.append(word1_end2)
        temp_list_ending_2.append(word2_end2)
        if temp_list_ending_2 in tracker_2:
            pass
        else:
            tracker_2.append(temp_list_ending_2)
            if word1_end2 in instance and word2_end2 in instance:
                index_res = row["index"]
                #print("index_res:", index_res, "instance:", instance)
                #print("result_list:", results_list)
                if index_res in results_list_2000:
                    #print("index_res:", index_res)
                    pass
                else:
                    results_list_2000.append(index_res)
                tracker_2.clear()

spearmans_ranking_df_1949 = pd.DataFrame()
spearmans_ranking_df_1949["Eval index"] = results_list_1949
spearmans_ranking_df_1949["Embedding index"] = spearmans_list_1949
spearmans_ranking_df_1949["Words"] = instances_list_1949
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
spearmans_ranking_df_1949

spearmans_ranking_df_2000 = pd.DataFrame()
spearmans_ranking_df_2000["Eval index"] = results_list_2000
spearmans_ranking_df_2000["Embedding index"] = spearmans_list_2000
spearmans_ranking_df_2000["Words"] = instances_list_2000
#spearmans_ranking_df.columns = ["Eval index", "Embedding index"]
spearmans_ranking_df_2000

spearmans_ranking_df_1949

spearmans_ranking_df_2000["Eval index"] = results_list_2000
spearmans_ranking_df_2000["Embedding index"] = spearmans_list_2000

from sklearn.metrics import ndcg_score

my_r_1949 = spearmans_ranking_df_1949.corr(method="spearman")
spearmans_rank_correlation_df = pd.DataFrame()
spearmans_rank_correlation_df["SPR1949"] = [my_r_1949]

my_r_2000 = spearmans_ranking_df_2000.corr(method="spearman")
spearmans_rank_correlation_df["SPR2000"] = [my_r_2000]

dsgeval_1949 = list([spearmans_list_1949])
dsgemb_1949 = list([results_list_1949])

dcgresults_1949 = ndcg_score(dsgeval_1949, dsgemb_1949)

spearmans_rank_correlation_df["DCG1949"] = dcgresults_1949

dsgeval_2000 = list([spearmans_list_2000])
dsgemb_2000 = list([results_list_2000])

dcgresults_2000 = ndcg_score(dsgeval_2000, dsgemb_2000)

spearmans_rank_correlation_df["DCG2000"] = dcgresults_2000



spearmans_rank_correlation_df.to_csv("/content/gdrive/My Drive/Thesis/Results/PMI/SpearmansDCGPMIwindow4.csv", index = False)


#spearmans_rank_correlation_1949.to_csv('spearmans_rank_corr_PMI_1949.csv', index = False)
#spearmans_rank_correlation_2000.to_csv('spearmans_rank_corr_PMI_2000.csv', index = False)

#loading the name data
#Processing the name dataset
    
with open('/content/gdrive/My Drive/Thesis/Data/Overig/1000Names.txt', 'r') as fp:
    nameslist = fp.readlines()
    
names = []
for name in nameslist:
    new = name.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    names.append(new6)
    

print(len(names))

with open('/content/gdrive/My Drive/Thesis/Data/Overig/Dolgopolsky.txt', 'r') as fp:
    dolgolist = fp.readlines()
    
dolgopolsky_list = []
for dol in dolgolist:
    new = dol.replace(",", "")
    new2 = new.replace("\n", "")
    new3 = new2.lower()
    new4 = new3.rstrip()
    new5 = new4.lstrip()
    new6 = new5.replace("'", "")
    dolgopolsky_list.append(new6)
    

print(len(dolgopolsky_list))

## looking at words that appear both in 1949 and 2000
combinations_words = []

for word in vocab_1949:
    if word in vocab_2000:
        combinations_words.append(word)
  
combinations_vectors_1949 = []
combinations_vectors_2000 = []


for word in combinations_words:
        
    tokindex = vocab_1949.index(word)                
    wordvec1 = word_vecs_norm_1949[tokindex]
    combinations_vectors_1949.append(wordvec1)
    
    tokindex2 = vocab_2000.index(word)                
    wordvec2 = word_vecs_norm_2000[tokindex2]
    combinations_vectors_2000.append(wordvec2)

from scipy import spatial

combinationsdf = pd.DataFrame(combinations_words)
combinationsdf.columns = ["Words"]


simlist = []

counter = 0
for word in combinations_words:
                   
    wordvec1 = combinations_vectors_1949[counter]
    
    wordvec2 = combinations_vectors_2000[counter]
    
    sim = spatial.distance.cosine(wordvec1, wordvec2)
    simlist.append(sim)
    counter += 1

combinationsdf["sim between 1949 & 2000"] = simlist

print(len(combinations_vectors_1949))
print(len(combinations_vectors_2000))

combinations_vectors_2000[0].shape

#orthogonal Procrustes
def smart_procrustes_align(base_embed, other_embed, post_normalize=True):
    m = other_embed.T.dot(base_embed)
    u, _, v = np.linalg.svd(m) 
    ortho = u.dot(v)
    final = other_embed.dot(ortho)
    return final

from scipy import spatial

counter = 0
alinged_drift_results = {}

for word in combinations_words:
  aligned = smart_procrustes_align(combinations_vectors_1949[counter].reshape(combinations_vectors_1949[counter].shape[0], 1), combinations_vectors_2000[counter].reshape(combinations_vectors_2000[counter].shape[0], 1))
  dist  =  spatial.distance.cosine(combinations_vectors_1949[counter].reshape(combinations_vectors_2000[counter].shape[0], 1), aligned)
  if word in alinged_drift_results:
    pass
  else:
    alinged_drift_results[word] = dist
  counter += 1

data_dict = alinged_drift_results
data_items = data_dict.items()
data_list = list(data_items)
aligned_resultsdf = pd.DataFrame(data_list)
average_token_drift_aligned = round(aligned_resultsdf[1].mean(),3)
print(average_token_drift_aligned)
aligned_resultsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/PMI/Aligned_drift_results_PMI.csv', index = False)

name_results_aligned = []
overview_aligned = []
counter = 0
for word in combinations_words:
  if word in names:
    drift = aligned_resultsdf.iloc[counter][1]
    setje = [word, drift]
    overview_aligned.append(setje)
    name_results_aligned.append(drift)
    counter += 1

overview_aligned

print(name_results_aligned)
print(len(name_results_aligned))
namedriftarray = np.array(name_results_aligned)
avg_name_drift = np.mean(namedriftarray)
print(avg_name_drift)

alinged_drift_results_values = list(alinged_drift_results.values())

# Import the libraries
import matplotlib.pyplot as plt
import seaborn as sns

alinged_drift_results_values = np.array(alinged_drift_results_values)
name_results_aligned = np.array(name_results_aligned)

# seaborn histogram
sns.distplot(alinged_drift_results_values, hist=True, kde=False, color = 'gray',
             hist_kws={'edgecolor':'black'})

sns.distplot(name_results_aligned, hist=True, kde=False, color = 'black',
             hist_kws={'edgecolor':'black'})


# Add labels
plt.title('Semantic drift measured with aligned vector spaces PPMI')
plt.xlabel('Semantic drift (cosine distance)')
plt.ylabel('Frequency')
plt.axvline(x=alinged_drift_results_values.mean(),
            color='red', linestyle = "dashed")
plt.axvline(x=name_results_aligned.mean(),
            color = "blue")
plt.legend( ["Mean", "Names mean", 'All word types', "Historical names"])


plt.savefig('/content/gdrive/My Drive/Thesis/Results/PMI/semantic_drict_dist_everythingPPMI.png')

combination_names = []

for word in combinations_words:
  if word in names:
    combination_names.append(word)

dolgo_aligned_results = []
dolgo_overview = []
counter = 0
for word in combinations_words:
  if word in dolgopolsky_list:
    drift = aligned_resultsdf.iloc[counter][1]
    setje = [word, drift]
    dolgo_overview.append(setje)
    dolgo_aligned_results.append(drift)
    counter += 1

dolgo_overview

nearest_neighbors_1949 = []
nearest_neighbors_words_1949 = []

counter = 0
for word in combinations_words:
    
    wordvec = combinations_vectors_1949[counter]
    counter += 1
    
    
    others = [x for x in combinations_words if x != word]
    
    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    
    for other in others:

        index = combinations_words.index(other)
        othervec = combinations_vectors_1949[index]
        othervecs.append(othervec)
    
    for value in othervecs:
        sim = spatial.distance.cosine(wordvec, value)
        sims.append(sim)
        setje2 = [other, sim]
        setlist.append(setje2)
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_1949.append(setje)
    for n in near:
        for s in setlist:
            nmr = s[1]
            for m in n:
                if m == nmr:
                    neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_1949.append(makeset)
        
            #nearest_neighbors.append(sim)

nn_df_1949 = pd.DataFrame(nearest_neighbors_1949)
nn_df_1949_2 = pd.DataFrame(nearest_neighbors_words_1949)
nn_df_1949.to_csv('/content/gdrive/My Drive/Thesis/Results/PMI/nn_df_1949PMIwindow4.csv', index = False)
nn_df_1949_2.to_csv("/content/gdrive/My Drive/Thesis/Results/PMI/nn_df_1949_wordsPMIwindow4.csv", index = False)



nearest_neighbors_2000 = []
nearest_neighbors_words_2000 = []

counter = 0
for word in combinations_words:
    
    wordvec = combinations_vectors_2000[counter]
    counter += 1
    
    
    others = [x for x in combinations_words if x != word]
    
    othervecs = []
    near = []
    sims = []
    setlist = []
    neigbor_words = []
    
    for other in others:

        index = combinations_words.index(other)
        othervec = combinations_vectors_1949[index]
        othervecs.append(othervec)
    
    for value in othervecs:
        sim = spatial.distance.cosine(wordvec, value)
        sims.append(sim)
        setje2 = [other, sim]
        setlist.append(setje2)
            

    sims.sort()
    neigbors = sims[-50:]
    near.append(neigbors)
    setje = [word, neigbors]
    nearest_neighbors_2000.append(setje)
    for n in near:
        for s in setlist:
            nmr = s[1]
            for m in n:
                if m == nmr:
                  if s[0] not in neigbor_words:
                      neigbor_words.append(s[0])
                
    
    makeset = [word, neigbor_words]
    nearest_neighbors_words_2000.append(makeset)
        
            #nearest_neighbors.append(sim)

nn_df_2000 = pd.DataFrame(nearest_neighbors_2000)
nn_df_2000_2 = pd.DataFrame(nearest_neighbors_words_2000)
nn_df_2000.to_csv('/content/gdrive/My Drive/Thesis/Results/PMI/nn_df_2000PMIwindow4.csv', index = False)
nn_df_2000_2.to_csv("/content/gdrive/My Drive/Thesis/Results/PMI/nn_df_2000_wordsPMIwindow4.csv", index = False)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from scipy.sparse.linalg import svds
# U, _, _ = svds(pmi_use_1949, k=20)

norms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))
U /= np.maximum(norms, 1e-7)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from itertools import combinations
# 
# cx = Counter()
# cxy = Counter()
# for text in list_1949:
#     for x in text:
#       cx[x] += 1
#       for x, y in map(sorted, combinations(text, 2)):
#           cxy[(x, y)] += 1

# Commented out IPython magic to ensure Python compatibility.
# %%time
# x2i, i2x = {}, {}
# for i, x in enumerate(cx.keys()):
#     x2i[x] = i
#     i2x[i] = x

k = 250
pmi_new_nn_method_1949 = []
for x in vocab_1949:
    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.
    s = ''
    # Compile the list of nearest neighbor descriptions.
    # Argpartition is faster than argsort and meets our needs.
    for i in np.argpartition(-1 * dd, k + 1)[:k + 1]:
        if i2x[i] == x: 
          continue
        s += '(%s, %.3lf) ' % (i2x[i], dd[i])
    #print('%s, %d\n %s' % (x, cx[x], s))
    #print('-' * 10)
    addition = ['%s, %d\n %s' % (x, cx[x], s)]
    pmi_new_nn_method_1949.append(addition)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from scipy.sparse.linalg import svds
# U, _, _ = svds(pmi_use_2000, k=20)

norms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))
U /= np.maximum(norms, 1e-7)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from itertools import combinations as cmbc
# 
# cx = Counter()
# cxy = Counter()
# for text in list_2000:
#     for x in text:
#       cx[x] += 1
#       for x, y in map(sorted, cmbc(text, 2)):
#           cxy[(x, y)] += 1

# Commented out IPython magic to ensure Python compatibility.
# %%time
# x2i, i2x = {}, {}
# for i, x in enumerate(cx.keys()):
#     x2i[x] = i
#     i2x[i] = x

k = 250
pmi_new_nn_method_2000 = []
for x in vocab_2000:
    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.
    s = ''
    # Compile the list of nearest neighbor descriptions.
    # Argpartition is faster than argsort and meets our needs.
    for i in np.argpartition(-1 * dd, k + 1)[:k + 1]:
        if i2x[i] == x: 
          continue
        s += '(%s, %.3lf) ' % (i2x[i], dd[i])
    #print('%s, %d\n %s' % (x, cx[x], s))
    #print('-' * 10)
    addition = ['%s, %d\n %s' % (x, cx[x], s)]
    pmi_new_nn_method_2000.append(addition)

print(pmi_new_nn_method_1949[0])
print(pmi_new_nn_method_2000[0])



f= open("/content/gdrive/My Drive/Thesis/Results/PMI/NewtechniqueNNPMI1949.txt","w+")
f.write(str(pmi_new_nn_method_1949))
f.close



f= open("/content/gdrive/My Drive/Thesis/Results/PMI/NewtechniqueNNPMI2000.txt","w+")
f.write(str(pmi_new_nn_method_2000))
f.close

trial_listje_2000 = []

for trial in pmi_new_nn_method_2000:
  track_list = []
  items = trial[0]
  useful = list(items.split("\n"))
  target = useful[0]
  count = 1
  while count < 500:
    indenttrial = list(useful[1].split(" "))
    make_listje1 = indenttrial[count]
    count += 1
    make_listje2 = indenttrial[count]
    completelistje = [make_listje1, make_listje2]
    track_list.append(completelistje)
    count += 1
  completelistje = [target, track_list]
  trial_listje_2000.append(completelistje)

combination_words_NN_2000 = []

for i in trial_listje_2000:
  target = i[0].split(",")
  target = target[0]
  if target in combinations_words:
    combination_words_NN_2000.append(i)

print(len(combination_words_NN_2000))
print(len(combinations_words))

print(len(pmi_new_nn_method_1949))
print(len(pmi_new_nn_method_2000))

print(pmi_new_nn_method_1949[0])
print(pmi_new_nn_method_2000[0])

"""1949!!"""

trial_listje_1949 = []

for trial in pmi_new_nn_method_1949:
  print("trial", trial)
  track_list = []
  items = trial[0]
  useful = list(items.split("\n"))
  print("useful", useful)
  target = useful[0]
  count = 1
  while count < 500:
    indenttrial = list(useful[1].split(" "))
    make_listje1 = indenttrial[count]
    count += 1
    make_listje2 = indenttrial[count]
    completelistje = [make_listje1, make_listje2]
    track_list.append(completelistje)
    count += 1
  completelistje = [target, track_list]
  trial_listje_1949.append(completelistje)

combination_words_NN_1949 = []

for i in trial_listje_1949:
  target = i[0].split(",")
  target = target[0]
  if target in combinations_words:
    combination_words_NN_1949.append(i)

print(len(combination_words_NN_1949))
print(len(combinations_words))
print(len(combination_words_NN_2000))

print(combination_words_NN_1949[1])
print(combination_words_NN_2000[2])

target_list_1949 = []
for probeersel in combination_words_NN_1949:
  target = probeersel[0].split(",")
  target = target[0]
  target_list_1949.append(target)

target_list_2000 = []
for probeersel in combination_words_NN_2000:
  target = probeersel[0].split(",")
  target = target[0]
  target_list_2000.append(target)

target_list_1949_probs = []
for probeersel in combination_words_NN_1949:
  probs = probeersel[1]
  probs_list = []
  for i in probs:
    probs_target = i[0].replace("(", "")
    probs_target = probs_target.replace(",", "")
    probs_list.append(probs_target)
  target_list_1949_probs.append(probs_list)

target_list_2000_probs = []
for probeersel in combination_words_NN_2000:
  probs = probeersel[1]
  probs_list = []
  for i in probs:
    probs_target = i[0].replace("(", "")
    probs_target = probs_target.replace(",", "")
    probs_list.append(probs_target)
  target_list_2000_probs.append(probs_list)

NN_results = {}
for word in combinations_words:
  idx_1949 = target_list_1949.index(word)
  idx_2000 = target_list_2000.index(word)
  list_1949 = target_list_1949_probs[idx_1949]
  list_2000 = target_list_2000_probs[idx_2000]
  thirdlist = len(set(list_1949)&set(list_2000))
  precentage = thirdlist / (len(list_1949))
  drift = 1 - precentage
  if word in NN_results:
    pass
  else:
    NN_results[word] = drift
  counter += 1

data_dict = NN_results
data_items = data_dict.items()
data_list = list(data_items)
NN_resultsdf = pd.DataFrame(data_list)
average_token_drift_NN = NN_resultsdf[1].mean()
print(average_token_drift_NN)
NN_resultsdf.to_csv('/content/gdrive/My Drive/Thesis/Results/W2V/NN_drift_results_W2V.csv', index = False)

overall_drift_NN = np.array(NN_resultsdf[1])

name_results_NN = []
overview_NN = []
counter = 0
for word in combinations_words:
  if word in names:
    drift = NN_resultsdf.iloc[counter][1]
    setje = [word, drift]
    overview_NN.append(setje)
    name_results_NN.append(drift)
    counter += 1

overview_NN

print(name_results_NN)
print(len(name_results_NN))
namedriftarray_NN = np.array(name_results_NN)
avg_name_drift_NN = np.mean(namedriftarray_NN)
print(avg_name_drift_NN)

# Import the libraries
import matplotlib.pyplot as plt
import seaborn as sns

overall_drift_NN = np.array(NN_resultsdf[1])
namedriftarray_NN = np.array(name_results_NN)

# seaborn histogram
sns.distplot(overall_drift_NN, hist=True, kde=False, color = 'gray',
             hist_kws={'edgecolor':'black'})

sns.distplot(namedriftarray_NN, hist=True, kde=False, color = 'black',
             hist_kws={'edgecolor':'black'})


# Add labels
plt.title('Semantic drift measured with nearest neighbors PPMI')
plt.xlabel('Semantic drift (nearest neighbors)')
plt.ylabel('Frequency')
plt.axvline(x=overall_drift_NN.mean(),
            color='red', linestyle = "dashed")
plt.axvline(x=namedriftarray_NN.mean(),
            color = "blue")
plt.legend( ["General Mean", "Names mean", 'All word types', "Historical names"])


plt.savefig('/content/gdrive/My Drive/Thesis/Results/PMI/semantic_drict_dist_everythingPMINN.png')

## DOlgo list NN:
dolgo_NN = []
overview_NN_dolgo = []
counter = 0
for word in combinations_words:
  if word in dolgopolsky_list:
    drift = NN_resultsdf.iloc[counter][1]
    setje = [word, drift]
    overview_NN_dolgo.append(setje)
    dolgo_NN.append(drift)
    counter += 1

overview_NN_dolgo

#Negative values analysis:
negative_val_dict19 = {}
negative_val_dict20 = {}
negative_val_alinged_dict = {}

counter = 0
for word in combination_words:
  ninety_vec = combinations_vectors_1949[counter]
  twenty_vec = combinations_vectors_2000[counter]
  aligned_vec = alinged_vecs[counter]
  neg_count_nine = len(list(filter(lambda x: (x < 0), ninety_vec[0])))
  neg_count_twen = len(list(filter(lambda x: (x < 0), twenty_vec[0])))
  neg_count_align = len(list(filter(lambda x: (x < 0), aligned_vec[0])))
  negative_val_dict19[word] = neg_count_nine
  negative_val_dict20[word] = neg_count_twen
  negative_val_alinged_dict[word] = neg_count_align
  counter += 1

negative_val_dict19_arr = np.array(list(negative_val_dict19.values()))
negative_val_dict20_arr = np.array(list(negative_val_dict20.values()))
negative_val_dictali_arr = np.array(list(negative_val_alinged_dict.values()))

print("average negative values 1949:", np.mean(negative_val_dict19_arr))
print("average negative values 2000:", np.mean(negative_val_dict20_arr))
print("average negative values aligned:", np.mean(negative_val_dictali_arr))